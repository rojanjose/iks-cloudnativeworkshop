{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"IKS Cloud Native Workshop \u00b6 Welcome to the IKS Cloud Native Workshop! In the workshop you will learn about foundational open source technologies and industry-wide accepted best practices for building modern, scalable, portable, and robust applications. You will learn about deploying and managing microservice applications on top of the IBM Kubernetes Service (IKS) environment. Day 1 - App Modernization 101 \u00b6 Each topic in this workshop consists of a technical overview and a hands-on lab. DAY 1 - 8am to 3pm EST Welcome, Introductions & Objectives All 10 mins Lecture: Overview of Docker (with Demo) Remko de Knikker 20 mins Lecture: Overview of Kubernetes Remko de Knikker 30 mins Lab: Kubernetes 101 Remko de Knikker 60 mins BREAK (10am) 10 mins Lecture: Helm Rojan Jose 30 mins Lab: Helm 101 Rojan Jose 50 mins LUNCH (12pm) 60 mins Lecture: CI/CD Options for Kubernetes Rojan Jose 20 mins Lab: CI/CD with Jenkins 101 Rojan Jose 45 mins Lecture: 12 Factors Javier Torres 30 mins BREAK 10 mins Lecture: Application Modernization & Microservices Javier Torres 30 mins Lab: Deploying Microservices Javier Torres 30 mins Day 2 - App Modernization 201 \u00b6 Each topic in this workshop consists of a technical overview and a hands-on lab. DAY 2 - 8am to 3pm EST Welcome back, recap All 10 mins Lecture: Microservices Javier Torres 15 mins Lecture: Kubernetes Storage Rojan Jose 30 mins Lab: Container, File and Block Storage for Kubernetes Rojan Jose 90 mins BREAK (10:25am) 10 mins Lecture: Kubernetes Extensions Rojan Jose 35 mins Lab: Custom Resources and Operators Rojan Jose 60 mins LUNCH (12pm) 60 mins Lecture: Istio Remko de Knikker 45 mins Lab: Istio 101 Remko de Knikker 60 mins Wrap up All Technologies \u00b6 Docker IBM Kubernetes : 1.19 Helm IBM Kubernetes Service Jenkins Istio Credits \u00b6 Javier Torres Rojan Jose Remko De Knikker John Zaccone","title":"Workshop Overview"},{"location":"#iks-cloud-native-workshop","text":"Welcome to the IKS Cloud Native Workshop! In the workshop you will learn about foundational open source technologies and industry-wide accepted best practices for building modern, scalable, portable, and robust applications. You will learn about deploying and managing microservice applications on top of the IBM Kubernetes Service (IKS) environment.","title":"IKS Cloud Native Workshop"},{"location":"#day-1-app-modernization-101","text":"Each topic in this workshop consists of a technical overview and a hands-on lab. DAY 1 - 8am to 3pm EST Welcome, Introductions & Objectives All 10 mins Lecture: Overview of Docker (with Demo) Remko de Knikker 20 mins Lecture: Overview of Kubernetes Remko de Knikker 30 mins Lab: Kubernetes 101 Remko de Knikker 60 mins BREAK (10am) 10 mins Lecture: Helm Rojan Jose 30 mins Lab: Helm 101 Rojan Jose 50 mins LUNCH (12pm) 60 mins Lecture: CI/CD Options for Kubernetes Rojan Jose 20 mins Lab: CI/CD with Jenkins 101 Rojan Jose 45 mins Lecture: 12 Factors Javier Torres 30 mins BREAK 10 mins Lecture: Application Modernization & Microservices Javier Torres 30 mins Lab: Deploying Microservices Javier Torres 30 mins","title":"Day 1 - App Modernization 101"},{"location":"#day-2-app-modernization-201","text":"Each topic in this workshop consists of a technical overview and a hands-on lab. DAY 2 - 8am to 3pm EST Welcome back, recap All 10 mins Lecture: Microservices Javier Torres 15 mins Lecture: Kubernetes Storage Rojan Jose 30 mins Lab: Container, File and Block Storage for Kubernetes Rojan Jose 90 mins BREAK (10:25am) 10 mins Lecture: Kubernetes Extensions Rojan Jose 35 mins Lab: Custom Resources and Operators Rojan Jose 60 mins LUNCH (12pm) 60 mins Lecture: Istio Remko de Knikker 45 mins Lab: Istio 101 Remko de Knikker 60 mins Wrap up All","title":"Day 2 - App Modernization 201"},{"location":"#technologies","text":"Docker IBM Kubernetes : 1.19 Helm IBM Kubernetes Service Jenkins Istio","title":"Technologies"},{"location":"#credits","text":"Javier Torres Rojan Jose Remko De Knikker John Zaccone","title":"Credits"},{"location":"generatedContent/deploy-microservices/","text":"Introduction to Microservice \u00b6 Prerequisite \u00b6 Access to a Kubernetes cluster - A cluster was created when the session started IBM Cloud Function sending email notification - was created before the session Docker image of each service has been made available on Docker Hub. Setup \u00b6 Lab Environment Setup Every time when you start a new terminal/command window, steps in the section must be performed to setup a new environment. Follow the instructions here Lab 1 - Deploy Office Space sample application on Kubernetes \u00b6 This lab demonstrates how a sample cloud native application can be deployed on top of Kubernetes. Follow the lab instructions here Lab 2 - Develop a Java Microservice with Spring Boot \u00b6 Optionally, if your IBM Cloud account supported a paid Kubernetes cluster, you may perform the steps in this lab to develop a Java Microservice in no time. In this lab, you develop a Java Microservice using the Java Microservice with Spring starter kit. With this kit, you can have a basic Java microservice developed and deployed in a few minutes. You create a CI/CD pipeline in IBM Cloud which automates your service's deployment to Kubernetes cluster. Follow the lab instructions here","title":"Introduction to Microservice"},{"location":"generatedContent/deploy-microservices/#introduction-to-microservice","text":"","title":"Introduction to Microservice"},{"location":"generatedContent/deploy-microservices/#prerequisite","text":"Access to a Kubernetes cluster - A cluster was created when the session started IBM Cloud Function sending email notification - was created before the session Docker image of each service has been made available on Docker Hub.","title":"Prerequisite"},{"location":"generatedContent/deploy-microservices/#setup","text":"Lab Environment Setup Every time when you start a new terminal/command window, steps in the section must be performed to setup a new environment. Follow the instructions here","title":"Setup"},{"location":"generatedContent/deploy-microservices/#lab-1-deploy-office-space-sample-application-on-kubernetes","text":"This lab demonstrates how a sample cloud native application can be deployed on top of Kubernetes. Follow the lab instructions here","title":"Lab 1 - Deploy Office Space sample application on Kubernetes"},{"location":"generatedContent/deploy-microservices/#lab-2-develop-a-java-microservice-with-spring-boot","text":"Optionally, if your IBM Cloud account supported a paid Kubernetes cluster, you may perform the steps in this lab to develop a Java Microservice in no time. In this lab, you develop a Java Microservice using the Java Microservice with Spring starter kit. With this kit, you can have a basic Java microservice developed and deployed in a few minutes. You create a CI/CD pipeline in IBM Cloud which automates your service's deployment to Kubernetes cluster. Follow the lab instructions here","title":"Lab 2 - Develop a Java Microservice with Spring Boot"},{"location":"generatedContent/deploy-microservices/README_con_k8s/","text":"Lab 0c. Connect to your kubernetes environment \u00b6 Complete steps in this section to connect to your Kubernetes cluster. Open a terminal window or command window. Note: use a web terminal if you attend a workshop Set an environment variable export USERNAME=user### Note: replace ### with your assigned ID Install the kubernetes and container registry plugins: ibmcloud plugin install kubernetes-service -r Bluemix ibmcloud plugin install container-registry -r Bluemix Login to Kubernetes service in IBM Cloud . Select your Kubernetes cluster. Navigate to Access tab. In the above terminal window, complete all steps in the section After your cluster provisions, gain access .","title":"Lab 0c. Connect to your kubernetes environment"},{"location":"generatedContent/deploy-microservices/README_con_k8s/#lab-0c-connect-to-your-kubernetes-environment","text":"Complete steps in this section to connect to your Kubernetes cluster. Open a terminal window or command window. Note: use a web terminal if you attend a workshop Set an environment variable export USERNAME=user### Note: replace ### with your assigned ID Install the kubernetes and container registry plugins: ibmcloud plugin install kubernetes-service -r Bluemix ibmcloud plugin install container-registry -r Bluemix Login to Kubernetes service in IBM Cloud . Select your Kubernetes cluster. Navigate to Access tab. In the above terminal window, complete all steps in the section After your cluster provisions, gain access .","title":"Lab 0c. Connect to your kubernetes environment"},{"location":"generatedContent/deploy-microservices/README_deployment/","text":"Deploying Microservices \u00b6 Deploy Office Space sample application on Kubernetes \u00b6 In this session, we demonstrate how a sample cloud native application can be deployed on top of Kubernetes. This application, Office Space, mimicks the fictitious app idea from Michael Bolton in the movie Office Space . The app takes advantage of a financial program that computes interest for transactions by diverting fractions of a cent that are usually rounded off into a seperate bank account. The application includes a few services developed in different languages. The key coponent of the application is a Java 8/Spring Boot microservice that computes the interest then takes the fraction of the pennies to a database. Another Spring Boot microservice is the notification service. It sends email when the account balance reach more than $50,000. It is triggered by the Spring Boot webserver that computes the interest. The frontend user interafce is a Node.js application that shows the current account balance accumulated by the Spring Boot app. The backend uses a MySQL database to store the account balance. The transaction generator is a Python application that generates random transactions with accumulated interest. It's the last piece of your service mesh and used to simulate the transaction activities. The instructions were adapted from the more comprehensive tutorial found here - https://github.com/IBM/spring-boot-microservices-on-kubernetes. Develop Microservices \u00b6 During this session, we focus on deploying the microservices due to time constraint. Feel free to review the source code of the services, if you are interested. Couple of microservices were developed in Spring Boot which is one of the popular Java microservices framework. Spring Cloud has a rich set of well integrated Java libraries to address runtime concerns as part of the Java application stack, and Kubernetes provides a rich featureset to run polyglot microservices. Together these technologies complement each other and make a great platform for Spring Boot applications. Other microservices were developed in Node.js and Python . MySQL Database running in a separate container serves as persistent store. As a whole, the sample application delivers a native cloud architecture and follows 12 factors best practice. The source code are in the following subfolders containers/compute-interest-api containers/send-notification containers/account-summary containers/transaction-generator Flow \u00b6 The Transaction Generator service written in Python simulates transactions and pushes them to the Compute Interest microservice. The Compute Interest microservice computes the interest and then moves the fraction of pennies to the MySQL database to be stored. The database can be running within a container in the same deployment or on a public cloud such as IBM Cloud. The Compute Interest microservice then calls the notification service to notify the user when the total amount in the user\u2019s account reaches $50,000. The Notification service uses IBM Cloud Function to send an email message to the user. The front end user interface in Node.js retrieves the account balance and display. Included Components \u00b6 IBM Cloud Kubernetes Service : IBM Bluemix Container Service manages highly available apps inside Docker containers and Kubernetes clusters on the IBM Cloud. Compose for MySQL : Probably the most popular open source relational database in the world. IBM Cloud Functions : Execute code on demand in a highly scalable, serverless environment. Featured Technologies \u00b6 Container Orchestration : Automating the deployment, scaling and management of containerized applications. Databases : Repository for storing and managing collections of data. Serverless : An event-action platform that allows you to execute code in response to an event. Steps \u00b6 Clone the repo Modify send-notification.yaml file for email notification Deploy Database MySQL service Deploy Microservice compute-interest-api Deploy Microservice send-notification Deploy Microservice account-summary - the Frontend User Interface Deploy Microservice transaction-generator - the Transaction Generator service Access Your Application Each service in the application run in their containers. It has a Deployment and a Service. The deployment manages the pods started for each microservice. The Service creates a stable DNS entry for each microservice so they can reference their dependencies by name. 1. Clone the repo \u00b6 Clone this repository. In a terminal, run: cd ~ git clone https://github.com/lee-zhg/spring-boot-microservices-on-kubernetes cd spring-boot-microservices-on-kubernetes 2. Modify send-notification.yaml file for email notification \u00b6 Note: Additional Gmail security configurations may be required by Gmail to send/received email in this way. Optionally, if you like to send and receive email (gmail) notification, You will need to modify the environment variables in the send-notification.yaml file: env : - name : GMAIL_SENDER_USER value : 'username@gmail.com' # change this to the gmail that will send the email - name : GMAIL_SENDER_PASSWORD value : 'password' # change this to the the password of the gmail above - name : EMAIL_RECEIVER value : 'sendTo@gmail.com' # change this to the email of the receiver 3. Deploy MySQL Database \u00b6 Deploy MySQL database $ kubectl create -f account-database.yaml service \"account-database\" created deployment \"account-database\" created Create a secure storing credential of MySQL database Default credentials are already encoded in base64 in secrets.yaml. Note: Encoding in base64 does not encrypt or hide your secrets. Do not put this in your Github. $ kubectl apply -f secrets.yaml secret \"demo-credentials\" created 4. Deploy compute-interest-api service \u00b6 Microservice compute-interest-api is written in Spring Boot. It's deployed to your cluster as one component of your service mesh. $ kubectl apply -f compute-interest-api.yaml service \"compute-interest-api\" created deployment \"compute-interest-api\" created 5. Deploy send-notification service \u00b6 Microservice send-notification is written in Spring Boot. It's deployed to your cluster as one component of your service mesh. $ kubectl apply -f send-notification.yaml service \"send-notification\" created deployment \"send-notification\" created 6. Deploy account-summary service - the Frontend User Interface \u00b6 The Frontend User Interface is a Node.js app serving static files (HTML, CSS, JavaScript) that shows the total account balance. It's another component of your service mesh. $ kubectl apply -f account-summary.yaml service \"account-summary\" created deployment \"account-summary\" created 7. Deploy transaction-generator service - the Transaction Generator service \u00b6 The transaction generator is a Python application that generates random transactions with accumulated interest. It's the last piece of your service mesh. $ kubectl apply -f transaction-generator.yaml service \"transaction-generator\" created deployment \"transaction-generator\" created 8. Access Your Application \u00b6 One way to access your application is through Public IP and NodePort . Locate public IP address $ ibmcloud ks workers <your k8s cluster> ID Public IP Private IP Machine Type State Status kube-dal10-paac005a5fa6c44786b5dfb3ed8728548f-w1 169 .47.241.213 10 .177.155.13 free normal Ready Take note of Public IP . 169.47.241.213 in this example. Locate NodePort $ kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE ...... account-summary 10 .10.10.74 <nodes> 80 :30080/TCP 2d ...... Take note of PORT(S) of the service account-summary . 30080 in the example. Access your application To test your application, go to http://<Public IP>:<Port> in your browser. For example, http://169.47.241.213:30080 in this example. Explore the Kubernetes Dashboard \u00b6 Great, your application is deployed and working. There are quite a few microservices within your application. As your application is working, you would expect that all services are working. Are they? Let's find out. After you have deployed all services, the Kubernetes Dashboard can provide an overview of your application and its components. Login to IBM Cloud. Locate and select your Kubernetes cluster. Click the Kubernetes Daskboard button. Kubernetes Dashboard window opens. The charts in Workloads Statuses section on the Overview page provides a high level view of your cluster status. They are color-coded. RED indicates major issue. Explore section Deployments , Pods and Replica Sets , they all indicate that the service send-notification failed. Navigate to different pages in the Kubernetes Dashboard and you may find specific information that may be more interesting to you. Debug Deployment \u00b6 One of the cloud native architecture benefits is that your application can still function even individual services are not working. As your observed in the previous sections, your application appears working fine before you identified the down service in the Kubernetes Dashboard . In this section, you learn the very basic debugging technics in the Kubernetes Dashboard . Select the Overview page in the Kubernetes Dashboard . Select send-notification-xxxxxxx entry in the Replica Sets section. This opened send-notification-xxxxxxx entry in the Replica Sets page. Click the LOGS link on the top of the page. Scan the log entries and you should find a section similar to the one below. It shows that Could not resolve placeholder 'OPENWHISK_API_URL_SLACK' in value \"${OPENWHISK_API_URL_SLACK}\" . 2019-08-20 18:07:24.209 WARN 14 --- [ main] ationConfigEmbeddedWebApplicationContext : Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'triggerEmail': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'OPENWHISK_API_URL_SLACK' in value \"${OPENWHISK_API_URL_SLACK}\" 2019-08-20 18:07:24.212 INFO 14 --- [ main] o.apache.catalina.core.StandardService : Stopping service Tomcat 2019-08-20 18:07:24.228 INFO 14 --- [ main] utoConfigurationReportLoggingInitializer : Error starting ApplicationContext. To display the auto-configuration report re-run your application with 'debug' enabled. 2019-08-20 18:07:24.236 ERROR 14 --- [ main] o.s.boot.SpringApplication : Application startup failed org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'triggerEmail': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'OPENWHISK_API_URL_SLACK' in value \"${OPENWHISK_API_URL_SLACK}\" The service send-notification failed because it can't resolve environment variable OPENWHISK_API_URL_SLACK . There are multiple ways to fix the problem. If you prefer, you may edit the deployment.yaml file in the Kubernetes Dashboard window. For the lab exercise, you fix the problem in the original .yaml file. Locate and open file send-notification.yaml at the root folder of spring-boot-microservices-on-kubernetes repo that you downloaded. Locate the section below. --- - name: OPENWHISK_API_URL_SLACK --- value: '' --- - name: SLACK_MESSAGE --- value: '' Uncomment the entries by removing the leading --- . Save. Apply the changes. $ kubectl apply -f send-notification.yaml service/send-notification unchanged deployment.extensions/send-notification configured Go back to the Overview page of the Kubernetes Dashboard . All services are working now. Clean up \u00b6 To delete everything created during this session: kubectl delete svc,deploy -l app = office-space References \u00b6 John Zaccone - The original author of the office space app deployed via Docker . The Office Space app is based on the 1999 film that used that concept. License \u00b6 This code pattern is licensed under the Apache Software License, Version 2. Separate third party code objects invoked within this code pattern are licensed by their respective providers pursuant to their own separate licenses. Contributions are subject to the Developer Certificate of Origin, Version 1.1 (DCO) and the Apache Software License, Version 2 . Apache Software License (ASL) FAQ","title":"Lab 1. Deploy Microservices Application"},{"location":"generatedContent/deploy-microservices/README_deployment/#deploying-microservices","text":"","title":"Deploying Microservices"},{"location":"generatedContent/deploy-microservices/README_deployment/#deploy-office-space-sample-application-on-kubernetes","text":"In this session, we demonstrate how a sample cloud native application can be deployed on top of Kubernetes. This application, Office Space, mimicks the fictitious app idea from Michael Bolton in the movie Office Space . The app takes advantage of a financial program that computes interest for transactions by diverting fractions of a cent that are usually rounded off into a seperate bank account. The application includes a few services developed in different languages. The key coponent of the application is a Java 8/Spring Boot microservice that computes the interest then takes the fraction of the pennies to a database. Another Spring Boot microservice is the notification service. It sends email when the account balance reach more than $50,000. It is triggered by the Spring Boot webserver that computes the interest. The frontend user interafce is a Node.js application that shows the current account balance accumulated by the Spring Boot app. The backend uses a MySQL database to store the account balance. The transaction generator is a Python application that generates random transactions with accumulated interest. It's the last piece of your service mesh and used to simulate the transaction activities. The instructions were adapted from the more comprehensive tutorial found here - https://github.com/IBM/spring-boot-microservices-on-kubernetes.","title":"Deploy Office Space sample application on Kubernetes"},{"location":"generatedContent/deploy-microservices/README_deployment/#develop-microservices","text":"During this session, we focus on deploying the microservices due to time constraint. Feel free to review the source code of the services, if you are interested. Couple of microservices were developed in Spring Boot which is one of the popular Java microservices framework. Spring Cloud has a rich set of well integrated Java libraries to address runtime concerns as part of the Java application stack, and Kubernetes provides a rich featureset to run polyglot microservices. Together these technologies complement each other and make a great platform for Spring Boot applications. Other microservices were developed in Node.js and Python . MySQL Database running in a separate container serves as persistent store. As a whole, the sample application delivers a native cloud architecture and follows 12 factors best practice. The source code are in the following subfolders containers/compute-interest-api containers/send-notification containers/account-summary containers/transaction-generator","title":"Develop Microservices"},{"location":"generatedContent/deploy-microservices/README_deployment/#flow","text":"The Transaction Generator service written in Python simulates transactions and pushes them to the Compute Interest microservice. The Compute Interest microservice computes the interest and then moves the fraction of pennies to the MySQL database to be stored. The database can be running within a container in the same deployment or on a public cloud such as IBM Cloud. The Compute Interest microservice then calls the notification service to notify the user when the total amount in the user\u2019s account reaches $50,000. The Notification service uses IBM Cloud Function to send an email message to the user. The front end user interface in Node.js retrieves the account balance and display.","title":"Flow"},{"location":"generatedContent/deploy-microservices/README_deployment/#included-components","text":"IBM Cloud Kubernetes Service : IBM Bluemix Container Service manages highly available apps inside Docker containers and Kubernetes clusters on the IBM Cloud. Compose for MySQL : Probably the most popular open source relational database in the world. IBM Cloud Functions : Execute code on demand in a highly scalable, serverless environment.","title":"Included Components"},{"location":"generatedContent/deploy-microservices/README_deployment/#featured-technologies","text":"Container Orchestration : Automating the deployment, scaling and management of containerized applications. Databases : Repository for storing and managing collections of data. Serverless : An event-action platform that allows you to execute code in response to an event.","title":"Featured Technologies"},{"location":"generatedContent/deploy-microservices/README_deployment/#steps","text":"Clone the repo Modify send-notification.yaml file for email notification Deploy Database MySQL service Deploy Microservice compute-interest-api Deploy Microservice send-notification Deploy Microservice account-summary - the Frontend User Interface Deploy Microservice transaction-generator - the Transaction Generator service Access Your Application Each service in the application run in their containers. It has a Deployment and a Service. The deployment manages the pods started for each microservice. The Service creates a stable DNS entry for each microservice so they can reference their dependencies by name.","title":"Steps"},{"location":"generatedContent/deploy-microservices/README_deployment/#1-clone-the-repo","text":"Clone this repository. In a terminal, run: cd ~ git clone https://github.com/lee-zhg/spring-boot-microservices-on-kubernetes cd spring-boot-microservices-on-kubernetes","title":"1. Clone the repo"},{"location":"generatedContent/deploy-microservices/README_deployment/#2-modify-send-notificationyaml-file-for-email-notification","text":"Note: Additional Gmail security configurations may be required by Gmail to send/received email in this way. Optionally, if you like to send and receive email (gmail) notification, You will need to modify the environment variables in the send-notification.yaml file: env : - name : GMAIL_SENDER_USER value : 'username@gmail.com' # change this to the gmail that will send the email - name : GMAIL_SENDER_PASSWORD value : 'password' # change this to the the password of the gmail above - name : EMAIL_RECEIVER value : 'sendTo@gmail.com' # change this to the email of the receiver","title":"2. Modify send-notification.yaml file for email notification"},{"location":"generatedContent/deploy-microservices/README_deployment/#3-deploy-mysql-database","text":"Deploy MySQL database $ kubectl create -f account-database.yaml service \"account-database\" created deployment \"account-database\" created Create a secure storing credential of MySQL database Default credentials are already encoded in base64 in secrets.yaml. Note: Encoding in base64 does not encrypt or hide your secrets. Do not put this in your Github. $ kubectl apply -f secrets.yaml secret \"demo-credentials\" created","title":"3. Deploy MySQL Database"},{"location":"generatedContent/deploy-microservices/README_deployment/#4-deploy-compute-interest-api-service","text":"Microservice compute-interest-api is written in Spring Boot. It's deployed to your cluster as one component of your service mesh. $ kubectl apply -f compute-interest-api.yaml service \"compute-interest-api\" created deployment \"compute-interest-api\" created","title":"4. Deploy compute-interest-api service"},{"location":"generatedContent/deploy-microservices/README_deployment/#5-deploy-send-notification-service","text":"Microservice send-notification is written in Spring Boot. It's deployed to your cluster as one component of your service mesh. $ kubectl apply -f send-notification.yaml service \"send-notification\" created deployment \"send-notification\" created","title":"5. Deploy send-notification service"},{"location":"generatedContent/deploy-microservices/README_deployment/#6-deploy-account-summary-service-the-frontend-user-interface","text":"The Frontend User Interface is a Node.js app serving static files (HTML, CSS, JavaScript) that shows the total account balance. It's another component of your service mesh. $ kubectl apply -f account-summary.yaml service \"account-summary\" created deployment \"account-summary\" created","title":"6. Deploy account-summary service - the Frontend User Interface"},{"location":"generatedContent/deploy-microservices/README_deployment/#7-deploy-transaction-generator-service-the-transaction-generator-service","text":"The transaction generator is a Python application that generates random transactions with accumulated interest. It's the last piece of your service mesh. $ kubectl apply -f transaction-generator.yaml service \"transaction-generator\" created deployment \"transaction-generator\" created","title":"7. Deploy transaction-generator service - the Transaction Generator service"},{"location":"generatedContent/deploy-microservices/README_deployment/#8-access-your-application","text":"One way to access your application is through Public IP and NodePort . Locate public IP address $ ibmcloud ks workers <your k8s cluster> ID Public IP Private IP Machine Type State Status kube-dal10-paac005a5fa6c44786b5dfb3ed8728548f-w1 169 .47.241.213 10 .177.155.13 free normal Ready Take note of Public IP . 169.47.241.213 in this example. Locate NodePort $ kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE ...... account-summary 10 .10.10.74 <nodes> 80 :30080/TCP 2d ...... Take note of PORT(S) of the service account-summary . 30080 in the example. Access your application To test your application, go to http://<Public IP>:<Port> in your browser. For example, http://169.47.241.213:30080 in this example.","title":"8. Access Your Application"},{"location":"generatedContent/deploy-microservices/README_deployment/#explore-the-kubernetes-dashboard","text":"Great, your application is deployed and working. There are quite a few microservices within your application. As your application is working, you would expect that all services are working. Are they? Let's find out. After you have deployed all services, the Kubernetes Dashboard can provide an overview of your application and its components. Login to IBM Cloud. Locate and select your Kubernetes cluster. Click the Kubernetes Daskboard button. Kubernetes Dashboard window opens. The charts in Workloads Statuses section on the Overview page provides a high level view of your cluster status. They are color-coded. RED indicates major issue. Explore section Deployments , Pods and Replica Sets , they all indicate that the service send-notification failed. Navigate to different pages in the Kubernetes Dashboard and you may find specific information that may be more interesting to you.","title":"Explore the Kubernetes Dashboard"},{"location":"generatedContent/deploy-microservices/README_deployment/#debug-deployment","text":"One of the cloud native architecture benefits is that your application can still function even individual services are not working. As your observed in the previous sections, your application appears working fine before you identified the down service in the Kubernetes Dashboard . In this section, you learn the very basic debugging technics in the Kubernetes Dashboard . Select the Overview page in the Kubernetes Dashboard . Select send-notification-xxxxxxx entry in the Replica Sets section. This opened send-notification-xxxxxxx entry in the Replica Sets page. Click the LOGS link on the top of the page. Scan the log entries and you should find a section similar to the one below. It shows that Could not resolve placeholder 'OPENWHISK_API_URL_SLACK' in value \"${OPENWHISK_API_URL_SLACK}\" . 2019-08-20 18:07:24.209 WARN 14 --- [ main] ationConfigEmbeddedWebApplicationContext : Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'triggerEmail': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'OPENWHISK_API_URL_SLACK' in value \"${OPENWHISK_API_URL_SLACK}\" 2019-08-20 18:07:24.212 INFO 14 --- [ main] o.apache.catalina.core.StandardService : Stopping service Tomcat 2019-08-20 18:07:24.228 INFO 14 --- [ main] utoConfigurationReportLoggingInitializer : Error starting ApplicationContext. To display the auto-configuration report re-run your application with 'debug' enabled. 2019-08-20 18:07:24.236 ERROR 14 --- [ main] o.s.boot.SpringApplication : Application startup failed org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'triggerEmail': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'OPENWHISK_API_URL_SLACK' in value \"${OPENWHISK_API_URL_SLACK}\" The service send-notification failed because it can't resolve environment variable OPENWHISK_API_URL_SLACK . There are multiple ways to fix the problem. If you prefer, you may edit the deployment.yaml file in the Kubernetes Dashboard window. For the lab exercise, you fix the problem in the original .yaml file. Locate and open file send-notification.yaml at the root folder of spring-boot-microservices-on-kubernetes repo that you downloaded. Locate the section below. --- - name: OPENWHISK_API_URL_SLACK --- value: '' --- - name: SLACK_MESSAGE --- value: '' Uncomment the entries by removing the leading --- . Save. Apply the changes. $ kubectl apply -f send-notification.yaml service/send-notification unchanged deployment.extensions/send-notification configured Go back to the Overview page of the Kubernetes Dashboard . All services are working now.","title":"Debug Deployment"},{"location":"generatedContent/deploy-microservices/README_deployment/#clean-up","text":"To delete everything created during this session: kubectl delete svc,deploy -l app = office-space","title":"Clean up"},{"location":"generatedContent/deploy-microservices/README_deployment/#references","text":"John Zaccone - The original author of the office space app deployed via Docker . The Office Space app is based on the 1999 film that used that concept.","title":"References"},{"location":"generatedContent/deploy-microservices/README_deployment/#license","text":"This code pattern is licensed under the Apache Software License, Version 2. Separate third party code objects invoked within this code pattern are licensed by their respective providers pursuant to their own separate licenses. Contributions are subject to the Developer Certificate of Origin, Version 1.1 (DCO) and the Apache Software License, Version 2 . Apache Software License (ASL) FAQ","title":"License"},{"location":"generatedContent/deploy-microservices/README_java/","text":"Develop a Java Microservice with Spring Boot \u00b6 IBM Cloud provides Starter Kits as the starting point of quick cloud native development and deployment. For example * Java Microservice with Spring * Java Microservice with Eclipse MicroProfile and Java EE * Node.js Microservice with Express.js * Python Microservice with Flask In this session, you develop a Java Microservice using the Java Microservice with Spring starter kit. With this kit, you can have a basic Java microservice developed and deployed in a few minutes. You create a CI/CD pipeline in IBM Cloud which automates your service's deployment to Kubernetes cluster. The Java source code is available in the Git repo that is created as part of the CI/CD pipeline. It can serve as a starting point of your development project. You can download the repo to your local machine, develop and test your service locally, then check in the complete version of your project back to your Git repo. This will trigger a re-build and re-deployment process. The latest version of your microservice is deployed and running in Kubernetes cluster. Java Microservice with Spring Boot starter kit is used as an example to jump start a development project of Java Microservice . Other starter kits in the IBM Cloud work similarly. If you prefer other language or framework in your project, the steps in this session can still be applied. The instructions were adapted from the more comprehensive tutorial found here - https://cloud.ibm.com/docs/apps/tutorials?topic=creating-apps-tutorial-scratch#prereqs-scratch. Create a Java Microservice with Spring Boot \u00b6 To createa a Java Microservice with Spring Boot using the starter kit, Login to IBM Cloud. Select Catalog in your account dashboard. Go to Software tab. Search and select the Java Spring App . Click Create app . Accept the default settings and click Create . Create a Continuous Delivery pipeline \u00b6 Continuous Delivery automates builds, tests, and deployments of your service through Delivery Pipeline, GitLab, and more. By default, it is not enabled. To enable the continous delivery for your service, On the App details page, click the Deploy your app button in the Configure continuous delivery section. Select the IBM Cloud Kubernetes Service option. Click New to generate a new IBM Cloud API key . Click OK when prompted. Select the desired Region and Cluster . Select the Region to create your toolchain in, and then select the Resource group that provides access to your new toolchain. Accept the defaults for the rest of settings and Create . It may take a few minutes to complete the Continuous Delivery configuration. You can continue to the next step. The toolchain offers two main components. A Git repo is created for storing the source code of your service development project. It also provides version control service. A Deliver Pipeline is also created. It automates builds, tests, and deployments of your service. Click on the name of your toolchain. For example, JavaSpringAppXXXXXX to open the Delivery Pipeline window. The Delivery Pipeline should run through its stages. This may take a few minutes. After it completes, you should see Stage Passed messages for each stage. Verify your service \u00b6 Now, your microservice is running in your Kubernetes cluster on IBM Cloud. To find out how your servoice can be accessed, execute commands below $ ibmcloud ks workers <your cluster> $ kubectl get svc Locate the Public IP and Port(s) . To verify your microservice, enter http: : in a browser.","title":"Develop a Java Microservice with Spring Boot"},{"location":"generatedContent/deploy-microservices/README_java/#develop-a-java-microservice-with-spring-boot","text":"IBM Cloud provides Starter Kits as the starting point of quick cloud native development and deployment. For example * Java Microservice with Spring * Java Microservice with Eclipse MicroProfile and Java EE * Node.js Microservice with Express.js * Python Microservice with Flask In this session, you develop a Java Microservice using the Java Microservice with Spring starter kit. With this kit, you can have a basic Java microservice developed and deployed in a few minutes. You create a CI/CD pipeline in IBM Cloud which automates your service's deployment to Kubernetes cluster. The Java source code is available in the Git repo that is created as part of the CI/CD pipeline. It can serve as a starting point of your development project. You can download the repo to your local machine, develop and test your service locally, then check in the complete version of your project back to your Git repo. This will trigger a re-build and re-deployment process. The latest version of your microservice is deployed and running in Kubernetes cluster. Java Microservice with Spring Boot starter kit is used as an example to jump start a development project of Java Microservice . Other starter kits in the IBM Cloud work similarly. If you prefer other language or framework in your project, the steps in this session can still be applied. The instructions were adapted from the more comprehensive tutorial found here - https://cloud.ibm.com/docs/apps/tutorials?topic=creating-apps-tutorial-scratch#prereqs-scratch.","title":"Develop a Java Microservice with Spring Boot"},{"location":"generatedContent/deploy-microservices/README_java/#create-a-java-microservice-with-spring-boot","text":"To createa a Java Microservice with Spring Boot using the starter kit, Login to IBM Cloud. Select Catalog in your account dashboard. Go to Software tab. Search and select the Java Spring App . Click Create app . Accept the default settings and click Create .","title":"Create a Java Microservice with Spring Boot"},{"location":"generatedContent/deploy-microservices/README_java/#create-a-continuous-delivery-pipeline","text":"Continuous Delivery automates builds, tests, and deployments of your service through Delivery Pipeline, GitLab, and more. By default, it is not enabled. To enable the continous delivery for your service, On the App details page, click the Deploy your app button in the Configure continuous delivery section. Select the IBM Cloud Kubernetes Service option. Click New to generate a new IBM Cloud API key . Click OK when prompted. Select the desired Region and Cluster . Select the Region to create your toolchain in, and then select the Resource group that provides access to your new toolchain. Accept the defaults for the rest of settings and Create . It may take a few minutes to complete the Continuous Delivery configuration. You can continue to the next step. The toolchain offers two main components. A Git repo is created for storing the source code of your service development project. It also provides version control service. A Deliver Pipeline is also created. It automates builds, tests, and deployments of your service. Click on the name of your toolchain. For example, JavaSpringAppXXXXXX to open the Delivery Pipeline window. The Delivery Pipeline should run through its stages. This may take a few minutes. After it completes, you should see Stage Passed messages for each stage.","title":"Create a Continuous Delivery pipeline"},{"location":"generatedContent/deploy-microservices/README_java/#verify-your-service","text":"Now, your microservice is running in your Kubernetes cluster on IBM Cloud. To find out how your servoice can be accessed, execute commands below $ ibmcloud ks workers <your cluster> $ kubectl get svc Locate the Public IP and Port(s) . To verify your microservice, enter http: : in a browser.","title":"Verify your service"},{"location":"generatedContent/deploy-microservices/README_pre02/","text":"Lab Environment Setup \u00b6 Every time when you start a new terminal/command window, steps in the section must be performed to setup a new environment. Run these steps inside the console-in-a-browser environment provided by your instructor, or in a terminal/command window on your local machine. Open a new terminal ort command window. Login to the IBM Cloud. If prompted, enter user name and password. ibmcloud login List the clusters and locate the cluster corresponding to the userId you used to login to the console-in-a-browser environment. For example, if you are user028, your cluster will be user028-cluster. ibmcloud ks clusters Your Lite cluster should show up on the list. The cluster name should be in the format of user###-cluster , for example user003-cluster . Configure your Kubernetes client using this command. This will also configure your Kubernetes client for future login sessions by adding the command into your .bash_profile. eval $( ibmcloud ks cluster-config --cluster <your k8s cluster> --export | tee -a ~/.bash_profile ) If the command has a syntax error in your terminal (e.g. windows cmd shell), you may instead run the command ibmcloud ks cluster-config --cluster <your user>-cluster . Then, copy the output and execute it in the same terminal. You should be able to use kubectl to list kubernetes resources. Try getting the list of pods (there should be none yet) kubectl get pods No resources found. Login to the registry service ibmcloud cr login","title":"Lab Environment Setup"},{"location":"generatedContent/deploy-microservices/README_pre02/#lab-environment-setup","text":"Every time when you start a new terminal/command window, steps in the section must be performed to setup a new environment. Run these steps inside the console-in-a-browser environment provided by your instructor, or in a terminal/command window on your local machine. Open a new terminal ort command window. Login to the IBM Cloud. If prompted, enter user name and password. ibmcloud login List the clusters and locate the cluster corresponding to the userId you used to login to the console-in-a-browser environment. For example, if you are user028, your cluster will be user028-cluster. ibmcloud ks clusters Your Lite cluster should show up on the list. The cluster name should be in the format of user###-cluster , for example user003-cluster . Configure your Kubernetes client using this command. This will also configure your Kubernetes client for future login sessions by adding the command into your .bash_profile. eval $( ibmcloud ks cluster-config --cluster <your k8s cluster> --export | tee -a ~/.bash_profile ) If the command has a syntax error in your terminal (e.g. windows cmd shell), you may instead run the command ibmcloud ks cluster-config --cluster <your user>-cluster . Then, copy the output and execute it in the same terminal. You should be able to use kubectl to list kubernetes resources. Try getting the list of pods (there should be none yet) kubectl get pods No resources found. Login to the registry service ibmcloud cr login","title":"Lab Environment Setup"},{"location":"generatedContent/helm101/","text":"Helm 101 \u00b6 Helm is often described as the Kubernetes application package manager. So, what does Helm give you over using kubectl directly? Objectives \u00b6 These labs provide an insight on the advantages of using Helm over using Kubernetes directly through kubectl . In several of the labs there are two scenarios. The first scenario gives an example of how to perform the task using kubectl , the second scenario, using helm . When you complete all the labs, you'll: Understand the core concepts of Helm Understand the advantages of deployment using Helm over Kubernetes directly, looking at: Application management Updates Configuration Revision management Repositories and chart sharing Prerequisites \u00b6 Have a running Kubernetes cluster. See the IBM Cloud Kubernetes Service or Kubernetes Getting Started Guide for details about creating a cluster. Have Helm installed and initialized with the Kubernetes cluster. See Installing Helm on IBM Cloud Kubernetes Service or the Helm Quickstart Guide for getting started with Helm. Helm Overview \u00b6 Helm is a tool that streamlines installation and management of Kubernetes applications. It uses a packaging format called \"charts\", which are a collection of files that describe Kubernetes resources. It can run anywhere (laptop, CI/CD, etc.) and is available for various operating systems, like OSX, Linux and Windows. Helm 3 pivoted from the Helm 2 client-server architecture to a client architecture. The client is still called helm and, there is an improved Go library which encapsulates the Helm logic so that it can be leveraged by different clients. The client is a CLI which users interact with to perform different operations like install/upgrade/delete etc. The client interacts with the Kubernetes API server and the chart repository. It renders Helm template files into Kubernetes manifest files which it uses to perform operations on the Kubernetes cluster via the Kubernetes API. See the Helm Architecture for more details. A chart is organized as a collection of files inside of a directory where the directory name is the name of the chart. It contains template YAML files which facilitates providing configuration values at runtime and eliminates the need of modifying YAML files. These templates provide programming logic as they are based on the Go template language , functions from the Sprig lib and other specialized functions . The chart repository is a location where packaged charts can be stored and shared. This is akin to the image repository in Docker. Refer to The Chart Repository Guide for more details. Helm Abstractions \u00b6 Helm terms: Chart - It contains all of the resource definitions necessary to run an application, tool, or service inside of a Kubernetes cluster. A chart is basically a package of pre-configured Kubernetes resources. Config - Contains configuration information that can be merged into a packaged chart to create a releasable object. helm - Helm client. It renders charts into manifest files. It interacts directly with the Kubernetes API server to install, upgrade, query, and remove Kubernetes resources. Release - An instance of a chart running in a Kubernetes cluster. Repository - Place where charts reside and can be shared with others. To get started, head on over to Lab 1 .","title":"Helm 101"},{"location":"generatedContent/helm101/#helm-101","text":"Helm is often described as the Kubernetes application package manager. So, what does Helm give you over using kubectl directly?","title":"Helm 101"},{"location":"generatedContent/helm101/#objectives","text":"These labs provide an insight on the advantages of using Helm over using Kubernetes directly through kubectl . In several of the labs there are two scenarios. The first scenario gives an example of how to perform the task using kubectl , the second scenario, using helm . When you complete all the labs, you'll: Understand the core concepts of Helm Understand the advantages of deployment using Helm over Kubernetes directly, looking at: Application management Updates Configuration Revision management Repositories and chart sharing","title":"Objectives"},{"location":"generatedContent/helm101/#prerequisites","text":"Have a running Kubernetes cluster. See the IBM Cloud Kubernetes Service or Kubernetes Getting Started Guide for details about creating a cluster. Have Helm installed and initialized with the Kubernetes cluster. See Installing Helm on IBM Cloud Kubernetes Service or the Helm Quickstart Guide for getting started with Helm.","title":"Prerequisites"},{"location":"generatedContent/helm101/#helm-overview","text":"Helm is a tool that streamlines installation and management of Kubernetes applications. It uses a packaging format called \"charts\", which are a collection of files that describe Kubernetes resources. It can run anywhere (laptop, CI/CD, etc.) and is available for various operating systems, like OSX, Linux and Windows. Helm 3 pivoted from the Helm 2 client-server architecture to a client architecture. The client is still called helm and, there is an improved Go library which encapsulates the Helm logic so that it can be leveraged by different clients. The client is a CLI which users interact with to perform different operations like install/upgrade/delete etc. The client interacts with the Kubernetes API server and the chart repository. It renders Helm template files into Kubernetes manifest files which it uses to perform operations on the Kubernetes cluster via the Kubernetes API. See the Helm Architecture for more details. A chart is organized as a collection of files inside of a directory where the directory name is the name of the chart. It contains template YAML files which facilitates providing configuration values at runtime and eliminates the need of modifying YAML files. These templates provide programming logic as they are based on the Go template language , functions from the Sprig lib and other specialized functions . The chart repository is a location where packaged charts can be stored and shared. This is akin to the image repository in Docker. Refer to The Chart Repository Guide for more details.","title":"Helm Overview"},{"location":"generatedContent/helm101/#helm-abstractions","text":"Helm terms: Chart - It contains all of the resource definitions necessary to run an application, tool, or service inside of a Kubernetes cluster. A chart is basically a package of pre-configured Kubernetes resources. Config - Contains configuration information that can be merged into a packaged chart to create a releasable object. helm - Helm client. It renders charts into manifest files. It interacts directly with the Kubernetes API server to install, upgrade, query, and remove Kubernetes resources. Release - An instance of a chart running in a Kubernetes cluster. Repository - Place where charts reside and can be shared with others. To get started, head on over to Lab 1 .","title":"Helm Abstractions"},{"location":"generatedContent/helm101/SUMMARY/","text":"Summary \u00b6 Workshop \u00b6 Lab 0 Lab 1 Lab 2 Lab 3 Lab 4 Resources \u00b6 IBM Developer","title":"Summary"},{"location":"generatedContent/helm101/SUMMARY/#summary","text":"","title":"Summary"},{"location":"generatedContent/helm101/SUMMARY/#workshop","text":"Lab 0 Lab 1 Lab 2 Lab 3 Lab 4","title":"Workshop"},{"location":"generatedContent/helm101/SUMMARY/#resources","text":"IBM Developer","title":"Resources"},{"location":"generatedContent/helm101/Lab0/","text":"Lab 0. Installing Helm on IBM Cloud Kubernetes Service \u00b6 The Helm client ( helm ) can be installed from source or pre-built binary releases. In this lab, we are going to use the pre-built binary release (Linux amd64) from the Helm community. Refer to the Helm install docs for more details. Prerequisites \u00b6 Create a Kubernetes cluster with IBM Cloud Kubernetes Service , following the steps to also configure the IBM Cloud CLI with the Kubernetes Service plug-in. Installing the Helm Client (helm) \u00b6 Download the latest release of Helm v3 for your environment, the steps below are for Linux amd64 , adjust the examples as needed for your environment. Unpack it: $ tar -zxvf helm-v3.<x>.<y>-linux-amd64.tgz . Find the helm binary in the unpacked directory, and move it to its desired location: mv linux-amd64/helm /usr/local/bin/helm . It is best if the location you copy to is pathed, as it avoids having to path the helm commands. The Helm client is now installed and can be tested with the command, helm help . Conclusion \u00b6 You are now ready to start using Helm.","title":"Lab 0. Installing Helm on IBM Cloud Kubernetes Service"},{"location":"generatedContent/helm101/Lab0/#lab-0-installing-helm-on-ibm-cloud-kubernetes-service","text":"The Helm client ( helm ) can be installed from source or pre-built binary releases. In this lab, we are going to use the pre-built binary release (Linux amd64) from the Helm community. Refer to the Helm install docs for more details.","title":"Lab 0. Installing Helm on IBM Cloud Kubernetes Service"},{"location":"generatedContent/helm101/Lab0/#prerequisites","text":"Create a Kubernetes cluster with IBM Cloud Kubernetes Service , following the steps to also configure the IBM Cloud CLI with the Kubernetes Service plug-in.","title":"Prerequisites"},{"location":"generatedContent/helm101/Lab0/#installing-the-helm-client-helm","text":"Download the latest release of Helm v3 for your environment, the steps below are for Linux amd64 , adjust the examples as needed for your environment. Unpack it: $ tar -zxvf helm-v3.<x>.<y>-linux-amd64.tgz . Find the helm binary in the unpacked directory, and move it to its desired location: mv linux-amd64/helm /usr/local/bin/helm . It is best if the location you copy to is pathed, as it avoids having to path the helm commands. The Helm client is now installed and can be tested with the command, helm help .","title":"Installing the Helm Client (helm)"},{"location":"generatedContent/helm101/Lab0/#conclusion","text":"You are now ready to start using Helm.","title":"Conclusion"},{"location":"generatedContent/helm101/Lab1/","text":"Lab 1. Deploy with Helm \u00b6 Let's investigate how Helm can help us focus on other things by letting a chart do the work for us. We'll first deploy an application to a Kubernetes cluster by using kubectl and then show how we can offload the work to a chart by deploying the same app with Helm. The application is the Guestbook App , which is a sample multi-tier web application. Scenario 1: Deploy the application using kubectl \u00b6 In this part of the lab, we will deploy the application using the Kubernetes client kubectl . We will use Version 1 of the app for deploying here. If you already have a copy of the guestbook application installed from the kube101 lab , skip this section and go the helm example in Scenario 2 . Clone the Guestbook App repo to get the files: git clone https://github.com/IBM/guestbook.git Use the configuration files in the cloned Git repository to deploy the containers and create services for them by using the following commands: $ cd guestbook/v1 $ kubectl create -f redis-master-deployment.yaml deployment.apps/redis-master created $ kubectl create -f redis-master-service.yaml service/redis-master created $ kubectl create -f redis-slave-deployment.yaml deployment.apps/redis-slave created $ kubectl create -f redis-slave-service.yaml service/redis-slave created $ kubectl create -f guestbook-deployment.yaml deployment.apps/guestbook-v1 created $ kubectl create -f guestbook-service.yaml service/guestbook created Refer to the guestbook README for more details. View the guestbook: You can now play with the guestbook that you just created by opening it in a browser (it might take a few moments for the guestbook to come up). Local Host: If you are running Kubernetes locally, view the guestbook by navigating to http://localhost:3000 in your browser. Remote Host: To view the guestbook on a remote host, locate the external IP and port of the load balancer in the EXTERNAL-IP and PORTS columns of the $ kubectl get services output. $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE guestbook LoadBalancer 172.21.252.107 50.23.5.136 3000:31838/TCP 14m redis-master ClusterIP 172.21.97.222 <none> 6379/TCP 14m redis-slave ClusterIP 172.21.43.70 <none> 6379/TCP 14m ......... In this scenario the URL is http://50.23.5.136:31838 . Note: If no external IP is assigned, then you can get the external IP with the following command: $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.47.122.98 Ready <none> 1h v1.10.11+IKS 173.193.92.112 Ubuntu 16.04.5 LTS 4.4.0-141-generic docker://18.6.1 In this scenario the URL is http://173.193.92.112:31838 . Navigate to the output given (for example http://50.23.5.136:31838 ) in your browser. You should see the guestbook now displaying in your browser: Scenario 2: Deploy the application using Helm \u00b6 In this part of the lab, we will deploy the application by using Helm. We will set a release name of guestbook-demo to distinguish it from the previous deployment. The Helm chart is available here . Clone the Helm 101 repo to get the files: git clone https://github.com/IBM/helm101 A chart is defined as a collection of files that describe a related set of Kubernetes resources. We probably then should take a look at the the files before we go and install the chart. The files for the guestbook chart are as follows: . \u251c\u2500\u2500 Chart.yaml \\\\ A YAML file containing information about the chart \u251c\u2500\u2500 LICENSE \\\\ A plain text file containing the license for the chart \u251c\u2500\u2500 README.md \\\\ A README providing information about the chart usage, configuration, installation etc. \u251c\u2500\u2500 templates \\\\ A directory of templates that will generate valid Kubernetes manifest files when combined with values.yaml \u2502 \u251c\u2500\u2500 _helpers.tpl \\\\ Template helpers/definitions that are re-used throughout the chart \u2502 \u251c\u2500\u2500 guestbook-deployment.yaml \\\\ Guestbook app container resource \u2502 \u251c\u2500\u2500 guestbook-service.yaml \\\\ Guestbook app service resource \u2502 \u251c\u2500\u2500 NOTES.txt \\\\ A plain text file containing short usage notes about how to access the app post install \u2502 \u251c\u2500\u2500 redis-master-deployment.yaml \\\\ Redis master container resource \u2502 \u251c\u2500\u2500 redis-master-service.yaml \\\\ Redis master service resource \u2502 \u251c\u2500\u2500 redis-slave-deployment.yaml \\\\ Redis slave container resource \u2502 \u2514\u2500\u2500 redis-slave-service.yaml \\\\ Redis slave service resource \u2514\u2500\u2500 values.yaml \\\\ The default configuration values for the chart Note: The template files shown above will be rendered into Kubernetes manifest files before being passed to the Kubernetes API server. Therefore, they map to the manifest files that we deployed when we used kubectl (minus the helper and notes files). Let's go ahead and install the chart now. If the helm-demo namespace does not exist, you will need to create it using: kubectl create namespace helm-demo Install the app as a Helm chart: $ cd helm101/charts $ helm install guestbook-demo ./guestbook/ --namespace helm-demo NAME: guestbook-demo ... You should see output similar to the following: NAME: guestbook-demo LAST DEPLOYED: Mon Feb 24 18:08:02 2020 NAMESPACE: helm-demo STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: 1. Get the application URL by running these commands: NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running 'kubectl get svc -w guestbook-demo --namespace helm-demo' export SERVICE_IP=$(kubectl get svc --namespace helm-demo guestbook-demo -o jsonpath='{.status.loadBalancer.ingress[0].ip}') echo http://$SERVICE_IP:3000 The chart install performs the Kubernetes deployments and service creations of the redis master and slaves, and the guestbook app, as one. This is because the chart is a collection of files that describe a related set of Kubernetes resources and Helm manages the creation of these resources via the Kubernetes API. Check the deployment: kubectl get deployment guestbook-demo --namespace helm-demo You should see output similar to the following: $ kubectl get deployment guestbook-demo --namespace helm-dem NAME READY UP-TO-DATE AVAILABLE AGE guestbook-demo 2/2 2 2 51m To check the status of the running application pods, use: kubectl get pods --namespace helm-demo You should see output similar to the following: $ kubectl get pods --namespace helm-demo NAME READY STATUS RESTARTS AGE guestbook-demo-6c9cf8b9-jwbs9 1/1 Running 0 52m guestbook-demo-6c9cf8b9-qk4fb 1/1 Running 0 52m redis-master-5d8b66464f-j72jf 1/1 Running 0 52m redis-slave-586b4c847c-2xt99 1/1 Running 0 52m redis-slave-586b4c847c-q7rq5 1/1 Running 0 52m To check the services, use: kubectl get services --namespace helm-demo $ kubectl get services --namespace helm-demo NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE guestbook-demo LoadBalancer 172.21.43.244 <pending> 3000:31367/TCP 52m redis-master ClusterIP 172.21.12.43 <none> 6379/TCP 52m redis-slave ClusterIP 172.21.176.148 <none> 6379/TCP 52m View the guestbook: You can now play with the guestbook that you just created by opening it in a browser (it might take a few moments for the guestbook to come up). Local Host: If you are running Kubernetes locally, view the guestbook by navigating to http://localhost:3000 in your browser. Remote Host: To view the guestbook on a remote host, locate the external IP and the port of the load balancer by following the \"NOTES\" section in the install output. The commands will be similar to the following: $ export SERVICE_IP = $( kubectl get svc --namespace helm-demo guestbook-demo -o jsonpath = '{.status.loadBalancer.ingress[0].ip}' ) $ echo http:// $SERVICE_IP http://50.23.5.136 Combine the service IP with the port of the service printed earlier. In this scenario the URL is http://50.23.5.136:31367 . Note: If no external IP is assigned, then you can get the external IP with the following command: $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.47.122.98 Ready <none> 1h v1.10.11+IKS 173.193.92.112 Ubuntu 16.04.5 LTS 4.4.0-141-generic docker://18.6.1 In this scenario the URL is http://173.193.92.112:31367 . Navigate to the output given (for example http://50.23.5.136:31367 ) in your browser. You should see the guestbook now displaying in your browser: Conclusion \u00b6 Congratulations, you have now deployed an application by using two different methods to Kubernetes! From this lab, you can see that using Helm required less commands and less to think about (by giving it the chart path and not the individual files) versus using kubectl . Helm's application management provides the user with this simplicity. Move on to the next lab, Lab2 , to learn how to update our running app when the chart has been changed.","title":"Lab 1. Deploy with Helm"},{"location":"generatedContent/helm101/Lab1/#lab-1-deploy-with-helm","text":"Let's investigate how Helm can help us focus on other things by letting a chart do the work for us. We'll first deploy an application to a Kubernetes cluster by using kubectl and then show how we can offload the work to a chart by deploying the same app with Helm. The application is the Guestbook App , which is a sample multi-tier web application.","title":"Lab 1. Deploy with Helm"},{"location":"generatedContent/helm101/Lab1/#scenario-1-deploy-the-application-using-kubectl","text":"In this part of the lab, we will deploy the application using the Kubernetes client kubectl . We will use Version 1 of the app for deploying here. If you already have a copy of the guestbook application installed from the kube101 lab , skip this section and go the helm example in Scenario 2 . Clone the Guestbook App repo to get the files: git clone https://github.com/IBM/guestbook.git Use the configuration files in the cloned Git repository to deploy the containers and create services for them by using the following commands: $ cd guestbook/v1 $ kubectl create -f redis-master-deployment.yaml deployment.apps/redis-master created $ kubectl create -f redis-master-service.yaml service/redis-master created $ kubectl create -f redis-slave-deployment.yaml deployment.apps/redis-slave created $ kubectl create -f redis-slave-service.yaml service/redis-slave created $ kubectl create -f guestbook-deployment.yaml deployment.apps/guestbook-v1 created $ kubectl create -f guestbook-service.yaml service/guestbook created Refer to the guestbook README for more details. View the guestbook: You can now play with the guestbook that you just created by opening it in a browser (it might take a few moments for the guestbook to come up). Local Host: If you are running Kubernetes locally, view the guestbook by navigating to http://localhost:3000 in your browser. Remote Host: To view the guestbook on a remote host, locate the external IP and port of the load balancer in the EXTERNAL-IP and PORTS columns of the $ kubectl get services output. $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE guestbook LoadBalancer 172.21.252.107 50.23.5.136 3000:31838/TCP 14m redis-master ClusterIP 172.21.97.222 <none> 6379/TCP 14m redis-slave ClusterIP 172.21.43.70 <none> 6379/TCP 14m ......... In this scenario the URL is http://50.23.5.136:31838 . Note: If no external IP is assigned, then you can get the external IP with the following command: $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.47.122.98 Ready <none> 1h v1.10.11+IKS 173.193.92.112 Ubuntu 16.04.5 LTS 4.4.0-141-generic docker://18.6.1 In this scenario the URL is http://173.193.92.112:31838 . Navigate to the output given (for example http://50.23.5.136:31838 ) in your browser. You should see the guestbook now displaying in your browser:","title":"Scenario 1: Deploy the application using kubectl"},{"location":"generatedContent/helm101/Lab1/#scenario-2-deploy-the-application-using-helm","text":"In this part of the lab, we will deploy the application by using Helm. We will set a release name of guestbook-demo to distinguish it from the previous deployment. The Helm chart is available here . Clone the Helm 101 repo to get the files: git clone https://github.com/IBM/helm101 A chart is defined as a collection of files that describe a related set of Kubernetes resources. We probably then should take a look at the the files before we go and install the chart. The files for the guestbook chart are as follows: . \u251c\u2500\u2500 Chart.yaml \\\\ A YAML file containing information about the chart \u251c\u2500\u2500 LICENSE \\\\ A plain text file containing the license for the chart \u251c\u2500\u2500 README.md \\\\ A README providing information about the chart usage, configuration, installation etc. \u251c\u2500\u2500 templates \\\\ A directory of templates that will generate valid Kubernetes manifest files when combined with values.yaml \u2502 \u251c\u2500\u2500 _helpers.tpl \\\\ Template helpers/definitions that are re-used throughout the chart \u2502 \u251c\u2500\u2500 guestbook-deployment.yaml \\\\ Guestbook app container resource \u2502 \u251c\u2500\u2500 guestbook-service.yaml \\\\ Guestbook app service resource \u2502 \u251c\u2500\u2500 NOTES.txt \\\\ A plain text file containing short usage notes about how to access the app post install \u2502 \u251c\u2500\u2500 redis-master-deployment.yaml \\\\ Redis master container resource \u2502 \u251c\u2500\u2500 redis-master-service.yaml \\\\ Redis master service resource \u2502 \u251c\u2500\u2500 redis-slave-deployment.yaml \\\\ Redis slave container resource \u2502 \u2514\u2500\u2500 redis-slave-service.yaml \\\\ Redis slave service resource \u2514\u2500\u2500 values.yaml \\\\ The default configuration values for the chart Note: The template files shown above will be rendered into Kubernetes manifest files before being passed to the Kubernetes API server. Therefore, they map to the manifest files that we deployed when we used kubectl (minus the helper and notes files). Let's go ahead and install the chart now. If the helm-demo namespace does not exist, you will need to create it using: kubectl create namespace helm-demo Install the app as a Helm chart: $ cd helm101/charts $ helm install guestbook-demo ./guestbook/ --namespace helm-demo NAME: guestbook-demo ... You should see output similar to the following: NAME: guestbook-demo LAST DEPLOYED: Mon Feb 24 18:08:02 2020 NAMESPACE: helm-demo STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: 1. Get the application URL by running these commands: NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running 'kubectl get svc -w guestbook-demo --namespace helm-demo' export SERVICE_IP=$(kubectl get svc --namespace helm-demo guestbook-demo -o jsonpath='{.status.loadBalancer.ingress[0].ip}') echo http://$SERVICE_IP:3000 The chart install performs the Kubernetes deployments and service creations of the redis master and slaves, and the guestbook app, as one. This is because the chart is a collection of files that describe a related set of Kubernetes resources and Helm manages the creation of these resources via the Kubernetes API. Check the deployment: kubectl get deployment guestbook-demo --namespace helm-demo You should see output similar to the following: $ kubectl get deployment guestbook-demo --namespace helm-dem NAME READY UP-TO-DATE AVAILABLE AGE guestbook-demo 2/2 2 2 51m To check the status of the running application pods, use: kubectl get pods --namespace helm-demo You should see output similar to the following: $ kubectl get pods --namespace helm-demo NAME READY STATUS RESTARTS AGE guestbook-demo-6c9cf8b9-jwbs9 1/1 Running 0 52m guestbook-demo-6c9cf8b9-qk4fb 1/1 Running 0 52m redis-master-5d8b66464f-j72jf 1/1 Running 0 52m redis-slave-586b4c847c-2xt99 1/1 Running 0 52m redis-slave-586b4c847c-q7rq5 1/1 Running 0 52m To check the services, use: kubectl get services --namespace helm-demo $ kubectl get services --namespace helm-demo NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE guestbook-demo LoadBalancer 172.21.43.244 <pending> 3000:31367/TCP 52m redis-master ClusterIP 172.21.12.43 <none> 6379/TCP 52m redis-slave ClusterIP 172.21.176.148 <none> 6379/TCP 52m View the guestbook: You can now play with the guestbook that you just created by opening it in a browser (it might take a few moments for the guestbook to come up). Local Host: If you are running Kubernetes locally, view the guestbook by navigating to http://localhost:3000 in your browser. Remote Host: To view the guestbook on a remote host, locate the external IP and the port of the load balancer by following the \"NOTES\" section in the install output. The commands will be similar to the following: $ export SERVICE_IP = $( kubectl get svc --namespace helm-demo guestbook-demo -o jsonpath = '{.status.loadBalancer.ingress[0].ip}' ) $ echo http:// $SERVICE_IP http://50.23.5.136 Combine the service IP with the port of the service printed earlier. In this scenario the URL is http://50.23.5.136:31367 . Note: If no external IP is assigned, then you can get the external IP with the following command: $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.47.122.98 Ready <none> 1h v1.10.11+IKS 173.193.92.112 Ubuntu 16.04.5 LTS 4.4.0-141-generic docker://18.6.1 In this scenario the URL is http://173.193.92.112:31367 . Navigate to the output given (for example http://50.23.5.136:31367 ) in your browser. You should see the guestbook now displaying in your browser:","title":"Scenario 2: Deploy the application using Helm"},{"location":"generatedContent/helm101/Lab1/#conclusion","text":"Congratulations, you have now deployed an application by using two different methods to Kubernetes! From this lab, you can see that using Helm required less commands and less to think about (by giving it the chart path and not the individual files) versus using kubectl . Helm's application management provides the user with this simplicity. Move on to the next lab, Lab2 , to learn how to update our running app when the chart has been changed.","title":"Conclusion"},{"location":"generatedContent/helm101/Lab2/","text":"Lab 2. Make changes with Helm \u00b6 In Lab 1 , we installed the guestbook sample app by using Helm and saw the benefits over using kubectl . You probably think that you're done and know enough to use Helm. But what about updates or improvements to the chart? How do you update your running app to pick up these changes? In this lab, we're going to look at how to update our running app when the chart has been changed. To demonstrate this, we're going to make changes to the original guestbook chart by: Removing the Redis slaves and using just the in-memory DB Changing the type from LoadBalancer to NodePort . It seems contrived but the goal of this lab is to show you how to update your apps with Kubernetes and Helm. So, how easy is it to do this? Let's take a look below. Scenario 1: Update the application using kubectl \u00b6 In this part of the lab we will update the previously deployed application Guestbook , using Kubernetes directly. This is an optional step that is not technically required to update your running app. The reason for doing this step is \"house keeping\" - you want to have the correct files for the current configuration that you have deployed. This avoids making mistakes if you have future updates or even rollbacks. In this updated configuration, we remove the Redis slaves. To have the directory match the configuration, move/archive or simply remove the Redis slave files from the guestbook repo tree: cd guestbook/v1 rm redis-slave-service.yaml rm redis-slave-deployment.yaml Note: you can reclaim these files later with a git checkout -- <filename> command, if desired Delete the Redis slave service and pods: $ kubectl delete svc redis-slave --namespace default service \"redis-slave\" deleted $ kubectl delete deployment redis-slave --namespace default deployment.extensions \"redis-slave\" deleted Update the guestbook service from LoadBalancer to NodePort type: sed -i.bak 's/LoadBalancer/NodePort/g' guestbook-service.yaml Note: you can reset the files later with a git checkout -- <filename> command, if desired Delete the guestbook service: kubectl delete svc guestbook --namespace default Re-create the service with NodePort type: kubectl create -f guestbook-service.yaml Check the updates, using kubectl get all --namespace default $ kubectl get all --namespace default NAME READY STATUS RESTARTS AGE pod/guestbook-v1-7fc76dc46-9r4s7 1/1 Running 0 1h pod/guestbook-v1-7fc76dc46-hspnk 1/1 Running 0 1h pod/guestbook-v1-7fc76dc46-sxzkt 1/1 Running 0 1h pod/redis-master-5d8b66464f-pvbl9 1/1 Running 0 1h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook NodePort 172.21.45.29 <none> 3000:31989/TCP 31s service/kubernetes ClusterIP 172.21.0.1 <none> 443/TCP 9d service/redis-master ClusterIP 172.21.232.61 <none> 6379/TCP 1h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-demo 3/3 3 3 1h deployment.apps/redis-master 1/1 1 1 1h NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-v1-7fc76dc46 3 3 3 1h replicaset.apps/redis-master-5d8b66464f 1 1 1 1h Note: The service type has changed (to NodePort ) and a new port has been allocated ( 31989 in this output case) to the guestbook service. All redis-slave resources have been removed. View the guestbook Get the public IP of one of your nodes: kubectl get nodes -o wide Navigate to the IP address plus the node port that printed earlier. Scenario 2: Update the application using Helm \u00b6 In this section, we'll update the previously deployed guestbook-demo application by using Helm. Before we start, let's take a few minutes to see how Helm simplifies the process compared to using Kubernetes directly. Helm's use of a template language provides great flexibility and power to chart authors, which removes the complexity to the chart user. In the guestbook example, we'll use the following capabilities of templating: Values: An object that provides access to the values passed into the chart. An example of this is in guestbook-service , which contains the line type: {{ .Values.service.type }} . This line provides the capability to set the service type during an upgrade or install. Control structures: Also called \u201cactions\u201d in template parlance, control structures provide the template author with the ability to control the flow of a template\u2019s generation. An example of this is in redis-slave-service , which contains the line {{- if .Values.redis.slaveEnabled -}} . This line allows us to enable/disable the REDIS master/slave during an upgrade or install. The complete redis-slave-service.yaml file shown below, demonstrates how the file becomes redundant when the slaveEnabled flag is disabled and also how the port value is set. There are more examples of templating functionality in the other chart files. {{ - if .Values.redis.slaveEnabled - }} apiVersion : v1 kind : Service metadata : name : redis-slave labels : app : redis role : slave spec : ports : - port : {{ .Values.redis.port }} targetPort : redis-server selector : app : redis role : slave {{ - end }} Enough talking about the theory. Now let's give it a go! First, lets check the app we deployed in Lab 1 with Helm. This can be done by checking the Helm releases: helm list -n helm-demo Note that we specify the namespace. If not specified, it uses the current namespace context. You should see output similar to the following: $ helm list -n helm-demo NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION guestbook-demo helm-demo 1 2020-02-24 18:08:02.017401264 +0000 UTC deployed guestbook-0.2.0 The list command provides the list of deployed charts (releases) giving information of chart version, namespace, number of updates (revisions) etc. We now know the release is there from step 1., so we can update the application: $ cd helm101/charts $ helm upgrade guestbook-demo ./guestbook --set redis.slaveEnabled = false,service.type = NodePort --namespace helm-demo Release \"guestbook-demo\" has been upgraded. Happy Helming! ... A Helm upgrade takes an existing release and upgrades it according to the information you provide. You should see output similar to the following: $ helm upgrade guestbook-demo ./guestbook --set redis.slaveEnabled = false,service.type = NodePort --namespace helm-demo Release \"guestbook-demo\" has been upgraded. Happy Helming! NAME: guestbook-demo LAST DEPLOYED: Tue Feb 25 14:23:27 2020 NAMESPACE: helm-demo STATUS: deployed REVISION: 2 TEST SUITE: None NOTES: 1. Get the application URL by running these commands: export NODE_PORT=$(kubectl get --namespace helm-demo -o jsonpath=\"{.spec.ports[0].nodePort}\" services guestbook-demo) export NODE_IP=$(kubectl get nodes --namespace helm-demo -o jsonpath=\"{.items[0].status.addresses[0].address}\") echo http://$NODE_IP:$NODE_PORT The upgrade command upgrades the app to a specified version of a chart, removes the redis-slave resources, and updates the app service.type to NodePort . Check the updates, using kubectl get all --namespace helm-demo : $ kubectl get all --namespace helm-demo NAME READY STATUS RESTARTS AGE pod/guestbook-demo-6c9cf8b9-dhqk9 1/1 Running 0 20h pod/guestbook-demo-6c9cf8b9-zddn2 1/1 Running 0 20h pod/redis-master-5d8b66464f-g7sh6 1/1 Running 0 20h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook-demo NodePort 172.21.43.244 <none> 3000:31202/TCP 20h service/redis-master ClusterIP 172.21.12.43 <none> 6379/TCP 20h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-demo 2/2 2 2 20h deployment.apps/redis-master 1/1 1 1 20h NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-demo-6c9cf8b9 2 2 2 20h replicaset.apps/redis-master-5d8b66464f 1 1 1 20h Note: The service type has changed (to NodePort ) and a new port has been allocated ( 31202 in this output case) to the guestbook service. All redis-slave resources have been removed. When you check the Helm release with helm list -n helm-demo , you will see the revision and date has been updated: $ helm list -n helm-demo NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION guestbook-demo helm-demo 2 2020-02-25 14:23:27.06732381 +0000 UTC deployed guestbook-0.2.0 View the guestbook Get the public IP of one of your nodes: kubectl get nodes -o wide Navigate to the IP address plus the node port that printed earlier. Conclusion \u00b6 Congratulations, you have now updated the applications! Helm does not require any manual changing of resources and is therefore so much easier to upgrade! All configurations can be set on the fly on the command line or by using override files. This is made possible from when the logic was added to the template files, which enables or disables the capability, depending on the flag set. Check out Lab 3 to get an insight into revision management.","title":"Lab 2. Make changes with Helm"},{"location":"generatedContent/helm101/Lab2/#lab-2-make-changes-with-helm","text":"In Lab 1 , we installed the guestbook sample app by using Helm and saw the benefits over using kubectl . You probably think that you're done and know enough to use Helm. But what about updates or improvements to the chart? How do you update your running app to pick up these changes? In this lab, we're going to look at how to update our running app when the chart has been changed. To demonstrate this, we're going to make changes to the original guestbook chart by: Removing the Redis slaves and using just the in-memory DB Changing the type from LoadBalancer to NodePort . It seems contrived but the goal of this lab is to show you how to update your apps with Kubernetes and Helm. So, how easy is it to do this? Let's take a look below.","title":"Lab 2. Make changes with Helm"},{"location":"generatedContent/helm101/Lab2/#scenario-1-update-the-application-using-kubectl","text":"In this part of the lab we will update the previously deployed application Guestbook , using Kubernetes directly. This is an optional step that is not technically required to update your running app. The reason for doing this step is \"house keeping\" - you want to have the correct files for the current configuration that you have deployed. This avoids making mistakes if you have future updates or even rollbacks. In this updated configuration, we remove the Redis slaves. To have the directory match the configuration, move/archive or simply remove the Redis slave files from the guestbook repo tree: cd guestbook/v1 rm redis-slave-service.yaml rm redis-slave-deployment.yaml Note: you can reclaim these files later with a git checkout -- <filename> command, if desired Delete the Redis slave service and pods: $ kubectl delete svc redis-slave --namespace default service \"redis-slave\" deleted $ kubectl delete deployment redis-slave --namespace default deployment.extensions \"redis-slave\" deleted Update the guestbook service from LoadBalancer to NodePort type: sed -i.bak 's/LoadBalancer/NodePort/g' guestbook-service.yaml Note: you can reset the files later with a git checkout -- <filename> command, if desired Delete the guestbook service: kubectl delete svc guestbook --namespace default Re-create the service with NodePort type: kubectl create -f guestbook-service.yaml Check the updates, using kubectl get all --namespace default $ kubectl get all --namespace default NAME READY STATUS RESTARTS AGE pod/guestbook-v1-7fc76dc46-9r4s7 1/1 Running 0 1h pod/guestbook-v1-7fc76dc46-hspnk 1/1 Running 0 1h pod/guestbook-v1-7fc76dc46-sxzkt 1/1 Running 0 1h pod/redis-master-5d8b66464f-pvbl9 1/1 Running 0 1h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook NodePort 172.21.45.29 <none> 3000:31989/TCP 31s service/kubernetes ClusterIP 172.21.0.1 <none> 443/TCP 9d service/redis-master ClusterIP 172.21.232.61 <none> 6379/TCP 1h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-demo 3/3 3 3 1h deployment.apps/redis-master 1/1 1 1 1h NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-v1-7fc76dc46 3 3 3 1h replicaset.apps/redis-master-5d8b66464f 1 1 1 1h Note: The service type has changed (to NodePort ) and a new port has been allocated ( 31989 in this output case) to the guestbook service. All redis-slave resources have been removed. View the guestbook Get the public IP of one of your nodes: kubectl get nodes -o wide Navigate to the IP address plus the node port that printed earlier.","title":"Scenario 1: Update the application using kubectl"},{"location":"generatedContent/helm101/Lab2/#scenario-2-update-the-application-using-helm","text":"In this section, we'll update the previously deployed guestbook-demo application by using Helm. Before we start, let's take a few minutes to see how Helm simplifies the process compared to using Kubernetes directly. Helm's use of a template language provides great flexibility and power to chart authors, which removes the complexity to the chart user. In the guestbook example, we'll use the following capabilities of templating: Values: An object that provides access to the values passed into the chart. An example of this is in guestbook-service , which contains the line type: {{ .Values.service.type }} . This line provides the capability to set the service type during an upgrade or install. Control structures: Also called \u201cactions\u201d in template parlance, control structures provide the template author with the ability to control the flow of a template\u2019s generation. An example of this is in redis-slave-service , which contains the line {{- if .Values.redis.slaveEnabled -}} . This line allows us to enable/disable the REDIS master/slave during an upgrade or install. The complete redis-slave-service.yaml file shown below, demonstrates how the file becomes redundant when the slaveEnabled flag is disabled and also how the port value is set. There are more examples of templating functionality in the other chart files. {{ - if .Values.redis.slaveEnabled - }} apiVersion : v1 kind : Service metadata : name : redis-slave labels : app : redis role : slave spec : ports : - port : {{ .Values.redis.port }} targetPort : redis-server selector : app : redis role : slave {{ - end }} Enough talking about the theory. Now let's give it a go! First, lets check the app we deployed in Lab 1 with Helm. This can be done by checking the Helm releases: helm list -n helm-demo Note that we specify the namespace. If not specified, it uses the current namespace context. You should see output similar to the following: $ helm list -n helm-demo NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION guestbook-demo helm-demo 1 2020-02-24 18:08:02.017401264 +0000 UTC deployed guestbook-0.2.0 The list command provides the list of deployed charts (releases) giving information of chart version, namespace, number of updates (revisions) etc. We now know the release is there from step 1., so we can update the application: $ cd helm101/charts $ helm upgrade guestbook-demo ./guestbook --set redis.slaveEnabled = false,service.type = NodePort --namespace helm-demo Release \"guestbook-demo\" has been upgraded. Happy Helming! ... A Helm upgrade takes an existing release and upgrades it according to the information you provide. You should see output similar to the following: $ helm upgrade guestbook-demo ./guestbook --set redis.slaveEnabled = false,service.type = NodePort --namespace helm-demo Release \"guestbook-demo\" has been upgraded. Happy Helming! NAME: guestbook-demo LAST DEPLOYED: Tue Feb 25 14:23:27 2020 NAMESPACE: helm-demo STATUS: deployed REVISION: 2 TEST SUITE: None NOTES: 1. Get the application URL by running these commands: export NODE_PORT=$(kubectl get --namespace helm-demo -o jsonpath=\"{.spec.ports[0].nodePort}\" services guestbook-demo) export NODE_IP=$(kubectl get nodes --namespace helm-demo -o jsonpath=\"{.items[0].status.addresses[0].address}\") echo http://$NODE_IP:$NODE_PORT The upgrade command upgrades the app to a specified version of a chart, removes the redis-slave resources, and updates the app service.type to NodePort . Check the updates, using kubectl get all --namespace helm-demo : $ kubectl get all --namespace helm-demo NAME READY STATUS RESTARTS AGE pod/guestbook-demo-6c9cf8b9-dhqk9 1/1 Running 0 20h pod/guestbook-demo-6c9cf8b9-zddn2 1/1 Running 0 20h pod/redis-master-5d8b66464f-g7sh6 1/1 Running 0 20h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook-demo NodePort 172.21.43.244 <none> 3000:31202/TCP 20h service/redis-master ClusterIP 172.21.12.43 <none> 6379/TCP 20h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-demo 2/2 2 2 20h deployment.apps/redis-master 1/1 1 1 20h NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-demo-6c9cf8b9 2 2 2 20h replicaset.apps/redis-master-5d8b66464f 1 1 1 20h Note: The service type has changed (to NodePort ) and a new port has been allocated ( 31202 in this output case) to the guestbook service. All redis-slave resources have been removed. When you check the Helm release with helm list -n helm-demo , you will see the revision and date has been updated: $ helm list -n helm-demo NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION guestbook-demo helm-demo 2 2020-02-25 14:23:27.06732381 +0000 UTC deployed guestbook-0.2.0 View the guestbook Get the public IP of one of your nodes: kubectl get nodes -o wide Navigate to the IP address plus the node port that printed earlier.","title":"Scenario 2: Update the application using Helm"},{"location":"generatedContent/helm101/Lab2/#conclusion","text":"Congratulations, you have now updated the applications! Helm does not require any manual changing of resources and is therefore so much easier to upgrade! All configurations can be set on the fly on the command line or by using override files. This is made possible from when the logic was added to the template files, which enables or disables the capability, depending on the flag set. Check out Lab 3 to get an insight into revision management.","title":"Conclusion"},{"location":"generatedContent/helm101/Lab3/","text":"Lab 3. Keeping track of the deployed application \u00b6 Let's say you deployed different release versions of your application (i.e., you upgraded the running application). How do you keep track of the versions and how can you do a rollback? Scenario 1: Revision management using Kubernetes \u00b6 In this part of the lab, we should illustrate revision management of guestbook by using Kubernetes directly, but we can't. This is because Kubernetes does not provide any support for revision management. The onus is on you to manage your systems and any updates or changes you make. However, we can use Helm to conduct revision management. Scenario 2: Revision management using Helm \u00b6 In this part of the lab, we illustrate revision management on the deployed application guestbook-demo by using Helm. With Helm, every time an install, upgrade, or rollback happens, the revision number is incremented by 1. The first revision number is always 1. Helm persists release metadata in Secrets (default) or ConfigMaps, stored in the Kubernetes cluster. Every time your release changes, it appends that to the existing data. This provides Helm with the capability to rollback to a previous release. Let's see how this works in practice. Check the number of deployments: helm history guestbook-demo -n helm-demo You should see output similar to the following because we did an upgrade in Lab 2 after the initial install in Lab 1 : $ helm history guestbook-demo -n helm-demo REVISION UPDATED STATUS CHART APP VERSION DESCRIPTION 1 Mon Feb 24 18:08:02 2020 superseded guestbook-0.2.0 Install complete 2 Tue Feb 25 14:23:27 2020 deployed guestbook-0.2.0 Upgrade complete Roll back to the previous revision: In this rollback, Helm checks the changes that occured when upgrading from the revision 1 to revision 2. This information enables it to makes the calls to the Kubernetes API server, to update the deployed application as per the initial deployment - in other words with Redis slaves and using a load balancer. Rollback with this command: helm rollback guestbook-demo 1 -n helm-demo $ helm rollback guestbook-demo 1 -n helm-demo Rollback was a success! Happy Helming! Check the history again: helm history guestbook-demo -n helm-demo You should see output similar to the following: $ helm history guestbook-demo -n helm-demo REVISION UPDATED STATUS CHART APP VERSION DESCRIPTION 1 Mon Feb 24 18:08:02 2020 superseded guestbook-0.2.0 Install complete 2 Tue Feb 25 14:23:27 2020 superseded guestbook-0.2.0 Upgrade complete 3 Tue Feb 25 14:53:45 2020 deployed guestbook-0.2.0 Rollback to 1 Check the rollback, using: kubectl get all --namespace helm-demo $ kubectl get all --namespace helm-demo NAME READY STATUS RESTARTS AGE pod/guestbook-demo-6c9cf8b9-dhqk9 1/1 Running 0 20h pod/guestbook-demo-6c9cf8b9-zddn 1/1 Running 0 20h pod/redis-master-5d8b66464f-g7sh6 1/1 Running 0 20h pod/redis-slave-586b4c847c-tkfj5 1/1 Running 0 5m15s pod/redis-slave-586b4c847c-xxrdn 1/1 Running 0 5m15s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook-demo LoadBalancer 172.21.43.244 <pending> 3000:31202/TCP 20h service/redis-master ClusterIP 172.21.12.43 <none> 6379/TCP 20h service/redis-slave ClusterIP 172.21.232.16 <none> 6379/TCP 5m15s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-demo 2/2 2 2 20h deployment.apps/redis-master 1/1 1 1 20h deployment.apps/redis-slave 2/2 2 2 5m15s NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-demo-26c9cf8b9 2 2 2 20h replicaset.apps/redis-master-5d8b66464f 1 1 1 20h replicaset.apps/redis-slave-586b4c847c 2 2 2 5m15s You can see from the output that the app service is the service type of LoadBalancer again and the Redis master/slave deployment has returned. This shows a complete rollback from the upgrade in Lab 2 Conclusion \u00b6 From this lab, we can say that Helm does revision management well and Kubernetes does not have the capability built in! You might be wondering why we need helm rollback when you could just re-run the helm upgrade from a previous version. And that's a good question. Technically, you should end up with the same resources (with same parameters) deployed. However, the advantage of using helm rollback is that helm manages (ie. remembers) all of the variations/parameters of the previous helm install\\upgrade for you. Doing the rollback via a helm upgrade requires you (and your entire team) to manually track how the command was previously executed. That's not only tedious but very error prone. It is much easier, safer and reliable to let Helm manage all of that for you and all you need to do it tell it which previous version to go back to, and it does the rest. Lab 4 awaits.","title":"Lab 3. Keeping track of the deployed application"},{"location":"generatedContent/helm101/Lab3/#lab-3-keeping-track-of-the-deployed-application","text":"Let's say you deployed different release versions of your application (i.e., you upgraded the running application). How do you keep track of the versions and how can you do a rollback?","title":"Lab 3. Keeping track of the deployed application"},{"location":"generatedContent/helm101/Lab3/#scenario-1-revision-management-using-kubernetes","text":"In this part of the lab, we should illustrate revision management of guestbook by using Kubernetes directly, but we can't. This is because Kubernetes does not provide any support for revision management. The onus is on you to manage your systems and any updates or changes you make. However, we can use Helm to conduct revision management.","title":"Scenario 1: Revision management using Kubernetes"},{"location":"generatedContent/helm101/Lab3/#scenario-2-revision-management-using-helm","text":"In this part of the lab, we illustrate revision management on the deployed application guestbook-demo by using Helm. With Helm, every time an install, upgrade, or rollback happens, the revision number is incremented by 1. The first revision number is always 1. Helm persists release metadata in Secrets (default) or ConfigMaps, stored in the Kubernetes cluster. Every time your release changes, it appends that to the existing data. This provides Helm with the capability to rollback to a previous release. Let's see how this works in practice. Check the number of deployments: helm history guestbook-demo -n helm-demo You should see output similar to the following because we did an upgrade in Lab 2 after the initial install in Lab 1 : $ helm history guestbook-demo -n helm-demo REVISION UPDATED STATUS CHART APP VERSION DESCRIPTION 1 Mon Feb 24 18:08:02 2020 superseded guestbook-0.2.0 Install complete 2 Tue Feb 25 14:23:27 2020 deployed guestbook-0.2.0 Upgrade complete Roll back to the previous revision: In this rollback, Helm checks the changes that occured when upgrading from the revision 1 to revision 2. This information enables it to makes the calls to the Kubernetes API server, to update the deployed application as per the initial deployment - in other words with Redis slaves and using a load balancer. Rollback with this command: helm rollback guestbook-demo 1 -n helm-demo $ helm rollback guestbook-demo 1 -n helm-demo Rollback was a success! Happy Helming! Check the history again: helm history guestbook-demo -n helm-demo You should see output similar to the following: $ helm history guestbook-demo -n helm-demo REVISION UPDATED STATUS CHART APP VERSION DESCRIPTION 1 Mon Feb 24 18:08:02 2020 superseded guestbook-0.2.0 Install complete 2 Tue Feb 25 14:23:27 2020 superseded guestbook-0.2.0 Upgrade complete 3 Tue Feb 25 14:53:45 2020 deployed guestbook-0.2.0 Rollback to 1 Check the rollback, using: kubectl get all --namespace helm-demo $ kubectl get all --namespace helm-demo NAME READY STATUS RESTARTS AGE pod/guestbook-demo-6c9cf8b9-dhqk9 1/1 Running 0 20h pod/guestbook-demo-6c9cf8b9-zddn 1/1 Running 0 20h pod/redis-master-5d8b66464f-g7sh6 1/1 Running 0 20h pod/redis-slave-586b4c847c-tkfj5 1/1 Running 0 5m15s pod/redis-slave-586b4c847c-xxrdn 1/1 Running 0 5m15s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook-demo LoadBalancer 172.21.43.244 <pending> 3000:31202/TCP 20h service/redis-master ClusterIP 172.21.12.43 <none> 6379/TCP 20h service/redis-slave ClusterIP 172.21.232.16 <none> 6379/TCP 5m15s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-demo 2/2 2 2 20h deployment.apps/redis-master 1/1 1 1 20h deployment.apps/redis-slave 2/2 2 2 5m15s NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-demo-26c9cf8b9 2 2 2 20h replicaset.apps/redis-master-5d8b66464f 1 1 1 20h replicaset.apps/redis-slave-586b4c847c 2 2 2 5m15s You can see from the output that the app service is the service type of LoadBalancer again and the Redis master/slave deployment has returned. This shows a complete rollback from the upgrade in Lab 2","title":"Scenario 2: Revision management using Helm"},{"location":"generatedContent/helm101/Lab3/#conclusion","text":"From this lab, we can say that Helm does revision management well and Kubernetes does not have the capability built in! You might be wondering why we need helm rollback when you could just re-run the helm upgrade from a previous version. And that's a good question. Technically, you should end up with the same resources (with same parameters) deployed. However, the advantage of using helm rollback is that helm manages (ie. remembers) all of the variations/parameters of the previous helm install\\upgrade for you. Doing the rollback via a helm upgrade requires you (and your entire team) to manually track how the command was previously executed. That's not only tedious but very error prone. It is much easier, safer and reliable to let Helm manage all of that for you and all you need to do it tell it which previous version to go back to, and it does the rest. Lab 4 awaits.","title":"Conclusion"},{"location":"generatedContent/helm101/Lab4/","text":"Lab 4. Share Helm Charts \u00b6 A key aspect of providing an application means sharing with others. Sharing can be direct counsumption (by users or in CI/CD pipelines) or as a dependency for other charts. If people can't find your app then they can't use it. A means of sharing is a chart repository, which is a location where packaged charts can be stored and shared. As the chart repository only applies to Helm, we will just look at the usage and storage of Helm charts. Using charts from a public repository \u00b6 Helm charts can be available on a remote repository or in a local environment/repository. The remote repositories can be public like Bitnami Charts or IBM Helm Charts , or hosted repositories like on Google Cloud Storage or GitHub. Refer to Helm Chart Repository Guide for more details. You can learn more about the structure of a chart repository by examining the chart index file in this lab. In this part of the lab, we show you how to install the guestbook chart from the Helm101 repo . Check the repositories configured on your system: helm repo list The output should be similar to the following: $ helm repo list Error: no repositories to show Note: Chart repositories are not installed by default with Helm v3. It is expected that you add the repositories for the charts you want to use. The Helm Hub provides a centralized search for publicly available distributed charts. Using the hub you can identify the chart with its hosted repository and then add it to your local respoistory list. The Helm chart repository like Helm v2 is in \"maintenance mode\" and will be deprecated by November 13, 2020. See the project status for more details. Add helm101 repo: helm repo add helm101 https://ibm.github.io/helm101/ Should generate an output as follows: $ helm repo add helm101 https://ibm.github.io/helm101/ \"helm101\" has been added to your repositories You can also search your repositories for charts by running the following command: helm search repo helm101 $ helm search repo helm101 NAME CHART VERSION APP VERSION DESCRIPTION helm101/guestbook 0.2.1 A Helm chart to deploy Guestbook three tier web... Install the chart As mentioned we are going to install the guestbook chart from the Helm101 repo . As the repo is added to our local respoitory list we can reference the chart using the repo name/chart name , in other words helm101/guestbook . To see this in action, you will install the application to a new namespace called repo-demo . If the repo-demo namespace does not exist, create it using: kubectl create namespace repo-demo Now install the chart using this command: helm install guestbook-demo helm101/guestbook --namespace repo-demo The output should be similar to the following: $ helm install guestbook-demo helm101/guestbook --namespace repo-demo NAME: guestbook-demo LAST DEPLOYED: Tue Feb 25 15:40:17 2020 NAMESPACE: repo-demo STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: 1. Get the application URL by running these commands: NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running 'kubectl get svc -w guestbook-demo --namespace repo-demo' export SERVICE_IP=$(kubectl get svc --namespace repo-demo guestbook-demo -o jsonpath='{.status.loadBalancer.ingress[0].ip}') echo http://$SERVICE_IP:3000 Check that release deployed as expected as follows: $ helm list -n repo-demo NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION guestbook-demo repo-demo 1 2020-02-25 15:40:17.627745329 +0000 UTC deployed guestbook-0.2.1 Conclusion \u00b6 This lab provided you with a brief introduction to the Helm repositories to show how charts can be installed. The ability to share your chart means ease of use to both you and your consumers.","title":"Lab 4. Share Helm Charts"},{"location":"generatedContent/helm101/Lab4/#lab-4-share-helm-charts","text":"A key aspect of providing an application means sharing with others. Sharing can be direct counsumption (by users or in CI/CD pipelines) or as a dependency for other charts. If people can't find your app then they can't use it. A means of sharing is a chart repository, which is a location where packaged charts can be stored and shared. As the chart repository only applies to Helm, we will just look at the usage and storage of Helm charts.","title":"Lab 4. Share Helm Charts"},{"location":"generatedContent/helm101/Lab4/#using-charts-from-a-public-repository","text":"Helm charts can be available on a remote repository or in a local environment/repository. The remote repositories can be public like Bitnami Charts or IBM Helm Charts , or hosted repositories like on Google Cloud Storage or GitHub. Refer to Helm Chart Repository Guide for more details. You can learn more about the structure of a chart repository by examining the chart index file in this lab. In this part of the lab, we show you how to install the guestbook chart from the Helm101 repo . Check the repositories configured on your system: helm repo list The output should be similar to the following: $ helm repo list Error: no repositories to show Note: Chart repositories are not installed by default with Helm v3. It is expected that you add the repositories for the charts you want to use. The Helm Hub provides a centralized search for publicly available distributed charts. Using the hub you can identify the chart with its hosted repository and then add it to your local respoistory list. The Helm chart repository like Helm v2 is in \"maintenance mode\" and will be deprecated by November 13, 2020. See the project status for more details. Add helm101 repo: helm repo add helm101 https://ibm.github.io/helm101/ Should generate an output as follows: $ helm repo add helm101 https://ibm.github.io/helm101/ \"helm101\" has been added to your repositories You can also search your repositories for charts by running the following command: helm search repo helm101 $ helm search repo helm101 NAME CHART VERSION APP VERSION DESCRIPTION helm101/guestbook 0.2.1 A Helm chart to deploy Guestbook three tier web... Install the chart As mentioned we are going to install the guestbook chart from the Helm101 repo . As the repo is added to our local respoitory list we can reference the chart using the repo name/chart name , in other words helm101/guestbook . To see this in action, you will install the application to a new namespace called repo-demo . If the repo-demo namespace does not exist, create it using: kubectl create namespace repo-demo Now install the chart using this command: helm install guestbook-demo helm101/guestbook --namespace repo-demo The output should be similar to the following: $ helm install guestbook-demo helm101/guestbook --namespace repo-demo NAME: guestbook-demo LAST DEPLOYED: Tue Feb 25 15:40:17 2020 NAMESPACE: repo-demo STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: 1. Get the application URL by running these commands: NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running 'kubectl get svc -w guestbook-demo --namespace repo-demo' export SERVICE_IP=$(kubectl get svc --namespace repo-demo guestbook-demo -o jsonpath='{.status.loadBalancer.ingress[0].ip}') echo http://$SERVICE_IP:3000 Check that release deployed as expected as follows: $ helm list -n repo-demo NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION guestbook-demo repo-demo 1 2020-02-25 15:40:17.627745329 +0000 UTC deployed guestbook-0.2.1","title":"Using charts from a public repository"},{"location":"generatedContent/helm101/Lab4/#conclusion","text":"This lab provided you with a brief introduction to the Helm repositories to show how charts can be installed. The ability to share your chart means ease of use to both you and your consumers.","title":"Conclusion"},{"location":"generatedContent/istio101/","text":"Beyond the Basics: Istio and IBM Cloud Kubernetes Service \u00b6 Istio is an open platform to connect, secure, control and observe microservices, also known as a service mesh, on cloud platforms such as Kubernetes in IBM Cloud Kubernetes Service and VMs. With Istio, You can manage network traffic, load balance across microservices, enforce access policies, verify service identity, secure service communication and observe what exactly is going on with your services. YouTube: Istio Service Mesh Explained: In this course, you can see how to install Istio alongside microservices for a simple mock app called Guestbook . When you deploy Guestbook's microservices into an IBM Cloud Kubernetes Service cluster where Istio is installed, you can choose to inject the Istio Envoy sidecar proxies in the pods of certain microservices. Estimated completion time: 2 hours Objectives \u00b6 After you complete this course, you'll be able to: Download and install Istio in your cluster Deploy the Guestbook sample app Use metrics, logging and tracing to observe services Set up the Istio Ingress Gateway Perform simple traffic management, such as A/B tests and canary deployments Secure your service mesh Enforce policies for your microservices Prerequisites \u00b6 You must you must have a Pay-As-You-Go, or Subscription IBM Cloud account to complete all the modules in this course. You must have already created a Standard 1.16+ cluster in IBM Cloud Kubernetes Service. FREE Cluster is not supported for this lab You should have a basic understanding of containers, IBM Cloud Kubernetes Service, and Istio. If you have no experience with those, take the following courses: Get started with Kubernetes and IBM Cloud Kubernetes Service Get started with Istio and IBM Cloud Kubernetes Service Workshop setup \u00b6 Exercise 1 - Accessing a Kubernetes cluster with IBM Cloud Kubernetes Service Exercise 2 - Installing Istio Exercise 3 - Deploying Guestbook sample application Creating a service mesh with Istio \u00b6 Exercise 4 - Observe service telemetry: metrics and tracing Exercise 5 - Expose the service mesh with the Istio Ingress Gateway Exercise 6 - Perform traffic management Exercise 7 - Secure your service mesh Cleaning up the Workshop \u00b6 Script to uninstall ibmcloud CLI: clean_your_local_machine.sh and unset KUBECONFIG . Script to delete Istio and Guestbook: clean_your_k8s_cluster.sh .","title":"Beyond the Basics: Istio and IBM Cloud Kubernetes Service"},{"location":"generatedContent/istio101/#beyond-the-basics-istio-and-ibm-cloud-kubernetes-service","text":"Istio is an open platform to connect, secure, control and observe microservices, also known as a service mesh, on cloud platforms such as Kubernetes in IBM Cloud Kubernetes Service and VMs. With Istio, You can manage network traffic, load balance across microservices, enforce access policies, verify service identity, secure service communication and observe what exactly is going on with your services. YouTube: Istio Service Mesh Explained: In this course, you can see how to install Istio alongside microservices for a simple mock app called Guestbook . When you deploy Guestbook's microservices into an IBM Cloud Kubernetes Service cluster where Istio is installed, you can choose to inject the Istio Envoy sidecar proxies in the pods of certain microservices. Estimated completion time: 2 hours","title":"Beyond the Basics: Istio and IBM Cloud Kubernetes Service"},{"location":"generatedContent/istio101/#objectives","text":"After you complete this course, you'll be able to: Download and install Istio in your cluster Deploy the Guestbook sample app Use metrics, logging and tracing to observe services Set up the Istio Ingress Gateway Perform simple traffic management, such as A/B tests and canary deployments Secure your service mesh Enforce policies for your microservices","title":"Objectives"},{"location":"generatedContent/istio101/#prerequisites","text":"You must you must have a Pay-As-You-Go, or Subscription IBM Cloud account to complete all the modules in this course. You must have already created a Standard 1.16+ cluster in IBM Cloud Kubernetes Service. FREE Cluster is not supported for this lab You should have a basic understanding of containers, IBM Cloud Kubernetes Service, and Istio. If you have no experience with those, take the following courses: Get started with Kubernetes and IBM Cloud Kubernetes Service Get started with Istio and IBM Cloud Kubernetes Service","title":"Prerequisites"},{"location":"generatedContent/istio101/#workshop-setup","text":"Exercise 1 - Accessing a Kubernetes cluster with IBM Cloud Kubernetes Service Exercise 2 - Installing Istio Exercise 3 - Deploying Guestbook sample application","title":"Workshop setup"},{"location":"generatedContent/istio101/#creating-a-service-mesh-with-istio","text":"Exercise 4 - Observe service telemetry: metrics and tracing Exercise 5 - Expose the service mesh with the Istio Ingress Gateway Exercise 6 - Perform traffic management Exercise 7 - Secure your service mesh","title":"Creating a service mesh with Istio"},{"location":"generatedContent/istio101/#cleaning-up-the-workshop","text":"Script to uninstall ibmcloud CLI: clean_your_local_machine.sh and unset KUBECONFIG . Script to delete Istio and Guestbook: clean_your_k8s_cluster.sh .","title":"Cleaning up the Workshop"},{"location":"generatedContent/istio101/SUMMARY/","text":"Table of contents \u00b6 About this workshop \u00b6 Overview Workshop setup \u00b6 Exercise 1 - Accessing a Kubernetes cluster with IBM Cloud Kubernetes Service Exercise 2 - Installing Istio Exercise 3 - Deploying Guestbook sample application Creating a service mesh with Istio \u00b6 Exercise 4 - Observe service telemetry: metrics and tracing Exercise 5 - Expose the service mesh with the Istio Ingress Gateway Exercise 6 - Perform traffic management Exercise 7 - Secure your service mesh","title":"Table of contents"},{"location":"generatedContent/istio101/SUMMARY/#table-of-contents","text":"","title":"Table of contents"},{"location":"generatedContent/istio101/SUMMARY/#about-this-workshop","text":"Overview","title":"About this workshop"},{"location":"generatedContent/istio101/SUMMARY/#workshop-setup","text":"Exercise 1 - Accessing a Kubernetes cluster with IBM Cloud Kubernetes Service Exercise 2 - Installing Istio Exercise 3 - Deploying Guestbook sample application","title":"Workshop setup"},{"location":"generatedContent/istio101/SUMMARY/#creating-a-service-mesh-with-istio","text":"Exercise 4 - Observe service telemetry: metrics and tracing Exercise 5 - Expose the service mesh with the Istio Ingress Gateway Exercise 6 - Perform traffic management Exercise 7 - Secure your service mesh","title":"Creating a service mesh with Istio"},{"location":"generatedContent/istio101/exercise-1/","text":"Exercise 1 - Accessing a Kubernetes cluster with IBM Cloud Kubernetes Service \u00b6 Pre-requirements \u00b6 a cluster created . Your cluster must have 2 or more worker nodes with at least 4 cores and 16GB RAM , and run Kubernetes version 1.19 or later. Access to a client terminal with: IBM Cloud CLI , kubectl , and istioctl . Install IBM Cloud Kubernetes Service command line \u00b6 Download and install the required CLI tools. curl -sL https://ibm.biz/idt-installer | bash Using IBM Cloud CLI, log in to IBM Cloud, select the account with the Kubernetes cluster. (If you have a federated account, include the --sso flag.) ibmcloud login Access your cluster \u00b6 Learn how to set the context to work with your cluster by using the kubectl CLI, access the Kubernetes dashboard, and gather basic information about your cluster. Set the context for your cluster in your CLI. Every time you log in to the IBM Cloud Kubernetes Service CLI to work with the cluster, you must run these commands to set the path to the cluster's configuration file as a session variable. The Kubernetes CLI uses this variable to find a local configuration file and certificates that are necessary to connect with the cluster in IBM Cloud. a. List the available clusters. ibmcloud ks clusters b. Set an environment variable for your cluster name: export MYCLUSTER = <your_cluster_name> c. Download the configuration file and certificates for your cluster using the cluster-config command. ibmcloud ks cluster config --cluster $MYCLUSTER Get basic information about your cluster and its worker nodes. This information can help you manage your cluster and troubleshoot issues. a. View details of your cluster. ibmcloud ks cluster get --cluster $MYCLUSTER b. Verify the worker nodes in the cluster. ibmcloud ks workers --cluster $MYCLUSTER Validate access to your cluster by viewing the nodes in the cluster. kubectl get nodes Clone the lab repo \u00b6 From your command line, run: git clone https://github.com/IBM/istio101 cd istio101/docs/plans This is the working directory for the workshop. You will use the example .yaml files that are located in the docs/plans directory in the following exercises. Continue to Exercise 2 - Installing Istio \u00b6","title":"Lab 1. Setup"},{"location":"generatedContent/istio101/exercise-1/#exercise-1-accessing-a-kubernetes-cluster-with-ibm-cloud-kubernetes-service","text":"","title":"Exercise 1 - Accessing a Kubernetes cluster with IBM Cloud Kubernetes Service"},{"location":"generatedContent/istio101/exercise-1/#pre-requirements","text":"a cluster created . Your cluster must have 2 or more worker nodes with at least 4 cores and 16GB RAM , and run Kubernetes version 1.19 or later. Access to a client terminal with: IBM Cloud CLI , kubectl , and istioctl .","title":"Pre-requirements"},{"location":"generatedContent/istio101/exercise-1/#install-ibm-cloud-kubernetes-service-command-line","text":"Download and install the required CLI tools. curl -sL https://ibm.biz/idt-installer | bash Using IBM Cloud CLI, log in to IBM Cloud, select the account with the Kubernetes cluster. (If you have a federated account, include the --sso flag.) ibmcloud login","title":"Install IBM Cloud Kubernetes Service command line"},{"location":"generatedContent/istio101/exercise-1/#access-your-cluster","text":"Learn how to set the context to work with your cluster by using the kubectl CLI, access the Kubernetes dashboard, and gather basic information about your cluster. Set the context for your cluster in your CLI. Every time you log in to the IBM Cloud Kubernetes Service CLI to work with the cluster, you must run these commands to set the path to the cluster's configuration file as a session variable. The Kubernetes CLI uses this variable to find a local configuration file and certificates that are necessary to connect with the cluster in IBM Cloud. a. List the available clusters. ibmcloud ks clusters b. Set an environment variable for your cluster name: export MYCLUSTER = <your_cluster_name> c. Download the configuration file and certificates for your cluster using the cluster-config command. ibmcloud ks cluster config --cluster $MYCLUSTER Get basic information about your cluster and its worker nodes. This information can help you manage your cluster and troubleshoot issues. a. View details of your cluster. ibmcloud ks cluster get --cluster $MYCLUSTER b. Verify the worker nodes in the cluster. ibmcloud ks workers --cluster $MYCLUSTER Validate access to your cluster by viewing the nodes in the cluster. kubectl get nodes","title":"Access your cluster"},{"location":"generatedContent/istio101/exercise-1/#clone-the-lab-repo","text":"From your command line, run: git clone https://github.com/IBM/istio101 cd istio101/docs/plans This is the working directory for the workshop. You will use the example .yaml files that are located in the docs/plans directory in the following exercises.","title":"Clone the lab repo"},{"location":"generatedContent/istio101/exercise-1/#continue-to-exercise-2-installing-istio","text":"","title":"Continue to Exercise 2 - Installing Istio"},{"location":"generatedContent/istio101/exercise-2/","text":"Exercise 2 - Installing Istio on IBM Cloud Kubernetes Service \u00b6 In this module, you will use the istioctl CLI to install Istio on your cluster. Check if istioctl is installed in the client terminal, $ istioctl version 2021 -03-13T23:02:25.382086Z warn will use ` --remote = false ` to retrieve version info due to ` no Istio pods in name 1 .5.4 If istioctl is not installed, download the istioctl CLI and add it to your PATH: curl -L https://istio.io/downloadIstio | ISTIO_VERSION = 1 .5.6 sh - export PATH = $PWD /istio-1.5.6/bin: $PATH List the available profiles, istioctl profile list Sample output: $ istioctl profile list Istio configuration profiles: minimal remote separate default demo empty Install Istio with the demo Istio profile into your IKS cluster: istioctl manifest apply --set profile = demo Sample output: $ istioctl manifest apply --set profile = demo - Applying manifest for component Base... \u2714 Finished applying manifest for component Base. - Applying manifest for component Pilot... \u2714 Finished applying manifest for component Pilot. Waiting for resources to become ready... Waiting for resources to become ready... Waiting for resources to become ready... - Applying manifest for component IngressGateways... - Applying manifest for component EgressGateways... - Applying manifest for component AddonComponents... \u2714 Finished applying manifest for component IngressGateways. \u2714 Finished applying manifest for component EgressGateways. \u2714 Finished applying manifest for component AddonComponents. \u2714 Installation complete The install can take up to 10 minutes. Ensure the corresponding pods are all in Running state before you continue. kubectl get pods -n istio-system Sample output: $ kubectl get pods -n istio-system NAME READY STATUS RESTARTS AGE grafana-57cb8b8d44-zld4p 1 /1 Running 0 49s istio-egressgateway-794f9f9d64-7hxw5 1 /1 Running 0 52s istio-ingressgateway-945588978-gxtqq 1 /1 Running 0 52s istio-tracing-7fcc6f5848-8fx8j 1 /1 Running 0 49s istiod-7b7d59548-bs9x7 1 /1 Running 0 65s kiali-6875bdf78-ppgn7 1 /1 Running 0 49s prometheus-5d4c44597d-w49b5 2 /2 Running 0 4m33s NOTE Before you continue, make sure all the pods are deployed and either in the Running or Completed state. If they're in pending state, wait a few minutes to let the installation and deployment finish. Check the version of your Istio: istioctl version Sample output: client version: 1 .5.4 control plane version: 1 .5.4 data plane version: 1 .5.4 ( 3 proxies ) Congratulations! You successfully installed Istio into your cluster. Continue to Exercise 3 - Deploy Guestbook with Istio Proxy \u00b6","title":"Lab 2. Install Istio"},{"location":"generatedContent/istio101/exercise-2/#exercise-2-installing-istio-on-ibm-cloud-kubernetes-service","text":"In this module, you will use the istioctl CLI to install Istio on your cluster. Check if istioctl is installed in the client terminal, $ istioctl version 2021 -03-13T23:02:25.382086Z warn will use ` --remote = false ` to retrieve version info due to ` no Istio pods in name 1 .5.4 If istioctl is not installed, download the istioctl CLI and add it to your PATH: curl -L https://istio.io/downloadIstio | ISTIO_VERSION = 1 .5.6 sh - export PATH = $PWD /istio-1.5.6/bin: $PATH List the available profiles, istioctl profile list Sample output: $ istioctl profile list Istio configuration profiles: minimal remote separate default demo empty Install Istio with the demo Istio profile into your IKS cluster: istioctl manifest apply --set profile = demo Sample output: $ istioctl manifest apply --set profile = demo - Applying manifest for component Base... \u2714 Finished applying manifest for component Base. - Applying manifest for component Pilot... \u2714 Finished applying manifest for component Pilot. Waiting for resources to become ready... Waiting for resources to become ready... Waiting for resources to become ready... - Applying manifest for component IngressGateways... - Applying manifest for component EgressGateways... - Applying manifest for component AddonComponents... \u2714 Finished applying manifest for component IngressGateways. \u2714 Finished applying manifest for component EgressGateways. \u2714 Finished applying manifest for component AddonComponents. \u2714 Installation complete The install can take up to 10 minutes. Ensure the corresponding pods are all in Running state before you continue. kubectl get pods -n istio-system Sample output: $ kubectl get pods -n istio-system NAME READY STATUS RESTARTS AGE grafana-57cb8b8d44-zld4p 1 /1 Running 0 49s istio-egressgateway-794f9f9d64-7hxw5 1 /1 Running 0 52s istio-ingressgateway-945588978-gxtqq 1 /1 Running 0 52s istio-tracing-7fcc6f5848-8fx8j 1 /1 Running 0 49s istiod-7b7d59548-bs9x7 1 /1 Running 0 65s kiali-6875bdf78-ppgn7 1 /1 Running 0 49s prometheus-5d4c44597d-w49b5 2 /2 Running 0 4m33s NOTE Before you continue, make sure all the pods are deployed and either in the Running or Completed state. If they're in pending state, wait a few minutes to let the installation and deployment finish. Check the version of your Istio: istioctl version Sample output: client version: 1 .5.4 control plane version: 1 .5.4 data plane version: 1 .5.4 ( 3 proxies ) Congratulations! You successfully installed Istio into your cluster.","title":"Exercise 2 - Installing Istio on IBM Cloud Kubernetes Service"},{"location":"generatedContent/istio101/exercise-2/#continue-to-exercise-3-deploy-guestbook-with-istio-proxy","text":"","title":"Continue to Exercise 3 - Deploy Guestbook with Istio Proxy"},{"location":"generatedContent/istio101/exercise-3/","text":"Exercise 3 - Deploy the Guestbook app with Istio Proxy \u00b6 The Guestbook app is a sample app for users to leave comments. It consists of a web front end, Redis master for storage, and a replicated set of Redis slaves. We will also integrate the app with Watson Tone Analyzer which detects the sentiment in users' comments and replies with emoticons. Download the Guestbook app \u00b6 Clone the Guestbook app into the current directory docs/plans . git clone -b kubecon2019 https://github.com/remkohdev/guestbook Navigate into the app directory. cd guestbook/v2 Enable the automatic sidecar injection for the default namespace \u00b6 In Kubernetes, a sidecar is a utility container in the pod, and its purpose is to support the main container. For Istio to work, Envoy proxies must be deployed as sidecars to each pod of the deployment. There are two ways of injecting the Istio sidecar into a pod: manually using the istioctl CLI tool or automatically using the Istio sidecar injector. In this exercise, we will use the automatic sidecar injection provided by Istio. Annotate the default namespace to enable automatic sidecar injection: kubectl label namespace default istio-injection = enabled Validate the namespace is annotated for automatic sidecar injection: kubectl get namespace -L istio-injection Sample output: NAME STATUS AGE ISTIO-INJECTION default Active 271d enabled istio-system Active 5d2h ... Create a Redis database \u00b6 The Redis database is a service that you can use to persist the data of your app. The Redis database comes with a master and slave modules. Create the Redis controllers and services for both the master and the slave. kubectl create -f redis-master-deployment.yaml kubectl create -f redis-master-service.yaml kubectl create -f redis-slave-deployment.yaml kubectl create -f redis-slave-service.yaml Verify that the Redis controllers for the master and the slave are created. kubectl get deployment Output: NAME READY UP-TO-DATE AVAILABLE AGE redis-master 1 /1 1 1 2m16s redis-slave 2 /2 2 2 2m15s Verify that the Redis services for the master and the slave are created. kubectl get svc Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 172 .21.0.1 <none> 443 /TCP 23h redis-master ClusterIP 172 .21.85.39 <none> 6379 /TCP 5d redis-slave ClusterIP 172 .21.205.35 <none> 6379 /TCP 5d Verify that the Redis pods for the master and the slave are up and running. kubectl get pods Output: NAME READY STATUS RESTARTS AGE redis-master-4sswq 2 /2 Running 0 5d redis-slave-kj8jp 2 /2 Running 0 5d redis-slave-nslps 2 /2 Running 0 5d Install the Guestbook app \u00b6 Inject the Istio Envoy sidecar into the guestbook pods, and deploy the Guestbook app on to the Kubernetes cluster. Deploy both the v1 and v2 versions of the app: kubectl apply -f ../v1/guestbook-deployment.yaml kubectl apply -f guestbook-deployment.yaml These commands deploy the Guestbook app on to the Kubernetes cluster. Since we enabled automation sidecar injection, these pods will be also include an Envoy sidecar as they are started in the cluster. Here we have two versions of deployments, a new version ( v2 ) in the current directory, and a previous version ( v1 ) in a sibling directory. They will be used in future sections to showcase the Istio traffic routing capabilities. Create the guestbook service. kubectl create -f guestbook-service.yaml Verify that the service was created. kubectl get svc Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE guestbook LoadBalancer 172 .21.36.181 169 .61.37.140 80 :32149/TCP 5d ... Verify that the pods are up and running. kubectl get pods Sample output: NAME READY STATUS RESTARTS AGE guestbook-v1-9dbd6cf86-7xvsl 2 /2 Running 0 57s guestbook-v2-7769488b85-hhzf9 2 /2 Running 0 57s redis-master-7bf5f6d487-c86jg 2 /2 Running 0 2m9s redis-slave-58db457857-8hjcn 2 /2 Running 0 9m16s redis-slave-58db457857-ckj2x 2 /2 Running 0 9m16s Note that each guestbook pod has 2 containers in it. One is the guestbook container, and the other is the Envoy proxy sidecar. Great! Your guestbook app is up and running. In the next exercise you will expose the Istio service mesh with the Ingress Gateway","title":"Lab 3. Deploy Guestbook App"},{"location":"generatedContent/istio101/exercise-3/#exercise-3-deploy-the-guestbook-app-with-istio-proxy","text":"The Guestbook app is a sample app for users to leave comments. It consists of a web front end, Redis master for storage, and a replicated set of Redis slaves. We will also integrate the app with Watson Tone Analyzer which detects the sentiment in users' comments and replies with emoticons.","title":"Exercise 3 - Deploy the Guestbook app with Istio Proxy"},{"location":"generatedContent/istio101/exercise-3/#download-the-guestbook-app","text":"Clone the Guestbook app into the current directory docs/plans . git clone -b kubecon2019 https://github.com/remkohdev/guestbook Navigate into the app directory. cd guestbook/v2","title":"Download the Guestbook app"},{"location":"generatedContent/istio101/exercise-3/#enable-the-automatic-sidecar-injection-for-the-default-namespace","text":"In Kubernetes, a sidecar is a utility container in the pod, and its purpose is to support the main container. For Istio to work, Envoy proxies must be deployed as sidecars to each pod of the deployment. There are two ways of injecting the Istio sidecar into a pod: manually using the istioctl CLI tool or automatically using the Istio sidecar injector. In this exercise, we will use the automatic sidecar injection provided by Istio. Annotate the default namespace to enable automatic sidecar injection: kubectl label namespace default istio-injection = enabled Validate the namespace is annotated for automatic sidecar injection: kubectl get namespace -L istio-injection Sample output: NAME STATUS AGE ISTIO-INJECTION default Active 271d enabled istio-system Active 5d2h ...","title":"Enable the automatic sidecar injection for the default namespace"},{"location":"generatedContent/istio101/exercise-3/#create-a-redis-database","text":"The Redis database is a service that you can use to persist the data of your app. The Redis database comes with a master and slave modules. Create the Redis controllers and services for both the master and the slave. kubectl create -f redis-master-deployment.yaml kubectl create -f redis-master-service.yaml kubectl create -f redis-slave-deployment.yaml kubectl create -f redis-slave-service.yaml Verify that the Redis controllers for the master and the slave are created. kubectl get deployment Output: NAME READY UP-TO-DATE AVAILABLE AGE redis-master 1 /1 1 1 2m16s redis-slave 2 /2 2 2 2m15s Verify that the Redis services for the master and the slave are created. kubectl get svc Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 172 .21.0.1 <none> 443 /TCP 23h redis-master ClusterIP 172 .21.85.39 <none> 6379 /TCP 5d redis-slave ClusterIP 172 .21.205.35 <none> 6379 /TCP 5d Verify that the Redis pods for the master and the slave are up and running. kubectl get pods Output: NAME READY STATUS RESTARTS AGE redis-master-4sswq 2 /2 Running 0 5d redis-slave-kj8jp 2 /2 Running 0 5d redis-slave-nslps 2 /2 Running 0 5d","title":"Create a Redis database"},{"location":"generatedContent/istio101/exercise-3/#install-the-guestbook-app","text":"Inject the Istio Envoy sidecar into the guestbook pods, and deploy the Guestbook app on to the Kubernetes cluster. Deploy both the v1 and v2 versions of the app: kubectl apply -f ../v1/guestbook-deployment.yaml kubectl apply -f guestbook-deployment.yaml These commands deploy the Guestbook app on to the Kubernetes cluster. Since we enabled automation sidecar injection, these pods will be also include an Envoy sidecar as they are started in the cluster. Here we have two versions of deployments, a new version ( v2 ) in the current directory, and a previous version ( v1 ) in a sibling directory. They will be used in future sections to showcase the Istio traffic routing capabilities. Create the guestbook service. kubectl create -f guestbook-service.yaml Verify that the service was created. kubectl get svc Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE guestbook LoadBalancer 172 .21.36.181 169 .61.37.140 80 :32149/TCP 5d ... Verify that the pods are up and running. kubectl get pods Sample output: NAME READY STATUS RESTARTS AGE guestbook-v1-9dbd6cf86-7xvsl 2 /2 Running 0 57s guestbook-v2-7769488b85-hhzf9 2 /2 Running 0 57s redis-master-7bf5f6d487-c86jg 2 /2 Running 0 2m9s redis-slave-58db457857-8hjcn 2 /2 Running 0 9m16s redis-slave-58db457857-ckj2x 2 /2 Running 0 9m16s Note that each guestbook pod has 2 containers in it. One is the guestbook container, and the other is the Envoy proxy sidecar. Great! Your guestbook app is up and running. In the next exercise you will expose the Istio service mesh with the Ingress Gateway","title":"Install the Guestbook app"},{"location":"generatedContent/istio101/exercise-5/","text":"Exercise 4 - Expose the service mesh with the Istio Ingress Gateway \u00b6 The components deployed on the service mesh by default are not exposed outside the cluster. External access to individual services so far has been provided by creating an external load balancer or node port on each service. An Ingress Gateway resource can be created to allow external requests through the Istio Ingress Gateway to the backing services. Expose the Guestbook app with Ingress Gateway \u00b6 Configure the guestbook default route with the Istio Ingress Gateway. The guestbook-gateway.yaml file is in this repository (istio101) in the workshop/plans directory. cd ../.. kubectl create -f guestbook-gateway.yaml Get the EXTERNAL-IP of the Istio Ingress Gateway. kubectl get service istio-ingressgateway -n istio-system Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE istio-ingressgateway LoadBalancer 172 .21.254.53 169 .6.1.1 80 :31380/TCP,443:31390/TCP,31400:31400/TCP 1m 2d Make note of the external IP address that you retrieved in the previous step, as it will be used to access the Guestbook app in later parts of the course. Create an environment variable called $INGRESS_IP with your IP address. Example: export INGRESS_IP = 169 .6.1.1 Connect Istio Ingress Gateway to the IBM Cloud Kubernetes Service NLB Host Name \u00b6 NLB host names are created for each IBM Cloud Kubernetes cluster that is exposed with the Network LoadBalancer (NLB) service. These host names come with SSL certificate, the DNS registration, and health checks so you can benefit from them for any deployments that you expose via the NLB on IBM Cloud Kubernetes Service. You can run the IBM Cloud Kubernetes Service ALB, an API gateway of your choice, an Istio ingress gateway, and an MQTT server in parallel in your IBM Cloud Kubernetes Service cluster. Each one will have its own: 1. Publicly available wildcard host name 2. Wildcard SSL certificate associated with the host name 3. Health checks that you can configure if you use multizone deployments. Let's leverage this feature with Istio ingress gateway: Let's first check if you have any NLB host names for your cluster: ibmcloud ks nlb-dnss --cluster $MYCLUSTER If you haven't used this feature before, you will get an empty list. You can also find the hostname or the Ingress subdomain in the Cluster overview page of your cluster in the IBM Cloud page. Obtain the Istio ingress gateway's external IP. Get the EXTERNAL-IP of the istio-ingressgateway service via output below: kubectl get service istio-ingressgateway -n istio-system Create the NLB host with the Istio ingress gateway's public IP address: ibmcloud ks nlb-dns create classic --cluster $MYCLUSTER --ip $INGRESS_IP List the NLB host names for your cluster: ibmcloud ks nlb-dnss --cluster $MYCLUSTER Example output: shell Retrieving host names, certificates, IPs, and health check monitors for network load balancer (NLB) pods in cluster <cluster_name>... OK Hostname IP(s) Health Monitor SSL Cert Status SSL Cert Secret Name mycluster-85f044fc29ce613c264409c04a76c95d-0001.us-east.containers.appdomain.cloud [\"169.1.1.1\"] None created mycluster-85f044fc29ce613c264409c04a76c95d-0001 Make note of the NLB host name ( ), as it will be used to access your Guestbook app in later parts of the course. Create an environment variable for it and test using curl or visit in your browser. Example: export NLB_HOSTNAME = mycluster-85f044fc29ce613c264409c04a76c95d-0001.us-east.containers.appdomain.cloud curl $NLB_HOSTNAME Enable health check of the NLB host for Istio ingress gateway: ibmcloud ks nlb-dns monitor configure --cluster $MYCLUSTER --nlb-host $NLB_HOSTNAME --type HTTP --description \"Istio ingress gateway health check\" --path \"/healthz/ready\" --port 15021 --enable Monitor the health check of the NLB host for Istio ingress gateway: ibmcloud ks nlb-dns monitor status --cluster $MYCLUSTER After waiting for a bit, you should start to see the health monitor's status changed to Enabled. Example output: Retrieving health check monitor statuses for NLB pods... OK Hostname IP Health Monitor H.Monitor Status mycluster-85f044fc29ce613c264409c04a76c95d-0001.us-east.containers.appdomain.cloud 169 .1.1.1 Enabled Healthy Congratulations! You extended the base Ingress features by providing a DNS entry to the Istio service. References \u00b6 Kubernetes Ingress Istio Ingress Bring your own ALB Continue to Exercise 4 - Traffic Management \u00b6","title":"Exercise 4 - Expose the service mesh with the Istio Ingress Gateway"},{"location":"generatedContent/istio101/exercise-5/#exercise-4-expose-the-service-mesh-with-the-istio-ingress-gateway","text":"The components deployed on the service mesh by default are not exposed outside the cluster. External access to individual services so far has been provided by creating an external load balancer or node port on each service. An Ingress Gateway resource can be created to allow external requests through the Istio Ingress Gateway to the backing services.","title":"Exercise 4 - Expose the service mesh with the Istio Ingress Gateway"},{"location":"generatedContent/istio101/exercise-5/#expose-the-guestbook-app-with-ingress-gateway","text":"Configure the guestbook default route with the Istio Ingress Gateway. The guestbook-gateway.yaml file is in this repository (istio101) in the workshop/plans directory. cd ../.. kubectl create -f guestbook-gateway.yaml Get the EXTERNAL-IP of the Istio Ingress Gateway. kubectl get service istio-ingressgateway -n istio-system Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE istio-ingressgateway LoadBalancer 172 .21.254.53 169 .6.1.1 80 :31380/TCP,443:31390/TCP,31400:31400/TCP 1m 2d Make note of the external IP address that you retrieved in the previous step, as it will be used to access the Guestbook app in later parts of the course. Create an environment variable called $INGRESS_IP with your IP address. Example: export INGRESS_IP = 169 .6.1.1","title":"Expose the Guestbook app with Ingress Gateway"},{"location":"generatedContent/istio101/exercise-5/#connect-istio-ingress-gateway-to-the-ibm-cloud-kubernetes-service-nlb-host-name","text":"NLB host names are created for each IBM Cloud Kubernetes cluster that is exposed with the Network LoadBalancer (NLB) service. These host names come with SSL certificate, the DNS registration, and health checks so you can benefit from them for any deployments that you expose via the NLB on IBM Cloud Kubernetes Service. You can run the IBM Cloud Kubernetes Service ALB, an API gateway of your choice, an Istio ingress gateway, and an MQTT server in parallel in your IBM Cloud Kubernetes Service cluster. Each one will have its own: 1. Publicly available wildcard host name 2. Wildcard SSL certificate associated with the host name 3. Health checks that you can configure if you use multizone deployments. Let's leverage this feature with Istio ingress gateway: Let's first check if you have any NLB host names for your cluster: ibmcloud ks nlb-dnss --cluster $MYCLUSTER If you haven't used this feature before, you will get an empty list. You can also find the hostname or the Ingress subdomain in the Cluster overview page of your cluster in the IBM Cloud page. Obtain the Istio ingress gateway's external IP. Get the EXTERNAL-IP of the istio-ingressgateway service via output below: kubectl get service istio-ingressgateway -n istio-system Create the NLB host with the Istio ingress gateway's public IP address: ibmcloud ks nlb-dns create classic --cluster $MYCLUSTER --ip $INGRESS_IP List the NLB host names for your cluster: ibmcloud ks nlb-dnss --cluster $MYCLUSTER Example output: shell Retrieving host names, certificates, IPs, and health check monitors for network load balancer (NLB) pods in cluster <cluster_name>... OK Hostname IP(s) Health Monitor SSL Cert Status SSL Cert Secret Name mycluster-85f044fc29ce613c264409c04a76c95d-0001.us-east.containers.appdomain.cloud [\"169.1.1.1\"] None created mycluster-85f044fc29ce613c264409c04a76c95d-0001 Make note of the NLB host name ( ), as it will be used to access your Guestbook app in later parts of the course. Create an environment variable for it and test using curl or visit in your browser. Example: export NLB_HOSTNAME = mycluster-85f044fc29ce613c264409c04a76c95d-0001.us-east.containers.appdomain.cloud curl $NLB_HOSTNAME Enable health check of the NLB host for Istio ingress gateway: ibmcloud ks nlb-dns monitor configure --cluster $MYCLUSTER --nlb-host $NLB_HOSTNAME --type HTTP --description \"Istio ingress gateway health check\" --path \"/healthz/ready\" --port 15021 --enable Monitor the health check of the NLB host for Istio ingress gateway: ibmcloud ks nlb-dns monitor status --cluster $MYCLUSTER After waiting for a bit, you should start to see the health monitor's status changed to Enabled. Example output: Retrieving health check monitor statuses for NLB pods... OK Hostname IP Health Monitor H.Monitor Status mycluster-85f044fc29ce613c264409c04a76c95d-0001.us-east.containers.appdomain.cloud 169 .1.1.1 Enabled Healthy Congratulations! You extended the base Ingress features by providing a DNS entry to the Istio service.","title":"Connect Istio Ingress Gateway to the IBM Cloud Kubernetes Service NLB Host Name"},{"location":"generatedContent/istio101/exercise-5/#references","text":"Kubernetes Ingress Istio Ingress Bring your own ALB","title":"References"},{"location":"generatedContent/istio101/exercise-5/#continue-to-exercise-4-traffic-management","text":"","title":"Continue to Exercise 4 - Traffic Management"},{"location":"generatedContent/istio101/exercise-6/","text":"Exercise 4 - Perform traffic management \u00b6 Using rules to manage traffic \u00b6 Pilot is the core component used for traffic management in Istio. Pilot manages and configures all the Envoy proxy instances deployed in a particular Istio service mesh. It lets you specify what rules you want to use to route traffic between Envoy proxies, which run as sidecars to each service in the mesh. Pilot translates high-level rules into low-level configurations and distributes this config to Envoy instances. Pilot uses three types of configuration resources to manage traffic within its service mesh: Virtual Services, Destination Rules, and Service Entries. Virtual Services \u00b6 A VirtualService defines a set of traffic routing rules. If a request matches to a routing rule, then it is sent to a named destination service (or subset or version of it) defined in the service registry. Destination Rules \u00b6 A DestinationRule is a policy that applies after routing has occurred. These rules specify configuration for load balancing, connection pool size from the sidecar, and outlier detection settings to detect and evict unhealthy hosts from the load balancing pool. Any destination host and subset referenced in a VirtualService rule must be defined in a corresponding DestinationRule . Service Entries \u00b6 A ServiceEntry configuration enables services within the mesh to access a service not necessarily managed by Istio. The rule describes the endpoints, ports and protocols of a white-listed set of mesh-external domains and IP blocks that services in the mesh are allowed to access. The Guestbook app \u00b6 The guestbook deployment has two versions: version 1 and version 2. Each version has three instances based replicas in guestbook-deployment.yaml and guestbook-v2-deployment.yaml . By default, Istio will route requests equally across version 1 and version 2 using a round robin algorithm. However, new versions can easily introduce bugs to a service mesh, so using A/B Testing and Canary Deployments is good practice. A/B testing with Istio \u00b6 A/B testing is a method of performing identical tests against two separate service versions in parallel. To prevent Istio from performing the default routing behavior between the original and modernized guestbook service, define the following rules (found in istio101/workshop/plans ). Let's examine the rule: apiVersion : networking.istio.io/v1alpha3 kind : DestinationRule metadata : name : destination-rule-guestbook spec : host : guestbook subsets : - name : v1 labels : version : '1.0' - name : v2 labels : version : '2.0' Create the destination rule, kubectl create -f guestbook-destination.yaml Next, examine the virtual service: apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : virtual-service-guestbook spec : hosts : - '*' gateways : - guestbook-gateway http : - route : - destination : host : guestbook subset : v1 Apply the VirtualService kubectl replace -f virtualservice-all-v1.yaml The VirtualService defines a rule that captures all HTTP traffic coming in through the Istio ingress gateway, guestbook-gateway , and routes 100% of the traffic to pods of the guestbook service with label \"version: v1\". a corresponding DestinationRule defines subsets , which are referenced as destinations in the VirtualService . Since there are three instances matching the criteria of hostname guestbook with subset version: v1 , by default Envoy will send traffic to all three instances in a round robin manner. Get the Ingress subdomain or NLB host name for your cluster, NLB_HOSTNAME = $( ibmcloud ks cluster get --show-resources -c $MYCLUSTER --json | jq -r '.ingressHostname' ) echo $NLB_HOSTNAME Open the URL in a private window of Firefox or Chrome. You should see version 1 of the Guestbook application load. To enable the Istio service mesh for A/B testing against version 2, modify the original VirtualService rule. Examine the new rule: apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : virtual-service-guestbook spec : hosts : - '*' gateways : - guestbook-gateway http : - match : - headers : user-agent : regex : '.*Firefox.*' route : - destination : host : guestbook subset : v2 - route : - destination : host : guestbook subset : v1 Create the rule, kubectl replace -f virtualservice-test.yaml In Istio VirtualService rules, there can be only one rule for each service and therefore when defining multiple HTTPRoute blocks, the order in which they are defined in the yaml matters. Hence, the original VirtualService rule is modified rather than creating a new rule. With the modified rule, incoming requests originating from Firefox browsers will go to version 2 of Guestbook. All other requests fall-through to the next block, which routes all traffic to version 1 of Guestbook. Canary deployment \u00b6 In Canary Deployments , newer versions of services are incrementally rolled out to users to minimize the risk and impact of any bugs introduced by the newer version. To begin incrementally routing traffic to the newer version of the guestbook service, modify the original VirtualService rule: kubectl replace -f virtualservice-80-20.yaml Let's examine the rule: apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : virtual-service-guestbook spec : hosts : - '*' gateways : - guestbook-gateway http : - route : - destination : host : guestbook subset : v1 weight : 80 - destination : host : guestbook subset : v2 weight : 20 In the modified rule, the routed traffic is split between two different subsets of the guestbook service. In this manner, traffic to version 2 of Guestbook is controlled on a percentage basis to limit the impact of any unforeseen bugs. This rule can be modified over time until eventually all traffic is directed to version 2 of the service. View the Guestbook application using the $NLB_HOSTNAME and enter it as a URL in Firefox or Chrome web browsers. Ensure that you are using a hard refresh (command + Shift + R on Mac or Ctrl + F5 on windows) to remove any browser caching. You should notice that the Guestbook should swap between V1 or V2 at about the weight you specified. Route all traffic to v2 \u00b6 For the following exercises, we will route all traffic to guestbook v2 with a new VirtualService rule: cat <<EOF | kubectl replace -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: virtual-service-guestbook spec: hosts: - '*' gateways: - guestbook-gateway http: - route: - destination: host: guestbook subset: v2 EOF Implementing Circuit Breakers with Destination Rules \u00b6 Istio DestinationRules allow users to configure Envoy's implementation of circuit breakers . Circuit breakers are critical for defining the behavior for service-to-service communication in the service mesh. In the event of a failure for a particular service, circuit breakers allow users to set global defaults for failure recovery on a per service and/or per service version basis. Users can apply a traffic policy at the top level of the DestinationRule to create circuit breaker settings for an entire service, or it can be defined at the subset level to create settings for a particular version of a service. Depending on whether a service handles HTTP requests or TCP connections, DestinationRules expose a number of ways for Envoy to limit traffic to a particular service as well as define failure recovery behavior for services initiating the connection to an unhealthy service. Further reading \u00b6 Istio Concept Istio Rules API Istio Proxy Debug Tool Traffic Management Circuit Breaking Timeouts and Retries Questions \u00b6 Where are routing rules defined? Options: (VirtualService, DestinationRule, ServiceEntry) Answer: VirtualService Where are service versions (subsets) defined? Options: (VirtualService, DestinationRule, ServiceEntry) Answer: DestinationRule Which Istio component is responsible for sending traffic management configurations to Istio sidecars? Options: (Mixer, Citadel, Pilot, Kubernetes) Answer: Pilot What is the name of the default proxy that runs in Istio sidecars and routes requests within the service mesh? Options: (NGINX, Envoy, HAProxy) Answer: Envoy Continue to Exercise 5 - Mutual TLS \u00b6","title":"Lab 4. Traffic Management"},{"location":"generatedContent/istio101/exercise-6/#exercise-4-perform-traffic-management","text":"","title":"Exercise 4 - Perform traffic management"},{"location":"generatedContent/istio101/exercise-6/#using-rules-to-manage-traffic","text":"Pilot is the core component used for traffic management in Istio. Pilot manages and configures all the Envoy proxy instances deployed in a particular Istio service mesh. It lets you specify what rules you want to use to route traffic between Envoy proxies, which run as sidecars to each service in the mesh. Pilot translates high-level rules into low-level configurations and distributes this config to Envoy instances. Pilot uses three types of configuration resources to manage traffic within its service mesh: Virtual Services, Destination Rules, and Service Entries.","title":"Using rules to manage traffic"},{"location":"generatedContent/istio101/exercise-6/#virtual-services","text":"A VirtualService defines a set of traffic routing rules. If a request matches to a routing rule, then it is sent to a named destination service (or subset or version of it) defined in the service registry.","title":"Virtual Services"},{"location":"generatedContent/istio101/exercise-6/#destination-rules","text":"A DestinationRule is a policy that applies after routing has occurred. These rules specify configuration for load balancing, connection pool size from the sidecar, and outlier detection settings to detect and evict unhealthy hosts from the load balancing pool. Any destination host and subset referenced in a VirtualService rule must be defined in a corresponding DestinationRule .","title":"Destination Rules"},{"location":"generatedContent/istio101/exercise-6/#service-entries","text":"A ServiceEntry configuration enables services within the mesh to access a service not necessarily managed by Istio. The rule describes the endpoints, ports and protocols of a white-listed set of mesh-external domains and IP blocks that services in the mesh are allowed to access.","title":"Service Entries"},{"location":"generatedContent/istio101/exercise-6/#the-guestbook-app","text":"The guestbook deployment has two versions: version 1 and version 2. Each version has three instances based replicas in guestbook-deployment.yaml and guestbook-v2-deployment.yaml . By default, Istio will route requests equally across version 1 and version 2 using a round robin algorithm. However, new versions can easily introduce bugs to a service mesh, so using A/B Testing and Canary Deployments is good practice.","title":"The Guestbook app"},{"location":"generatedContent/istio101/exercise-6/#ab-testing-with-istio","text":"A/B testing is a method of performing identical tests against two separate service versions in parallel. To prevent Istio from performing the default routing behavior between the original and modernized guestbook service, define the following rules (found in istio101/workshop/plans ). Let's examine the rule: apiVersion : networking.istio.io/v1alpha3 kind : DestinationRule metadata : name : destination-rule-guestbook spec : host : guestbook subsets : - name : v1 labels : version : '1.0' - name : v2 labels : version : '2.0' Create the destination rule, kubectl create -f guestbook-destination.yaml Next, examine the virtual service: apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : virtual-service-guestbook spec : hosts : - '*' gateways : - guestbook-gateway http : - route : - destination : host : guestbook subset : v1 Apply the VirtualService kubectl replace -f virtualservice-all-v1.yaml The VirtualService defines a rule that captures all HTTP traffic coming in through the Istio ingress gateway, guestbook-gateway , and routes 100% of the traffic to pods of the guestbook service with label \"version: v1\". a corresponding DestinationRule defines subsets , which are referenced as destinations in the VirtualService . Since there are three instances matching the criteria of hostname guestbook with subset version: v1 , by default Envoy will send traffic to all three instances in a round robin manner. Get the Ingress subdomain or NLB host name for your cluster, NLB_HOSTNAME = $( ibmcloud ks cluster get --show-resources -c $MYCLUSTER --json | jq -r '.ingressHostname' ) echo $NLB_HOSTNAME Open the URL in a private window of Firefox or Chrome. You should see version 1 of the Guestbook application load. To enable the Istio service mesh for A/B testing against version 2, modify the original VirtualService rule. Examine the new rule: apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : virtual-service-guestbook spec : hosts : - '*' gateways : - guestbook-gateway http : - match : - headers : user-agent : regex : '.*Firefox.*' route : - destination : host : guestbook subset : v2 - route : - destination : host : guestbook subset : v1 Create the rule, kubectl replace -f virtualservice-test.yaml In Istio VirtualService rules, there can be only one rule for each service and therefore when defining multiple HTTPRoute blocks, the order in which they are defined in the yaml matters. Hence, the original VirtualService rule is modified rather than creating a new rule. With the modified rule, incoming requests originating from Firefox browsers will go to version 2 of Guestbook. All other requests fall-through to the next block, which routes all traffic to version 1 of Guestbook.","title":"A/B testing with Istio"},{"location":"generatedContent/istio101/exercise-6/#canary-deployment","text":"In Canary Deployments , newer versions of services are incrementally rolled out to users to minimize the risk and impact of any bugs introduced by the newer version. To begin incrementally routing traffic to the newer version of the guestbook service, modify the original VirtualService rule: kubectl replace -f virtualservice-80-20.yaml Let's examine the rule: apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : virtual-service-guestbook spec : hosts : - '*' gateways : - guestbook-gateway http : - route : - destination : host : guestbook subset : v1 weight : 80 - destination : host : guestbook subset : v2 weight : 20 In the modified rule, the routed traffic is split between two different subsets of the guestbook service. In this manner, traffic to version 2 of Guestbook is controlled on a percentage basis to limit the impact of any unforeseen bugs. This rule can be modified over time until eventually all traffic is directed to version 2 of the service. View the Guestbook application using the $NLB_HOSTNAME and enter it as a URL in Firefox or Chrome web browsers. Ensure that you are using a hard refresh (command + Shift + R on Mac or Ctrl + F5 on windows) to remove any browser caching. You should notice that the Guestbook should swap between V1 or V2 at about the weight you specified.","title":"Canary deployment"},{"location":"generatedContent/istio101/exercise-6/#route-all-traffic-to-v2","text":"For the following exercises, we will route all traffic to guestbook v2 with a new VirtualService rule: cat <<EOF | kubectl replace -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: virtual-service-guestbook spec: hosts: - '*' gateways: - guestbook-gateway http: - route: - destination: host: guestbook subset: v2 EOF","title":"Route all traffic to v2"},{"location":"generatedContent/istio101/exercise-6/#implementing-circuit-breakers-with-destination-rules","text":"Istio DestinationRules allow users to configure Envoy's implementation of circuit breakers . Circuit breakers are critical for defining the behavior for service-to-service communication in the service mesh. In the event of a failure for a particular service, circuit breakers allow users to set global defaults for failure recovery on a per service and/or per service version basis. Users can apply a traffic policy at the top level of the DestinationRule to create circuit breaker settings for an entire service, or it can be defined at the subset level to create settings for a particular version of a service. Depending on whether a service handles HTTP requests or TCP connections, DestinationRules expose a number of ways for Envoy to limit traffic to a particular service as well as define failure recovery behavior for services initiating the connection to an unhealthy service.","title":"Implementing Circuit Breakers with Destination Rules"},{"location":"generatedContent/istio101/exercise-6/#further-reading","text":"Istio Concept Istio Rules API Istio Proxy Debug Tool Traffic Management Circuit Breaking Timeouts and Retries","title":"Further reading"},{"location":"generatedContent/istio101/exercise-6/#questions","text":"Where are routing rules defined? Options: (VirtualService, DestinationRule, ServiceEntry) Answer: VirtualService Where are service versions (subsets) defined? Options: (VirtualService, DestinationRule, ServiceEntry) Answer: DestinationRule Which Istio component is responsible for sending traffic management configurations to Istio sidecars? Options: (Mixer, Citadel, Pilot, Kubernetes) Answer: Pilot What is the name of the default proxy that runs in Istio sidecars and routes requests within the service mesh? Options: (NGINX, Envoy, HAProxy) Answer: Envoy","title":"Questions"},{"location":"generatedContent/istio101/exercise-6/#continue-to-exercise-5-mutual-tls","text":"","title":"Continue to Exercise 5 - Mutual TLS"},{"location":"generatedContent/istio101/exercise-7/","text":"Exercise 5 - Secure your services \u00b6 Mutual Authentication with mutual Transport Layer Security (mTLS) \u00b6 Istio can secure communication between microservices without requiring application changes. Security is provided by authenticating and encrypting communication paths within the cluster. This is becoming a common security and compliance requirement. Delegating communication security to Istio (as opposed to implementing TLS in each microservice), ensures that your application will be deployed with consistent and manageable security policies. Istio Citadel is an optional part of Istio's control plane components. When enabled, it provides each Envoy sidecar proxy with a strong (cryptographic) identity, in the form of a certificate. Identity is based on the microservice's service account and is independent of its specific network location, such as cluster or current IP address. Envoys then use the certificates to identify each other and establish an authenticated and encrypted communication channel between them. Citadel is responsible for: Providing each service with an identity representing its role. Providing a common trust root to allow Envoys to validate and authenticate each other. Providing a key management system, automating generation, distribution, and rotation of certificates and keys. When a microservice connects to another microservice, the communication is redirected through the client side and server side Envoys. The end-to-end communication path is: Local TCP connection (i.e., localhost , not reaching the \"wire\") between the application and Envoy (client- and server-side); Mutually authenticated and encrypted connection between Envoy proxies. When Envoy proxies establish a connection, they exchange and validate certificates to confirm that each is indeed connected to a valid and expected peer. The established identities can later be used as basis for policy checks (e.g., access authorization). Enforce mTLS between all Istio services \u00b6 Enable global mTLS in your Istio manifest, istioctl manifest apply --set profile = demo --set values.global.mtls.auto = true --set values.global.mtls.enabled = true To enforce a mesh-wide authentication policy that requires mutual TLS, submit the following policy. This policy specifies that all workloads in the mesh will only accept encrypted requests using TLS. kubectl apply -n default -f - <<EOF apiVersion: \"security.istio.io/v1beta1\" kind: \"PeerAuthentication\" metadata: name: \"default\" spec: mtls: mode: STRICT EOF Visit your guestbook application by going to it in your browser. Everything should be working as expected! To confirm mTLS is infact enabled, you can run: istioctl x describe service guestbook Example output: Service : guestbook Port : http 80/HTTP targets pod port 3000 DestinationRule : destination-rule-guestbook for \"guestbook\" Matching subsets : v1,v2 No Traffic Policy Pod is STRICT, clients configured automatically Cleanup \u00b6 Run the following commands to clean up the Istio configuration resources as part of this exercise: kubectl delete PeerAuthentication default kubectl delete dr default kubectl delete dr destination-rule-guestbook Quiz \u00b6 True or False? Istio Citadel provides each microservice with a strong, cryptographic, identity in the form of a certificate. The certificates' life cycle is fully managed by Istio. (True) Istio provides microservices with mutually authenticated connections, without requiring app code changes. (True) Mutual authentication must be on or off for the entire cluster, gradual adoption is not possible. (False) Further Reading \u00b6 Basic TLS/SSL Terminology TLS Handshake Explained Istio Task Istio Concept","title":"Lab 5. mutual TLS"},{"location":"generatedContent/istio101/exercise-7/#exercise-5-secure-your-services","text":"","title":"Exercise 5 - Secure your services"},{"location":"generatedContent/istio101/exercise-7/#mutual-authentication-with-mutual-transport-layer-security-mtls","text":"Istio can secure communication between microservices without requiring application changes. Security is provided by authenticating and encrypting communication paths within the cluster. This is becoming a common security and compliance requirement. Delegating communication security to Istio (as opposed to implementing TLS in each microservice), ensures that your application will be deployed with consistent and manageable security policies. Istio Citadel is an optional part of Istio's control plane components. When enabled, it provides each Envoy sidecar proxy with a strong (cryptographic) identity, in the form of a certificate. Identity is based on the microservice's service account and is independent of its specific network location, such as cluster or current IP address. Envoys then use the certificates to identify each other and establish an authenticated and encrypted communication channel between them. Citadel is responsible for: Providing each service with an identity representing its role. Providing a common trust root to allow Envoys to validate and authenticate each other. Providing a key management system, automating generation, distribution, and rotation of certificates and keys. When a microservice connects to another microservice, the communication is redirected through the client side and server side Envoys. The end-to-end communication path is: Local TCP connection (i.e., localhost , not reaching the \"wire\") between the application and Envoy (client- and server-side); Mutually authenticated and encrypted connection between Envoy proxies. When Envoy proxies establish a connection, they exchange and validate certificates to confirm that each is indeed connected to a valid and expected peer. The established identities can later be used as basis for policy checks (e.g., access authorization).","title":"Mutual Authentication with mutual Transport Layer Security (mTLS)"},{"location":"generatedContent/istio101/exercise-7/#enforce-mtls-between-all-istio-services","text":"Enable global mTLS in your Istio manifest, istioctl manifest apply --set profile = demo --set values.global.mtls.auto = true --set values.global.mtls.enabled = true To enforce a mesh-wide authentication policy that requires mutual TLS, submit the following policy. This policy specifies that all workloads in the mesh will only accept encrypted requests using TLS. kubectl apply -n default -f - <<EOF apiVersion: \"security.istio.io/v1beta1\" kind: \"PeerAuthentication\" metadata: name: \"default\" spec: mtls: mode: STRICT EOF Visit your guestbook application by going to it in your browser. Everything should be working as expected! To confirm mTLS is infact enabled, you can run: istioctl x describe service guestbook Example output: Service : guestbook Port : http 80/HTTP targets pod port 3000 DestinationRule : destination-rule-guestbook for \"guestbook\" Matching subsets : v1,v2 No Traffic Policy Pod is STRICT, clients configured automatically","title":"Enforce mTLS between all Istio services"},{"location":"generatedContent/istio101/exercise-7/#cleanup","text":"Run the following commands to clean up the Istio configuration resources as part of this exercise: kubectl delete PeerAuthentication default kubectl delete dr default kubectl delete dr destination-rule-guestbook","title":"Cleanup"},{"location":"generatedContent/istio101/exercise-7/#quiz","text":"True or False? Istio Citadel provides each microservice with a strong, cryptographic, identity in the form of a certificate. The certificates' life cycle is fully managed by Istio. (True) Istio provides microservices with mutually authenticated connections, without requiring app code changes. (True) Mutual authentication must be on or off for the entire cluster, gradual adoption is not possible. (False)","title":"Quiz"},{"location":"generatedContent/istio101/exercise-7/#further-reading","text":"Basic TLS/SSL Terminology TLS Handshake Explained Istio Task Istio Concept","title":"Further Reading"},{"location":"generatedContent/jenkins101/","text":"Creating a CI/CD Pipeline for deployment to IBM Cloud Kubernetes Service using Jenkins \u00b6 Overview \u00b6 In this lab you will be enabling CI/CD connecting your Git repository with the guestbook app to a Continuous Integration/Continuous Deployment pipeline built with Jenkins that will deploy to a IBM Cloud Kubernetes Service cluster. Note: You will need to create a GitHub account if you don't have one already. Setup \u00b6 If you haven't already, set up the guestbook application that will be deployed: Open a new browser window or tab and go to the guestbook application repository Click on the Fork icon. Note: you need to fork the repo to have full access to turn of Git WebHooks feature for the repo in later section. If you have Git installed on your machine, go ahead and clone your fork of the guestbook application, then cd into that directory. From a terminal window, execute the following commands (replace the url with your forked repo url): git clone https://github.com/ [ git username ] /guestbook cd guestbook Lab Steps \u00b6 We will now configure the CI/CD pipeline with Jenkins to automate application deployment to a IBM Kubernetes cluster. Step 0: Create and collect: API Key, Registry Namespace and Cluster Name \u00b6 We will need all these values when we configure our Jenkins pipeline later. Login to the IBM Cloud console . Make sure you have the right account selected in the dropdown (the one with your Kubernetes cluster which you want to deploy to) and open the cloud shell by clicking the icon on the top right of the screen. Create an API key using the following command (replace [key name] with a name of your choosing). Save the key value by Copy and Pasting it to a text editor, we will use it later in our Jenkins Pipeline. ibmcloud iam api-key-create [ key name ] Create or access a container registry namespace. First, see if you have access to an existing namespace already. ibmcloud cr namespace-list If you get a value above, copy and paste to a text editor for later. If you have no namespaces created, run the following command to create one (replace [namespace name] with a name of your choosing). ibmcloud cr namespace-add [ namespace name ] Access and save the name of your Kubernetes Cluster on IBM Cloud. ibmcloud ks clusters Save the cluster name to a variable by using the following command (replace [cluster name] with your cluster name from above step): export CLUSTER_NAME =[ cluster name ] Step 1: Add Jenkinsfile to the Guestbook App \u00b6 We will add a JenkinsFile to your guestbook repository. Download this JenkinsFile to your machine. Inspect the JenkinsFile to learn what stages we will setup in the next steps. For your convenience, here is a curl command you can use to download the file. curl https://raw.githubusercontent.com/IBMAppModernization/app-modernization-cicd-lab-iks/guestbook-app/Jenkinsfile.ext > Jenkinsfile.ext To add the file your guestbook repo, you can either (option 1) use the git CLI if you have it installed or (option 2) do it from the browser. (Option 1) Save/move the Jenkinsfile to the root of your guestbook project (where you cloned it). Configure git client (if needed): git config --global user.email \"[your email]\" git config --global user.name \"[your first and last name]\" Add the Jenkinsfile and commit the changes: git add . git commit -m \"adding jenkinsfile\" Push the changes to your repo: git push (Option 2) Upload the file and commit using a web browser. From your fork of the github repo, click on Add file , then Upload files . Choose the Jenkinsfile.ext you download earlier and click the Commit changes button. Step 2: Set up the CI/CD pipeline \u00b6 In this section we will be connecting your forked Git repo of this app to set up a Continuous Integration/Continuous Deployment pipeline built with Jenkins. This pipeline contains 3 main steps as follows: Stage Purpose Build Docker Image Builds the Docker image based on the Dockerfile Push Docker Image to Registry Uploads the Docker image to the Docker image registry within ICP Deploy New Docker Image Updates the image tag in the Kubernetes deployment triggering a rolling update More details of this pipeline can be found in the Jenkinsfile.ext . Log into Jenkins using the URL provided to you by your instructor with the credentials provided to you. The pipeline should have already been created for you. Click on your pipeline to open it and then click on the Configure link in the navigation area at the left to change it's properties. Scroll down to the This project is parameterized section, here you will have to set some values to connect this pipeline to your cluster. Set the value of API_KEY to the API_KEY you created and saved earlier for your ibmcloud account. We will be using this key to give Jenkins access to deploy to your cluster and to push images to your container registry. To update the value, click on the 'Change Password' button next to the field and paste your API key. Set the value of the CLUSTER_NAME to the name of your Kubernetes cluster you want to deploy to. Set the value of the REGISTRY_NS to the name of your container registry namespace you viewed (or created) earlier. We will deploy our application image to this location. Update REGION to match the location of your Kubernetes cluster. You can view the location by running ibmcloud ks clusters . Scroll down to the Build Trigger section and select GitHub hook trigger for GIT SCM polling . Scroll down to the Pipeline section and find the Definition drop down menu. Select Pipeline script from SCM and for SCM select Git . For Repository URL enter the url to the cloned repository that you forked earlier (i.e. https://github.com/[your username]/guestbook.git ) Change the Script Path to Jenkinsfile.ext . Click Save . Step 3: Manually trigger a build to test pipeline \u00b6 In Jenkins in the navigation area on the left click on Build with Parameters . Accept the defaults of the parameters and click on Build To see the console output, click on the build number in the Build History and then click on Console Output If the build is successful the end of the console output should look like the following: The Stage View of the pipeline should look like the following: When the pipeline is finish deploying, launch the app to verify the it has been deployed and is running. Run the following command to get the port number of your deployed app: kubectl --namespace default get service guestbook -o jsonpath = '{.spec.ports[0].nodePort}' Run the following command to get the external IP address of the first worker node in your cluster: ibmcloud ks workers --cluster $CLUSTER_NAME | grep -v '^*' | egrep -v \"(ID|OK)\" | awk '{print $2;}' | head -n1 Your app's URL is the IP address of the first worker node with the port number of the deployed app. For example if your external IP is 169.61.73.182 and the port is 30961 the URL will be http://169.61.73.182:30961 Enter the URL in your browser's address bar and verify that the application loads. Step 4: Trigger a build via a commit to Github \u00b6 Now you'll configure Github to trigger your pipeline whenever code is committed. Go back to Github and find your cloned repository Click on the repository settings Under Options select Webhooks and click Add webhook For the Payload URL use <Jenkins URL>/github-webhook/ where <Jenkins URL> is the URL you used to login to Jenkins ( Note Don't forget the trailing / ) Change content type to application/json Accept the other defaults and click Add webhook In the Github file browser drill down to /v1/guestbook/public/index.html Click on the pencil icon to edit index.html and on line 12 locate the header of the page Change Guestbook - v1 to Guestbook - updated! ... or whatever you want! At the bottom of the UI window add a commit message and click on Commit changes Switch back to Jenkins and open the pipeline that you were working on earlier. Verify that your pipeline starts building. When the pipeline is finish deploying, force-refresh ( \u2318 + shift + R on mac) the browser window where you previously loaded the app to verify the change you made. Note: If you closed the browser window, follow steps 5 - 9 of the previous section to get the URL of the application again. Summary \u00b6 You created a Jenkins pipeline to automatically build and deploy an app that has been updated in Github.","title":"Jenkins and Kubernetes"},{"location":"generatedContent/jenkins101/#creating-a-cicd-pipeline-for-deployment-to-ibm-cloud-kubernetes-service-using-jenkins","text":"","title":"Creating a CI/CD Pipeline for deployment to IBM Cloud Kubernetes Service using Jenkins"},{"location":"generatedContent/jenkins101/#overview","text":"In this lab you will be enabling CI/CD connecting your Git repository with the guestbook app to a Continuous Integration/Continuous Deployment pipeline built with Jenkins that will deploy to a IBM Cloud Kubernetes Service cluster. Note: You will need to create a GitHub account if you don't have one already.","title":"Overview"},{"location":"generatedContent/jenkins101/#setup","text":"If you haven't already, set up the guestbook application that will be deployed: Open a new browser window or tab and go to the guestbook application repository Click on the Fork icon. Note: you need to fork the repo to have full access to turn of Git WebHooks feature for the repo in later section. If you have Git installed on your machine, go ahead and clone your fork of the guestbook application, then cd into that directory. From a terminal window, execute the following commands (replace the url with your forked repo url): git clone https://github.com/ [ git username ] /guestbook cd guestbook","title":"Setup"},{"location":"generatedContent/jenkins101/#lab-steps","text":"We will now configure the CI/CD pipeline with Jenkins to automate application deployment to a IBM Kubernetes cluster.","title":"Lab Steps"},{"location":"generatedContent/jenkins101/#step-0-create-and-collect-api-key-registry-namespace-and-cluster-name","text":"We will need all these values when we configure our Jenkins pipeline later. Login to the IBM Cloud console . Make sure you have the right account selected in the dropdown (the one with your Kubernetes cluster which you want to deploy to) and open the cloud shell by clicking the icon on the top right of the screen. Create an API key using the following command (replace [key name] with a name of your choosing). Save the key value by Copy and Pasting it to a text editor, we will use it later in our Jenkins Pipeline. ibmcloud iam api-key-create [ key name ] Create or access a container registry namespace. First, see if you have access to an existing namespace already. ibmcloud cr namespace-list If you get a value above, copy and paste to a text editor for later. If you have no namespaces created, run the following command to create one (replace [namespace name] with a name of your choosing). ibmcloud cr namespace-add [ namespace name ] Access and save the name of your Kubernetes Cluster on IBM Cloud. ibmcloud ks clusters Save the cluster name to a variable by using the following command (replace [cluster name] with your cluster name from above step): export CLUSTER_NAME =[ cluster name ]","title":"Step 0: Create and collect: API Key, Registry Namespace and Cluster Name"},{"location":"generatedContent/jenkins101/#step-1-add-jenkinsfile-to-the-guestbook-app","text":"We will add a JenkinsFile to your guestbook repository. Download this JenkinsFile to your machine. Inspect the JenkinsFile to learn what stages we will setup in the next steps. For your convenience, here is a curl command you can use to download the file. curl https://raw.githubusercontent.com/IBMAppModernization/app-modernization-cicd-lab-iks/guestbook-app/Jenkinsfile.ext > Jenkinsfile.ext To add the file your guestbook repo, you can either (option 1) use the git CLI if you have it installed or (option 2) do it from the browser. (Option 1) Save/move the Jenkinsfile to the root of your guestbook project (where you cloned it). Configure git client (if needed): git config --global user.email \"[your email]\" git config --global user.name \"[your first and last name]\" Add the Jenkinsfile and commit the changes: git add . git commit -m \"adding jenkinsfile\" Push the changes to your repo: git push (Option 2) Upload the file and commit using a web browser. From your fork of the github repo, click on Add file , then Upload files . Choose the Jenkinsfile.ext you download earlier and click the Commit changes button.","title":"Step 1: Add Jenkinsfile to the Guestbook App"},{"location":"generatedContent/jenkins101/#step-2-set-up-the-cicd-pipeline","text":"In this section we will be connecting your forked Git repo of this app to set up a Continuous Integration/Continuous Deployment pipeline built with Jenkins. This pipeline contains 3 main steps as follows: Stage Purpose Build Docker Image Builds the Docker image based on the Dockerfile Push Docker Image to Registry Uploads the Docker image to the Docker image registry within ICP Deploy New Docker Image Updates the image tag in the Kubernetes deployment triggering a rolling update More details of this pipeline can be found in the Jenkinsfile.ext . Log into Jenkins using the URL provided to you by your instructor with the credentials provided to you. The pipeline should have already been created for you. Click on your pipeline to open it and then click on the Configure link in the navigation area at the left to change it's properties. Scroll down to the This project is parameterized section, here you will have to set some values to connect this pipeline to your cluster. Set the value of API_KEY to the API_KEY you created and saved earlier for your ibmcloud account. We will be using this key to give Jenkins access to deploy to your cluster and to push images to your container registry. To update the value, click on the 'Change Password' button next to the field and paste your API key. Set the value of the CLUSTER_NAME to the name of your Kubernetes cluster you want to deploy to. Set the value of the REGISTRY_NS to the name of your container registry namespace you viewed (or created) earlier. We will deploy our application image to this location. Update REGION to match the location of your Kubernetes cluster. You can view the location by running ibmcloud ks clusters . Scroll down to the Build Trigger section and select GitHub hook trigger for GIT SCM polling . Scroll down to the Pipeline section and find the Definition drop down menu. Select Pipeline script from SCM and for SCM select Git . For Repository URL enter the url to the cloned repository that you forked earlier (i.e. https://github.com/[your username]/guestbook.git ) Change the Script Path to Jenkinsfile.ext . Click Save .","title":"Step 2: Set up the CI/CD pipeline"},{"location":"generatedContent/jenkins101/#step-3-manually-trigger-a-build-to-test-pipeline","text":"In Jenkins in the navigation area on the left click on Build with Parameters . Accept the defaults of the parameters and click on Build To see the console output, click on the build number in the Build History and then click on Console Output If the build is successful the end of the console output should look like the following: The Stage View of the pipeline should look like the following: When the pipeline is finish deploying, launch the app to verify the it has been deployed and is running. Run the following command to get the port number of your deployed app: kubectl --namespace default get service guestbook -o jsonpath = '{.spec.ports[0].nodePort}' Run the following command to get the external IP address of the first worker node in your cluster: ibmcloud ks workers --cluster $CLUSTER_NAME | grep -v '^*' | egrep -v \"(ID|OK)\" | awk '{print $2;}' | head -n1 Your app's URL is the IP address of the first worker node with the port number of the deployed app. For example if your external IP is 169.61.73.182 and the port is 30961 the URL will be http://169.61.73.182:30961 Enter the URL in your browser's address bar and verify that the application loads.","title":"Step 3: Manually trigger a build to test pipeline"},{"location":"generatedContent/jenkins101/#step-4-trigger-a-build-via-a-commit-to-github","text":"Now you'll configure Github to trigger your pipeline whenever code is committed. Go back to Github and find your cloned repository Click on the repository settings Under Options select Webhooks and click Add webhook For the Payload URL use <Jenkins URL>/github-webhook/ where <Jenkins URL> is the URL you used to login to Jenkins ( Note Don't forget the trailing / ) Change content type to application/json Accept the other defaults and click Add webhook In the Github file browser drill down to /v1/guestbook/public/index.html Click on the pencil icon to edit index.html and on line 12 locate the header of the page Change Guestbook - v1 to Guestbook - updated! ... or whatever you want! At the bottom of the UI window add a commit message and click on Commit changes Switch back to Jenkins and open the pipeline that you were working on earlier. Verify that your pipeline starts building. When the pipeline is finish deploying, force-refresh ( \u2318 + shift + R on mac) the browser window where you previously loaded the app to verify the change you made. Note: If you closed the browser window, follow steps 5 - 9 of the previous section to get the URL of the application again.","title":"Step 4: Trigger a build via a commit to Github"},{"location":"generatedContent/jenkins101/#summary","text":"You created a Jenkins pipeline to automatically build and deploy an app that has been updated in Github.","title":"Summary"},{"location":"generatedContent/kube101/","text":"IBM Cloud Kubernetes Service Lab \u00b6 An introduction to containers \u00b6 Hey, are you looking for a containers 101 course? Check out our Docker Essentials . Containers allow you to run securely isolated applications with quotas on system resources. Containers started out as an individual feature delivered with the linux kernel. Docker launched with making containers easy to use and developers quickly latched onto that idea. Containers have also sparked an interest in microservice architecture, a design pattern for developing applications in which complex applications are down into smaller, composable pieces which work together. Watch this video to learn about production uses of containers. Objectives \u00b6 This lab is an introduction to using Docker containers on Kubernetes in the IBM Cloud Kubernetes Service. By the end of the course, you'll achieve these objectives: Understand core concepts of Kubernetes Build a Docker image and deploy an application on Kubernetes in the IBM Cloud Kubernetes Service Control application deployments, while minimizing your time with infrastructure management Add AI services to extend your app Secure and monitor your cluster and app Prerequisites \u00b6 A Pay-As-You-Go or Subscription IBM Cloud account Virtual machines \u00b6 Prior to containers, most infrastructure ran not on bare metal, but atop hypervisors managing multiple virtualized operating systems (OSes). This arrangement allowed isolation of applications from one another on a higher level than that provided by the OS. These virtualized operating systems see what looks like their own exclusive hardware. However, this also means that each of these virtual operating systems are replicating an entire OS, taking up disk space. Containers \u00b6 Containers provide isolation similar to VMs, except provided by the OS and at the process level. Each container is a process or group of processes run in isolation. Typical containers explicitly run only a single process, as they have no need for the standard system services. What they usually need to do can be provided by system calls to the base OS kernel. The isolation on linux is provided by a feature called 'namespaces'. Each different kind of isolation (IE user, cgroups) is provided by a different namespace. This is a list of some of the namespaces that are commonly used and visible to the user: PID - process IDs USER - user and group IDs UTS - hostname and domain name NS - mount points NET - network devices, stacks, and ports CGROUPS - control limits and monitoring of resources VM vs container \u00b6 Traditional applications are run on native hardware. A single application does not typically use the full resources of a single machine. We try to run multiple applications on a single machine to avoid wasting resources. We could run multiple copies of the same application, but to provide isolation we use VMs to run multiple application instances (VMs) on the same hardware. These VMs have full operating system stacks which make them relatively large and inefficient due to duplication both at runtime and on disk. Containers allow you to share the host OS. This reduces duplication while still providing the isolation. Containers also allow you to drop unneeded files such as system libraries and binaries to save space and reduce your attack surface. If SSHD or LIBC are not installed, they cannot be exploited. Get set up \u00b6 Before we dive into Kubernetes, you need to provision a cluster for your containerized app. Then you won't have to wait for it to be ready for the subsequent labs. You must install the CLIs per https://console.ng.bluemix.net/docs/containers/cs_cli_install.html . If you do not yet have these CLIs and the Kubernetes CLI, do lab 0 before starting the course. If you haven't already, provision a cluster. This can take a few minutes, so let it start first: ibmcloud cs cluster-create --name <name-of-cluster> After creation, before using the cluster, make sure it has completed provisioning and is ready for use. Run ibmcloud cs clusters and make sure that your cluster is in state \"deployed\". Then use ibmcloud cs workers <name-of-cluster> and make sure that all worker nodes are in state \"normal\" with Status \"Ready\". Kubernetes and containers: an overview \u00b6 Let's talk about Kubernetes orchestration for containers before we build an application on it. We need to understand the following facts about it: What is Kubernetes, exactly? How was Kubernetes created? Kubernetes architecture Kubernetes resource model Kubernetes at IBM Let's get started What is Kubernetes? \u00b6 Now that we know what containers are, let's define what Kubernetes is. Kubernetes is a container orchestrator to provision, manage, and scale applications. In other words, Kubernetes allows you to manage the lifecycle of containerized applications within a cluster of nodes (which are a collection of worker machines, for example, VMs, physical machines etc.). Your applications may need many other resources to run such as Volumes, Networks, and Secrets that will help you to do things such as connect to databases, talk to firewalled backends, and secure keys. Kubernetes helps you add these resources into your application. Infrastructure resources needed by applications are managed declaratively. Fast fact: Other orchestration technologies are Mesos and Swarm. The key paradigm of kubernetes is it\u2019s Declarative model. The user provides the \"desired state\" and Kubernetes will do it's best make it happen. If you need 5 instances, you do not start 5 separate instances on your own but rather tell Kubernetes that you need 5 instances and Kubernetes will reconcile the state automatically. Simply at this point you need to know that you declare the state you want and Kubernetes makes that happen. If something goes wrong with one of your instances and it crashes, Kubernetes still knows the desired state and creates a new instances on an available node. Fun to know: Kubernetes goes by many names. Sometimes it is shortened to k8s (losing the internal 8 letters), or kube . The word is rooted in ancient Greek and means \"Helmsman\". A helmsman is the person who steers a ship. We hope you can seen the analogy between directing a ship and the decisions made to orchestrate containers on a cluster. How was Kubernetes created? \u00b6 Google wanted to open source their knowledge of creating and running the internal tools Borg & Omega. It adopted Open Governance for Kubernetes by starting the Cloud Native Computing Foundation (CNCF) and giving Kubernetes to that foundation, therefore making it less influenced by Google directly. Many companies such as RedHat, Microsoft, IBM and Amazon quickly joined the foundation. Main entry point for the kubernetes project is at http://kubernetes.io and the source code can be found at https://github.com/kubernetes . Kubernetes architecture \u00b6 At its core, Kubernetes is a data store (etcd). The declarative model is stored in the data store as objects, that means when you say I want 5 instances of a container then that request is stored into the data store. This information change is watched and delegated to Controllers to take action. Controllers then react to the model and attempt to take action to achieve the desired state. The power of Kubernetes is in its simplistic model. As shown, API server is a simple HTTP server handling create/read/update/delete(CRUD) operations on the data store. Then the controller picks up the change you wanted and makes that happen. Controllers are responsible for instantiating the actual resource represented by any Kubernetes resource. These actual resources are what your application needs to allow it to run successfully. Kubernetes resource model \u00b6 Kubernetes Infrastructure defines a resource for every purpose. Each resource is monitored and processed by a controller. When you define your application, it contains a collection of these resources. This collection will then be read by Controllers to build your applications actual backing instances. Some of resources that you may work with are listed below for your reference, for a full list you should go to https://kubernetes.io/docs/concepts/ . In this class we will only use a few of them, like Pod, Deployment, etc. Config Maps holds configuration data for pods to consume. Daemon Sets ensure that each node in the cluster runs this Pod Deployments defines a desired state of a deployment object Events provides lifecycle events on Pods and other deployment objects Endpoints allows a inbound connections to reach the cluster services Ingress is a collection of rules that allow inbound connections to reach the cluster services Jobs creates one or more pods and as they complete successfully the job is marked as completed. Node is a worker machine in Kubernetes Namespaces are multiple virtual clusters backed by the same physical cluster Pods are the smallest deployable units of computing that can be created and managed in Kubernetes Persistent Volumes provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed Replica Sets ensures that a specified number of pod replicas are running at any given time Secrets are intended to hold sensitive information, such as passwords, OAuth tokens, and ssh keys Service Accounts provides an identity for processes that run in a Pod Services is an abstraction which defines a logical set of Pods and a policy by which to access them - sometimes called a micro-service. Stateful Sets is the workload API object used to manage stateful applications. and more... Kubernetes does not have the concept of an application. It has simple building blocks that you are required to compose. Kubernetes is a cloud native platform where the internal resource model is the same as the end user resource model. Key resources \u00b6 A Pod is the smallest object model that you can create and run. You can add labels to a pod to identify a subset to run operations on. When you are ready to scale your application you can use the label to tell Kubernetes which Pod you need to scale. A Pod typically represent a process in your cluster. Pods contain at least one container that runs the job and additionally may have other containers in it called sidecars for monitoring, logging, etc. Essentially a Pod is a group of containers. When we talk about a application, we usually refer to group of Pods. Although an entire application can be run in a single Pod, we usually build multiple Pods that talk to each other to make a useful application. We will see why separating the application logic and backend database into separate Pods will scale better when we build an application shortly. Services define how to expose your app as a DNS entry to have a stable reference. We use query based selector to choose which pods are supplying that service. The user directly manipulates resources via yaml: kubectl ( create | get | apply | delete ) -f myResource.yaml Kubernetes provides us with a client interface through \u2018kubectl\u2019. Kubectl commands allow you to manage your applications, manage cluster and cluster resources, by modifying the model in the data store. Kubernetes application deployment workflow \u00b6 User via \"kubectl\" deploys a new application. Kubectl sends the request to the API Server. API server receives the request and stores it in the data store (etcd). Once the request is written to data store, the API server is done with the request. Watchers detects the resource changes and send a notification to controller to act upon it Controller detects the new app and creates new pods to match the desired number# of instances. Any changes to the stored model will be picked up to create or delete Pods. Scheduler assigns new pods to a Node based on a criteria. Scheduler makes decisions to run Pods on specific Nodes in the cluster. Scheduler modifies the model with the node information. Kubelet on a node detects a pod with an assignment to itself, and deploys the requested containers via the container runtime (e.g. Docker). Each Node watches the storage to see what pods it is assigned to run. It takes necessary actions on resource assigned to it like create/delete Pods. Kubeproxy manages network traffic for the pods - including service discovery and load-balancing. Kubeproxy is responsible for communication between Pods that want to interact. Lab information \u00b6 IBM Cloud provides the capability to run applications in containers on Kubernetes. The IBM Cloud Kubernetes Service runs Kubernetes clusters which deliver the following: Powerful tools Intuitive user experience Built-in security and isolation to enable rapid delivery of secure applications Cloud services including cognitive capabilities from Watson Capability to manage dedicated cluster resources for both stateless applications and stateful workloads Lab overview \u00b6 Lab 0 (Optional): Provides a walkthrough for installing IBM Cloud command-line tools and the Kubernetes CLI. You can skip this lab if you have the IBM Cloud CLI, the container-service plugin, the containers-registry plugin, and the kubectl CLI already installed on your machine. Lab 1 : This lab walks through creating and deploying a simple \"guestbook\" app written in Go as a net/http Server and accessing it. Lab 2 : Builds on lab 1 to expand to a more resilient setup which can survive having containers fail and recover. Lab 2 will also walk through basic services you need to get started with Kubernetes and the IBM Cloud Kubernetes Service Lab 3 : Builds on lab 2 by increasing the capabilities of the deployed Guestbook application. This lab covers basic distributed application design and how kubernetes helps you use standard design practices. Lab 4 : How to enable your application so Kubernetes can automatically monitor and recover your applications with no user intervention. Lab D : Debugging tips and tricks to help you along your Kubernetes journey. This lab is useful reference that does not follow in a specific sequence of the other labs.","title":"IBM Cloud Kubernetes Service Lab"},{"location":"generatedContent/kube101/#ibm-cloud-kubernetes-service-lab","text":"","title":"IBM Cloud Kubernetes Service Lab"},{"location":"generatedContent/kube101/#an-introduction-to-containers","text":"Hey, are you looking for a containers 101 course? Check out our Docker Essentials . Containers allow you to run securely isolated applications with quotas on system resources. Containers started out as an individual feature delivered with the linux kernel. Docker launched with making containers easy to use and developers quickly latched onto that idea. Containers have also sparked an interest in microservice architecture, a design pattern for developing applications in which complex applications are down into smaller, composable pieces which work together. Watch this video to learn about production uses of containers.","title":"An introduction to containers"},{"location":"generatedContent/kube101/#objectives","text":"This lab is an introduction to using Docker containers on Kubernetes in the IBM Cloud Kubernetes Service. By the end of the course, you'll achieve these objectives: Understand core concepts of Kubernetes Build a Docker image and deploy an application on Kubernetes in the IBM Cloud Kubernetes Service Control application deployments, while minimizing your time with infrastructure management Add AI services to extend your app Secure and monitor your cluster and app","title":"Objectives"},{"location":"generatedContent/kube101/#prerequisites","text":"A Pay-As-You-Go or Subscription IBM Cloud account","title":"Prerequisites"},{"location":"generatedContent/kube101/#virtual-machines","text":"Prior to containers, most infrastructure ran not on bare metal, but atop hypervisors managing multiple virtualized operating systems (OSes). This arrangement allowed isolation of applications from one another on a higher level than that provided by the OS. These virtualized operating systems see what looks like their own exclusive hardware. However, this also means that each of these virtual operating systems are replicating an entire OS, taking up disk space.","title":"Virtual machines"},{"location":"generatedContent/kube101/#containers","text":"Containers provide isolation similar to VMs, except provided by the OS and at the process level. Each container is a process or group of processes run in isolation. Typical containers explicitly run only a single process, as they have no need for the standard system services. What they usually need to do can be provided by system calls to the base OS kernel. The isolation on linux is provided by a feature called 'namespaces'. Each different kind of isolation (IE user, cgroups) is provided by a different namespace. This is a list of some of the namespaces that are commonly used and visible to the user: PID - process IDs USER - user and group IDs UTS - hostname and domain name NS - mount points NET - network devices, stacks, and ports CGROUPS - control limits and monitoring of resources","title":"Containers"},{"location":"generatedContent/kube101/#vm-vs-container","text":"Traditional applications are run on native hardware. A single application does not typically use the full resources of a single machine. We try to run multiple applications on a single machine to avoid wasting resources. We could run multiple copies of the same application, but to provide isolation we use VMs to run multiple application instances (VMs) on the same hardware. These VMs have full operating system stacks which make them relatively large and inefficient due to duplication both at runtime and on disk. Containers allow you to share the host OS. This reduces duplication while still providing the isolation. Containers also allow you to drop unneeded files such as system libraries and binaries to save space and reduce your attack surface. If SSHD or LIBC are not installed, they cannot be exploited.","title":"VM vs container"},{"location":"generatedContent/kube101/#get-set-up","text":"Before we dive into Kubernetes, you need to provision a cluster for your containerized app. Then you won't have to wait for it to be ready for the subsequent labs. You must install the CLIs per https://console.ng.bluemix.net/docs/containers/cs_cli_install.html . If you do not yet have these CLIs and the Kubernetes CLI, do lab 0 before starting the course. If you haven't already, provision a cluster. This can take a few minutes, so let it start first: ibmcloud cs cluster-create --name <name-of-cluster> After creation, before using the cluster, make sure it has completed provisioning and is ready for use. Run ibmcloud cs clusters and make sure that your cluster is in state \"deployed\". Then use ibmcloud cs workers <name-of-cluster> and make sure that all worker nodes are in state \"normal\" with Status \"Ready\".","title":"Get set up"},{"location":"generatedContent/kube101/#kubernetes-and-containers-an-overview","text":"Let's talk about Kubernetes orchestration for containers before we build an application on it. We need to understand the following facts about it: What is Kubernetes, exactly? How was Kubernetes created? Kubernetes architecture Kubernetes resource model Kubernetes at IBM Let's get started","title":"Kubernetes and containers: an overview"},{"location":"generatedContent/kube101/#what-is-kubernetes","text":"Now that we know what containers are, let's define what Kubernetes is. Kubernetes is a container orchestrator to provision, manage, and scale applications. In other words, Kubernetes allows you to manage the lifecycle of containerized applications within a cluster of nodes (which are a collection of worker machines, for example, VMs, physical machines etc.). Your applications may need many other resources to run such as Volumes, Networks, and Secrets that will help you to do things such as connect to databases, talk to firewalled backends, and secure keys. Kubernetes helps you add these resources into your application. Infrastructure resources needed by applications are managed declaratively. Fast fact: Other orchestration technologies are Mesos and Swarm. The key paradigm of kubernetes is it\u2019s Declarative model. The user provides the \"desired state\" and Kubernetes will do it's best make it happen. If you need 5 instances, you do not start 5 separate instances on your own but rather tell Kubernetes that you need 5 instances and Kubernetes will reconcile the state automatically. Simply at this point you need to know that you declare the state you want and Kubernetes makes that happen. If something goes wrong with one of your instances and it crashes, Kubernetes still knows the desired state and creates a new instances on an available node. Fun to know: Kubernetes goes by many names. Sometimes it is shortened to k8s (losing the internal 8 letters), or kube . The word is rooted in ancient Greek and means \"Helmsman\". A helmsman is the person who steers a ship. We hope you can seen the analogy between directing a ship and the decisions made to orchestrate containers on a cluster.","title":"What is Kubernetes?"},{"location":"generatedContent/kube101/#how-was-kubernetes-created","text":"Google wanted to open source their knowledge of creating and running the internal tools Borg & Omega. It adopted Open Governance for Kubernetes by starting the Cloud Native Computing Foundation (CNCF) and giving Kubernetes to that foundation, therefore making it less influenced by Google directly. Many companies such as RedHat, Microsoft, IBM and Amazon quickly joined the foundation. Main entry point for the kubernetes project is at http://kubernetes.io and the source code can be found at https://github.com/kubernetes .","title":"How was Kubernetes created?"},{"location":"generatedContent/kube101/#kubernetes-architecture","text":"At its core, Kubernetes is a data store (etcd). The declarative model is stored in the data store as objects, that means when you say I want 5 instances of a container then that request is stored into the data store. This information change is watched and delegated to Controllers to take action. Controllers then react to the model and attempt to take action to achieve the desired state. The power of Kubernetes is in its simplistic model. As shown, API server is a simple HTTP server handling create/read/update/delete(CRUD) operations on the data store. Then the controller picks up the change you wanted and makes that happen. Controllers are responsible for instantiating the actual resource represented by any Kubernetes resource. These actual resources are what your application needs to allow it to run successfully.","title":"Kubernetes architecture"},{"location":"generatedContent/kube101/#kubernetes-resource-model","text":"Kubernetes Infrastructure defines a resource for every purpose. Each resource is monitored and processed by a controller. When you define your application, it contains a collection of these resources. This collection will then be read by Controllers to build your applications actual backing instances. Some of resources that you may work with are listed below for your reference, for a full list you should go to https://kubernetes.io/docs/concepts/ . In this class we will only use a few of them, like Pod, Deployment, etc. Config Maps holds configuration data for pods to consume. Daemon Sets ensure that each node in the cluster runs this Pod Deployments defines a desired state of a deployment object Events provides lifecycle events on Pods and other deployment objects Endpoints allows a inbound connections to reach the cluster services Ingress is a collection of rules that allow inbound connections to reach the cluster services Jobs creates one or more pods and as they complete successfully the job is marked as completed. Node is a worker machine in Kubernetes Namespaces are multiple virtual clusters backed by the same physical cluster Pods are the smallest deployable units of computing that can be created and managed in Kubernetes Persistent Volumes provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed Replica Sets ensures that a specified number of pod replicas are running at any given time Secrets are intended to hold sensitive information, such as passwords, OAuth tokens, and ssh keys Service Accounts provides an identity for processes that run in a Pod Services is an abstraction which defines a logical set of Pods and a policy by which to access them - sometimes called a micro-service. Stateful Sets is the workload API object used to manage stateful applications. and more... Kubernetes does not have the concept of an application. It has simple building blocks that you are required to compose. Kubernetes is a cloud native platform where the internal resource model is the same as the end user resource model.","title":"Kubernetes resource model"},{"location":"generatedContent/kube101/#key-resources","text":"A Pod is the smallest object model that you can create and run. You can add labels to a pod to identify a subset to run operations on. When you are ready to scale your application you can use the label to tell Kubernetes which Pod you need to scale. A Pod typically represent a process in your cluster. Pods contain at least one container that runs the job and additionally may have other containers in it called sidecars for monitoring, logging, etc. Essentially a Pod is a group of containers. When we talk about a application, we usually refer to group of Pods. Although an entire application can be run in a single Pod, we usually build multiple Pods that talk to each other to make a useful application. We will see why separating the application logic and backend database into separate Pods will scale better when we build an application shortly. Services define how to expose your app as a DNS entry to have a stable reference. We use query based selector to choose which pods are supplying that service. The user directly manipulates resources via yaml: kubectl ( create | get | apply | delete ) -f myResource.yaml Kubernetes provides us with a client interface through \u2018kubectl\u2019. Kubectl commands allow you to manage your applications, manage cluster and cluster resources, by modifying the model in the data store.","title":"Key resources"},{"location":"generatedContent/kube101/#kubernetes-application-deployment-workflow","text":"User via \"kubectl\" deploys a new application. Kubectl sends the request to the API Server. API server receives the request and stores it in the data store (etcd). Once the request is written to data store, the API server is done with the request. Watchers detects the resource changes and send a notification to controller to act upon it Controller detects the new app and creates new pods to match the desired number# of instances. Any changes to the stored model will be picked up to create or delete Pods. Scheduler assigns new pods to a Node based on a criteria. Scheduler makes decisions to run Pods on specific Nodes in the cluster. Scheduler modifies the model with the node information. Kubelet on a node detects a pod with an assignment to itself, and deploys the requested containers via the container runtime (e.g. Docker). Each Node watches the storage to see what pods it is assigned to run. It takes necessary actions on resource assigned to it like create/delete Pods. Kubeproxy manages network traffic for the pods - including service discovery and load-balancing. Kubeproxy is responsible for communication between Pods that want to interact.","title":"Kubernetes application deployment workflow"},{"location":"generatedContent/kube101/#lab-information","text":"IBM Cloud provides the capability to run applications in containers on Kubernetes. The IBM Cloud Kubernetes Service runs Kubernetes clusters which deliver the following: Powerful tools Intuitive user experience Built-in security and isolation to enable rapid delivery of secure applications Cloud services including cognitive capabilities from Watson Capability to manage dedicated cluster resources for both stateless applications and stateful workloads","title":"Lab information"},{"location":"generatedContent/kube101/#lab-overview","text":"Lab 0 (Optional): Provides a walkthrough for installing IBM Cloud command-line tools and the Kubernetes CLI. You can skip this lab if you have the IBM Cloud CLI, the container-service plugin, the containers-registry plugin, and the kubectl CLI already installed on your machine. Lab 1 : This lab walks through creating and deploying a simple \"guestbook\" app written in Go as a net/http Server and accessing it. Lab 2 : Builds on lab 1 to expand to a more resilient setup which can survive having containers fail and recover. Lab 2 will also walk through basic services you need to get started with Kubernetes and the IBM Cloud Kubernetes Service Lab 3 : Builds on lab 2 by increasing the capabilities of the deployed Guestbook application. This lab covers basic distributed application design and how kubernetes helps you use standard design practices. Lab 4 : How to enable your application so Kubernetes can automatically monitor and recover your applications with no user intervention. Lab D : Debugging tips and tricks to help you along your Kubernetes journey. This lab is useful reference that does not follow in a specific sequence of the other labs.","title":"Lab overview"},{"location":"generatedContent/kube101/SUMMARY/","text":"Summary \u00b6 Getting Started \u00b6 Lab 0: Get the IBM Cloud Container Service Labs \u00b6 Lab 1. Set up and deploy your first application Lab 2: Scale and Update Deployments Lab 3: Scale and update apps natively, building multi-tier applications Resources \u00b6 IBM Developer","title":"Summary"},{"location":"generatedContent/kube101/SUMMARY/#summary","text":"","title":"Summary"},{"location":"generatedContent/kube101/SUMMARY/#getting-started","text":"Lab 0: Get the IBM Cloud Container Service","title":"Getting Started"},{"location":"generatedContent/kube101/SUMMARY/#labs","text":"Lab 1. Set up and deploy your first application Lab 2: Scale and Update Deployments Lab 3: Scale and update apps natively, building multi-tier applications","title":"Labs"},{"location":"generatedContent/kube101/SUMMARY/#resources","text":"IBM Developer","title":"Resources"},{"location":"generatedContent/kube101/Lab0/","text":"Lab 0. Access a Kubernetes cluster \u00b6 Set up your kubernetes environment \u00b6 For the hands-on labs in this tutorial repository, you will need a kubernetes cluster. One option for creating a cluster is to make use of the Kubernetes as-a-service from the IBM Cloud Kubernetes Service as outlined below. Use the IBM Cloud Kubernetes Service \u00b6 You will need either a paid IBM Cloud account or an IBM Cloud account which is a Trial account (not a Lite account). If you have one of these accounts, use the Getting Started Guide to create your cluster. Use a hosted trial environment \u00b6 There are a few services that are accessible over the Internet for temporary use. As these are free services, they can sometimes experience periods of limited availablity/quality. On the other hand, they can be a quick way to get started! Kubernetes playground on Katacoda This environment starts with a master and worker node pre-configured. You can run the steps from Labs 1 and onward from the master node. Play with Kubernetes After signing in with your github or docker hub id, click on Start , then Add New Instance and follow steps shown in terminal to spin up the cluster and add workers. Set up on your own workstation \u00b6 If you would like to configure kubernetes to run on your local workstation for non-production, learning use, there are several options. Minikube This solution requires the installation of a supported VM provider (KVM, VirtualBox, HyperKit, Hyper-V - depending on platform) Kubernetes in Docker (kind) Runs a kubernetes cluster on Docker containers Docker Desktop (Mac) Docker Desktop (Windows) Docker Desktop includes a kubernetes environment Microk8s Installable kubernetes packaged as an Ubuntu snap image. Install the IBM Cloud command-line interface \u00b6 As a prerequisite for the IBM Cloud Kubernetes Service plug-in, install the IBM Cloud command-line interface . Once installed, you can access IBM Cloud from your command-line with the prefix bx . Log in to the IBM Cloud CLI: ibmcloud login . Enter your IBM Cloud credentials when prompted. Note: If you have a federated ID, use ibmcloud login --sso to log in to the IBM Cloud CLI. Enter your user name, and use the provided URL in your CLI output to retrieve your one-time passcode. You know you have a federated ID when the login fails without the --sso and succeeds with the --sso option. Install the IBM Cloud Kubernetes Service plug-in \u00b6 To create Kubernetes clusters and manage worker nodes, install the IBM Cloud Kubernetes Service plug-in: ibmcloud plugin install container-service -r Bluemix Note: The prefix for running commands by using the IBM Cloud Kubernetes Service plug-in is bx cs . To verify that the plug-in is installed properly, run the following command: ibmcloud plugin list The IBM Cloud Kubernetes Service plug-in is displayed in the results as container-service . Download the Kubernetes CLI \u00b6 To view a local version of the Kubernetes dashboard and to deploy apps into your clusters, you will need to install the Kubernetes CLI that corresponds with your operating system: OS X Linux Windows For Windows users: Install the Kubernetes CLI in the same directory as the IBM Cloud CLI. This setup saves you some filepath changes when you run commands later. For OS X and Linux users: Move the executable file to the /usr/local/bin directory using the command mv /<path_to_file>/kubectl /usr/local/bin/kubectl . Make sure that /usr/local/bin is listed in your PATH system variable. $ echo $PATH /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin Convert the binary file to an executable: chmod +x /usr/local/bin/kubectl Configure Kubectl to point to IBM Cloud Kubernetes Service \u00b6 List the clusters in your account: ibmcloud ks clusters Set an environment variable that will be used in subsequent commands in this lab. export CLUSTER_NAME = <your_cluster_name> Configure kubectl to point to your cluster ibmcloud ks cluster config --cluster $CLUSTER_NAME Validate proper configuration kubectl get namespace You should see output similar to the following, if so, then your're ready to continue. NAME STATUS AGE default Active 125m ibm-cert-store Active 121m ibm-system Active 124m kube-node-lease Active 125m kube-public Active 125m kube-system Active 125m Download the Workshop Source Code \u00b6 Repo guestbook has the application that we'll be deploying. While we're not going to build it we will use the deployment configuration files from that repo. Guestbook application has two versions v1 and v2 which we will use to demonstrate some rollout functionality later. All the configuration files we use are under the directory guestbook/v1. Repo kube101 contains the step by step instructions to run the workshop. git clone https://github.com/IBM/guestbook.git git clone https://github.com/IBM/kube101.git","title":"Lab 0. Access a Kubernetes cluster"},{"location":"generatedContent/kube101/Lab0/#lab-0-access-a-kubernetes-cluster","text":"","title":"Lab 0. Access a Kubernetes cluster"},{"location":"generatedContent/kube101/Lab0/#set-up-your-kubernetes-environment","text":"For the hands-on labs in this tutorial repository, you will need a kubernetes cluster. One option for creating a cluster is to make use of the Kubernetes as-a-service from the IBM Cloud Kubernetes Service as outlined below.","title":"Set up your kubernetes environment"},{"location":"generatedContent/kube101/Lab0/#use-the-ibm-cloud-kubernetes-service","text":"You will need either a paid IBM Cloud account or an IBM Cloud account which is a Trial account (not a Lite account). If you have one of these accounts, use the Getting Started Guide to create your cluster.","title":"Use the IBM Cloud Kubernetes Service"},{"location":"generatedContent/kube101/Lab0/#use-a-hosted-trial-environment","text":"There are a few services that are accessible over the Internet for temporary use. As these are free services, they can sometimes experience periods of limited availablity/quality. On the other hand, they can be a quick way to get started! Kubernetes playground on Katacoda This environment starts with a master and worker node pre-configured. You can run the steps from Labs 1 and onward from the master node. Play with Kubernetes After signing in with your github or docker hub id, click on Start , then Add New Instance and follow steps shown in terminal to spin up the cluster and add workers.","title":"Use a hosted trial environment"},{"location":"generatedContent/kube101/Lab0/#set-up-on-your-own-workstation","text":"If you would like to configure kubernetes to run on your local workstation for non-production, learning use, there are several options. Minikube This solution requires the installation of a supported VM provider (KVM, VirtualBox, HyperKit, Hyper-V - depending on platform) Kubernetes in Docker (kind) Runs a kubernetes cluster on Docker containers Docker Desktop (Mac) Docker Desktop (Windows) Docker Desktop includes a kubernetes environment Microk8s Installable kubernetes packaged as an Ubuntu snap image.","title":"Set up on your own workstation"},{"location":"generatedContent/kube101/Lab0/#install-the-ibm-cloud-command-line-interface","text":"As a prerequisite for the IBM Cloud Kubernetes Service plug-in, install the IBM Cloud command-line interface . Once installed, you can access IBM Cloud from your command-line with the prefix bx . Log in to the IBM Cloud CLI: ibmcloud login . Enter your IBM Cloud credentials when prompted. Note: If you have a federated ID, use ibmcloud login --sso to log in to the IBM Cloud CLI. Enter your user name, and use the provided URL in your CLI output to retrieve your one-time passcode. You know you have a federated ID when the login fails without the --sso and succeeds with the --sso option.","title":"Install the IBM Cloud command-line interface"},{"location":"generatedContent/kube101/Lab0/#install-the-ibm-cloud-kubernetes-service-plug-in","text":"To create Kubernetes clusters and manage worker nodes, install the IBM Cloud Kubernetes Service plug-in: ibmcloud plugin install container-service -r Bluemix Note: The prefix for running commands by using the IBM Cloud Kubernetes Service plug-in is bx cs . To verify that the plug-in is installed properly, run the following command: ibmcloud plugin list The IBM Cloud Kubernetes Service plug-in is displayed in the results as container-service .","title":"Install the IBM Cloud Kubernetes Service plug-in"},{"location":"generatedContent/kube101/Lab0/#download-the-kubernetes-cli","text":"To view a local version of the Kubernetes dashboard and to deploy apps into your clusters, you will need to install the Kubernetes CLI that corresponds with your operating system: OS X Linux Windows For Windows users: Install the Kubernetes CLI in the same directory as the IBM Cloud CLI. This setup saves you some filepath changes when you run commands later. For OS X and Linux users: Move the executable file to the /usr/local/bin directory using the command mv /<path_to_file>/kubectl /usr/local/bin/kubectl . Make sure that /usr/local/bin is listed in your PATH system variable. $ echo $PATH /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin Convert the binary file to an executable: chmod +x /usr/local/bin/kubectl","title":"Download the Kubernetes CLI"},{"location":"generatedContent/kube101/Lab0/#configure-kubectl-to-point-to-ibm-cloud-kubernetes-service","text":"List the clusters in your account: ibmcloud ks clusters Set an environment variable that will be used in subsequent commands in this lab. export CLUSTER_NAME = <your_cluster_name> Configure kubectl to point to your cluster ibmcloud ks cluster config --cluster $CLUSTER_NAME Validate proper configuration kubectl get namespace You should see output similar to the following, if so, then your're ready to continue. NAME STATUS AGE default Active 125m ibm-cert-store Active 121m ibm-system Active 124m kube-node-lease Active 125m kube-public Active 125m kube-system Active 125m","title":"Configure Kubectl to point to IBM Cloud Kubernetes Service"},{"location":"generatedContent/kube101/Lab0/#download-the-workshop-source-code","text":"Repo guestbook has the application that we'll be deploying. While we're not going to build it we will use the deployment configuration files from that repo. Guestbook application has two versions v1 and v2 which we will use to demonstrate some rollout functionality later. All the configuration files we use are under the directory guestbook/v1. Repo kube101 contains the step by step instructions to run the workshop. git clone https://github.com/IBM/guestbook.git git clone https://github.com/IBM/kube101.git","title":"Download the Workshop Source Code"},{"location":"generatedContent/kube101/Lab1/","text":"Lab 1. Deploy your first application \u00b6 Learn how to deploy an application to a Kubernetes cluster hosted within the IBM Container Service. 0. Prerequisites \u00b6 Make sure you satisfy the prerequisites as outlined in Lab 0 $ ibmcloud cs cluster-create --name <name-of-cluster> If the above command doesn't work, please try the command below\uff1a $ ibmcloud cs cluster create classic --name <name-of-cluseter> Once the cluster is provisioned, the kubernetes client CLI kubectl needs to be configured to talk to the provisioned cluster. Run $ ibmcloud cs cluster-config <name-of-cluster> , and set the KUBECONFIG environment variable based on the output of the command. This will make your kubectl client point to your new Kubernetes cluster. Once your client is configured, you are ready to deploy your first application, guestbook . 1. Deploy the guestbook application \u00b6 In this part of the lab we will deploy an application called guestbook that has already been built and uploaded to DockerHub under the name ibmcom/guestbook:v1 . Start by running guestbook : kubectl create deployment guestbook --image = ibmcom/guestbook:v1 This action will take a bit of time. To check the status of the running application, you can use $ kubectl get pods . You should see output similar to the following: kubectl get pods Eventually, the status should show up as Running . $ kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-59bd679fdc-bxdg7 1 /1 Running 0 1m The end result of the run command is not just the pod containing our application containers, but a Deployment resource that manages the lifecycle of those pods. Once the status reads Running , we need to expose that deployment as a service so we can access it through the IP of the worker nodes. The guestbook application listens on port 3000. Run: kubectl expose deployment guestbook --type = \"NodePort\" --port = 3000 To find the port used on that worker node, examine your new service: $ kubectl get service guestbook NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE guestbook NodePort 10 .10.10.253 <none> 3000 :31208/TCP 1m We can see that our <nodeport> is 31208 . We can see in the output the port mapping from 3000 inside the pod exposed to the cluster on port 31208. This port in the 31000 range is automatically chosen, and could be different for you. guestbook is now running on your cluster, and exposed to the internet. We need to find out where it is accessible. The worker nodes running in the container service get external IP addresses. Get the workers for your cluster and note one (any one) of the public IPs listed on the <public-IP> line. Replace $CLUSTER_NAME with your cluster name unless you have this environment variable set. $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10 .185.199.3 Ready master,worker 63d v1.16.2+283af84 10 .185.199.3 169 .59.228.215 Red Hat 3 .10.0-1127.13.1.el7.x86_64 cri-o://1.16.6-17.rhaos4.3.git4936f44.el7 10 .185.199.6 Ready master,worker 63d v1.16.2+283af84 10 .185.199.6 169 .47.78.51 Red Hat 3 .10.0-1127.13.1.el7.x86_64 cri-o://1.16.6-17.rhaos4.3.git4936f44.el7 We can see that our <public-IP> is 173.193.99.136 . Now that you have both the address and the port, you can now access the application in the web browser at <public-IP>:<nodeport> . In the example case this is 173.193.99.136:31208 . Congratulations, you've now deployed an application to Kubernetes! When you're all done, continue to the next lab of this course .","title":"Lab 1. Deploy your first application"},{"location":"generatedContent/kube101/Lab1/#lab-1-deploy-your-first-application","text":"Learn how to deploy an application to a Kubernetes cluster hosted within the IBM Container Service.","title":"Lab 1. Deploy your first application"},{"location":"generatedContent/kube101/Lab1/#0-prerequisites","text":"Make sure you satisfy the prerequisites as outlined in Lab 0 $ ibmcloud cs cluster-create --name <name-of-cluster> If the above command doesn't work, please try the command below\uff1a $ ibmcloud cs cluster create classic --name <name-of-cluseter> Once the cluster is provisioned, the kubernetes client CLI kubectl needs to be configured to talk to the provisioned cluster. Run $ ibmcloud cs cluster-config <name-of-cluster> , and set the KUBECONFIG environment variable based on the output of the command. This will make your kubectl client point to your new Kubernetes cluster. Once your client is configured, you are ready to deploy your first application, guestbook .","title":"0. Prerequisites"},{"location":"generatedContent/kube101/Lab1/#1-deploy-the-guestbook-application","text":"In this part of the lab we will deploy an application called guestbook that has already been built and uploaded to DockerHub under the name ibmcom/guestbook:v1 . Start by running guestbook : kubectl create deployment guestbook --image = ibmcom/guestbook:v1 This action will take a bit of time. To check the status of the running application, you can use $ kubectl get pods . You should see output similar to the following: kubectl get pods Eventually, the status should show up as Running . $ kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-59bd679fdc-bxdg7 1 /1 Running 0 1m The end result of the run command is not just the pod containing our application containers, but a Deployment resource that manages the lifecycle of those pods. Once the status reads Running , we need to expose that deployment as a service so we can access it through the IP of the worker nodes. The guestbook application listens on port 3000. Run: kubectl expose deployment guestbook --type = \"NodePort\" --port = 3000 To find the port used on that worker node, examine your new service: $ kubectl get service guestbook NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE guestbook NodePort 10 .10.10.253 <none> 3000 :31208/TCP 1m We can see that our <nodeport> is 31208 . We can see in the output the port mapping from 3000 inside the pod exposed to the cluster on port 31208. This port in the 31000 range is automatically chosen, and could be different for you. guestbook is now running on your cluster, and exposed to the internet. We need to find out where it is accessible. The worker nodes running in the container service get external IP addresses. Get the workers for your cluster and note one (any one) of the public IPs listed on the <public-IP> line. Replace $CLUSTER_NAME with your cluster name unless you have this environment variable set. $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10 .185.199.3 Ready master,worker 63d v1.16.2+283af84 10 .185.199.3 169 .59.228.215 Red Hat 3 .10.0-1127.13.1.el7.x86_64 cri-o://1.16.6-17.rhaos4.3.git4936f44.el7 10 .185.199.6 Ready master,worker 63d v1.16.2+283af84 10 .185.199.6 169 .47.78.51 Red Hat 3 .10.0-1127.13.1.el7.x86_64 cri-o://1.16.6-17.rhaos4.3.git4936f44.el7 We can see that our <public-IP> is 173.193.99.136 . Now that you have both the address and the port, you can now access the application in the web browser at <public-IP>:<nodeport> . In the example case this is 173.193.99.136:31208 . Congratulations, you've now deployed an application to Kubernetes! When you're all done, continue to the next lab of this course .","title":"1. Deploy the guestbook application"},{"location":"generatedContent/kube101/Lab1/script/script/","text":"Pod \u00b6 In Kubernetes, a group of one or more containers is called a pod. Containers in a pod are deployed together, and are started, stopped, and replicated as a group. The simplest pod definition describes the deployment of a single container. For example, an nginx web server pod might be defined as such: apiVersion : v1 kind : Pod metadata : name : mynginx namespace : default labels : run : nginx spec : containers : - name : mynginx image : nginx:latest ports : - containerPort : 80 Labels \u00b6 In Kubernetes, labels are a system to organize objects into groups. Labels are key-value pairs that are attached to each object. Label selectors can be passed along with a request to the apiserver to retrieve a list of objects which match that label selector. To add a label to a pod, add a labels section under metadata in the pod definition: apiVersion : v1 kind : Pod metadata : labels : run : nginx ... To label a running pod kubectl label pod mynginx type = webserver pod \"mynginx\" labeled To list pods based on labels kubectl get pods -l type = webserver NAME READY STATUS RESTARTS AGE mynginx 1 /1 Running 0 21m Deployments \u00b6 A Deployment provides declarative updates for pods and replicas. You only need to describe the desired state in a Deployment object, and it will change the actual state to the desired state. The Deployment object defines the following details: The elements of a Replication Controller definition The strategy for transitioning between deployments To create a deployment for a nginx webserver, edit the nginx-deploy.yaml file as apiVersion : apps/v1beta1 kind : Deployment metadata : generation : 1 labels : run : nginx name : nginx namespace : default spec : replicas : 3 selector : matchLabels : run : nginx strategy : rollingUpdate : maxSurge : 1 maxUnavailable : 1 type : RollingUpdate template : metadata : labels : run : nginx spec : containers : - image : nginx:latest imagePullPolicy : Always name : nginx ports : - containerPort : 80 protocol : TCP dnsPolicy : ClusterFirst restartPolicy : Always securityContext : {} terminationGracePeriodSeconds : 30 and create the deployment kubectl create -f nginx-deploy.yaml deployment \"nginx\" created The deployment creates the following objects kubectl get all -l run = nginx NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/nginx 3 3 3 3 4m NAME DESIRED CURRENT READY AGE rs/nginx-664452237 3 3 3 4m NAME READY STATUS RESTARTS AGE po/nginx-664452237-h8dh0 1 /1 Running 0 4m po/nginx-664452237-ncsh1 1 /1 Running 0 4m po/nginx-664452237-vts63 1 /1 Running 0 4m services \u00b6 Services Kubernetes pods, as containers, are ephemeral. Replication Controllers create and destroy pods dynamically, e.g. when scaling up or down or when doing rolling updates. While each pod gets its own IP address, even those IP addresses cannot be relied upon to be stable over time. This leads to a problem: if some set of pods provides functionality to other pods inside the Kubernetes cluster, how do those pods find out and keep track of which other? A Kubernetes Service is an abstraction which defines a logical set of pods and a policy by which to access them. The set of pods targeted by a Service is usually determined by a label selector. Kubernetes offers a simple Endpoints API that is updated whenever the set of pods in a service changes. To create a service for our nginx webserver, edit the nginx-service.yaml file apiVersion : v1 kind : Service metadata : name : nginx labels : run : nginx spec : selector : run : nginx ports : - protocol : TCP port : 8000 targetPort : 80 type : ClusterIP Create the service kubectl create -f nginx-service.yaml service \"nginx\" created kubectl get service -l run = nginx NAME CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE nginx 10 .254.60.24 <none> 8000 /TCP 38s Describe the service: kubectl describe service nginx Name: nginx Namespace: default Labels: run = nginx Selector: run = nginx Type: ClusterIP IP: 10 .254.60.24 Port: <unset> 8000 /TCP Endpoints: 172 .30.21.3:80,172.30.4.4:80,172.30.53.4:80 Session Affinity: None No events. The above service is associated to our previous nginx pods. Pay attention to the service selector run=nginx field. It tells Kubernetes that all pods with the label run=nginx are associated to this service, and should have traffic distributed amongst them. In other words, the service provides an abstraction layer, and it is the input point to reach all of the associated pods.","title":"Pod"},{"location":"generatedContent/kube101/Lab1/script/script/#pod","text":"In Kubernetes, a group of one or more containers is called a pod. Containers in a pod are deployed together, and are started, stopped, and replicated as a group. The simplest pod definition describes the deployment of a single container. For example, an nginx web server pod might be defined as such: apiVersion : v1 kind : Pod metadata : name : mynginx namespace : default labels : run : nginx spec : containers : - name : mynginx image : nginx:latest ports : - containerPort : 80","title":"Pod"},{"location":"generatedContent/kube101/Lab1/script/script/#labels","text":"In Kubernetes, labels are a system to organize objects into groups. Labels are key-value pairs that are attached to each object. Label selectors can be passed along with a request to the apiserver to retrieve a list of objects which match that label selector. To add a label to a pod, add a labels section under metadata in the pod definition: apiVersion : v1 kind : Pod metadata : labels : run : nginx ... To label a running pod kubectl label pod mynginx type = webserver pod \"mynginx\" labeled To list pods based on labels kubectl get pods -l type = webserver NAME READY STATUS RESTARTS AGE mynginx 1 /1 Running 0 21m","title":"Labels"},{"location":"generatedContent/kube101/Lab1/script/script/#deployments","text":"A Deployment provides declarative updates for pods and replicas. You only need to describe the desired state in a Deployment object, and it will change the actual state to the desired state. The Deployment object defines the following details: The elements of a Replication Controller definition The strategy for transitioning between deployments To create a deployment for a nginx webserver, edit the nginx-deploy.yaml file as apiVersion : apps/v1beta1 kind : Deployment metadata : generation : 1 labels : run : nginx name : nginx namespace : default spec : replicas : 3 selector : matchLabels : run : nginx strategy : rollingUpdate : maxSurge : 1 maxUnavailable : 1 type : RollingUpdate template : metadata : labels : run : nginx spec : containers : - image : nginx:latest imagePullPolicy : Always name : nginx ports : - containerPort : 80 protocol : TCP dnsPolicy : ClusterFirst restartPolicy : Always securityContext : {} terminationGracePeriodSeconds : 30 and create the deployment kubectl create -f nginx-deploy.yaml deployment \"nginx\" created The deployment creates the following objects kubectl get all -l run = nginx NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/nginx 3 3 3 3 4m NAME DESIRED CURRENT READY AGE rs/nginx-664452237 3 3 3 4m NAME READY STATUS RESTARTS AGE po/nginx-664452237-h8dh0 1 /1 Running 0 4m po/nginx-664452237-ncsh1 1 /1 Running 0 4m po/nginx-664452237-vts63 1 /1 Running 0 4m","title":"Deployments"},{"location":"generatedContent/kube101/Lab1/script/script/#services","text":"Services Kubernetes pods, as containers, are ephemeral. Replication Controllers create and destroy pods dynamically, e.g. when scaling up or down or when doing rolling updates. While each pod gets its own IP address, even those IP addresses cannot be relied upon to be stable over time. This leads to a problem: if some set of pods provides functionality to other pods inside the Kubernetes cluster, how do those pods find out and keep track of which other? A Kubernetes Service is an abstraction which defines a logical set of pods and a policy by which to access them. The set of pods targeted by a Service is usually determined by a label selector. Kubernetes offers a simple Endpoints API that is updated whenever the set of pods in a service changes. To create a service for our nginx webserver, edit the nginx-service.yaml file apiVersion : v1 kind : Service metadata : name : nginx labels : run : nginx spec : selector : run : nginx ports : - protocol : TCP port : 8000 targetPort : 80 type : ClusterIP Create the service kubectl create -f nginx-service.yaml service \"nginx\" created kubectl get service -l run = nginx NAME CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE nginx 10 .254.60.24 <none> 8000 /TCP 38s Describe the service: kubectl describe service nginx Name: nginx Namespace: default Labels: run = nginx Selector: run = nginx Type: ClusterIP IP: 10 .254.60.24 Port: <unset> 8000 /TCP Endpoints: 172 .30.21.3:80,172.30.4.4:80,172.30.53.4:80 Session Affinity: None No events. The above service is associated to our previous nginx pods. Pay attention to the service selector run=nginx field. It tells Kubernetes that all pods with the label run=nginx are associated to this service, and should have traffic distributed amongst them. In other words, the service provides an abstraction layer, and it is the input point to reach all of the associated pods.","title":"services"},{"location":"generatedContent/kube101/Lab2/","text":"Lab 2: Scale and Update Deployments \u00b6 In this lab, you'll learn how to update the number of instances a deployment has and how to safely roll out an update of your application on Kubernetes. For this lab, you need a running deployment of the guestbook application from the previous lab. If you need to create it, run: kubectl create deployment guestbook --image = ibmcom/guestbook:v1 1. Scale apps with replicas \u00b6 A replica is a copy of a pod that contains a running service. By having multiple replicas of a pod, you can ensure your deployment has the available resources to handle increasing load on your application. kubectl provides a scale subcommand to change the size of an existing deployment. Let's increase our capacity from a single running instance of guestbook up to 10 instances: kubectl scale --replicas = 10 deployment guestbook Kubernetes will now try to make reality match the desired state of 10 replicas by starting 9 new pods with the same configuration as the first. To see your changes being rolled out, you can run: kubectl rollout status deployment guestbook The rollout might occur so quickly that the following messages might not display: $ kubectl rollout status deployment guestbook Waiting for rollout to finish: 1 of 10 updated replicas are available... Waiting for rollout to finish: 2 of 10 updated replicas are available... Waiting for rollout to finish: 3 of 10 updated replicas are available... Waiting for rollout to finish: 4 of 10 updated replicas are available... Waiting for rollout to finish: 5 of 10 updated replicas are available... Waiting for rollout to finish: 6 of 10 updated replicas are available... Waiting for rollout to finish: 7 of 10 updated replicas are available... Waiting for rollout to finish: 8 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... deployment \"guestbook\" successfully rolled out Once the rollout has finished, ensure your pods are running by using: kubectl get pods You should see output listing 10 replicas of your deployment: $ kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-562211614-1tqm7 1 /1 Running 0 1d guestbook-562211614-1zqn4 1 /1 Running 0 2m guestbook-562211614-5htdz 1 /1 Running 0 2m guestbook-562211614-6h04h 1 /1 Running 0 2m guestbook-562211614-ds9hb 1 /1 Running 0 2m guestbook-562211614-nb5qp 1 /1 Running 0 2m guestbook-562211614-vtfp2 1 /1 Running 0 2m guestbook-562211614-vz5qw 1 /1 Running 0 2m guestbook-562211614-zksw3 1 /1 Running 0 2m guestbook-562211614-zsp0j 1 /1 Running 0 2m Tip: Another way to improve availability is to add clusters and regions to your deployment, as shown in the following diagram: 2. Update and roll back apps \u00b6 Kubernetes allows you to do rolling upgrade of your application to a new container image. This allows you to easily update the running image and also allows you to easily undo a rollout if a problem is discovered during or after deployment. In the previous lab, we used an image with a v1 tag. For our upgrade we'll use the image with the v2 tag. To update and roll back: Using kubectl , you can now update your deployment to use the v2 image. kubectl allows you to change details about existing resources with the set subcommand. We can use it to change the image being used. kubectl set image deployment/guestbook guestbook = ibmcom/guestbook:v2 Note that a pod could have multiple containers, each with its own name. Each image can be changed individually or all at once by referring to the name. In the case of our guestbook Deployment, the container name is also guestbook . Multiple containers can be updated at the same time. ( More information .) To check the status of the rollout, run: kubectl rollout status deployment/guestbook The rollout might occur so quickly that the following messages might not display: $ kubectl rollout status deployment/guestbook Waiting for rollout to finish: 2 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 9 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... deployment \"guestbook\" successfully rolled out Test the application as before, by accessing <public-IP>:<nodeport> in the browser to confirm your new code is active. Remember, to get the \"nodeport\" and \"public-ip\" use the following commands. Replace $CLUSTER_NAME with the name of your cluster if the environment variable is not set.: kubectl describe service guestbook and kubectl get nodes -o wide To verify that you're running \"v2\" of guestbook, look at the title of the page, it should now be Guestbook - v2 . If you are using a browser, make sure you force refresh (invalidating your cache). If you want to undo your latest rollout, use: kubectl rollout undo deployment guestbook You can then use this command to see the status: kubectl rollout status deployment/guestbook When doing a rollout, you see references to old replicas and new replicas. The old replicas are the original 10 pods deployed when we scaled the application. The new replicas come from the newly created pods with the different image. All of these pods are owned by the Deployment. The deployment manages these two sets of pods with a resource called a ReplicaSet. We can see the guestbook ReplicaSets with: $ kubectl get replicasets -l app = guestbook NAME DESIRED CURRENT READY AGE guestbook-5f5548d4f 10 10 10 21m guestbook-768cc55c78 0 0 0 3h Before we continue, let's delete the application so we can learn about a different way to achieve the same results: To remove the deployment, use kubectl delete deployment guestbook To remove the service, use: kubectl delete service guestbook Congratulations! You deployed the second version of the app. Lab 2 is now complete. Continue to the next lab of this course .","title":"Lab 2. Scale and update deployments"},{"location":"generatedContent/kube101/Lab2/#lab-2-scale-and-update-deployments","text":"In this lab, you'll learn how to update the number of instances a deployment has and how to safely roll out an update of your application on Kubernetes. For this lab, you need a running deployment of the guestbook application from the previous lab. If you need to create it, run: kubectl create deployment guestbook --image = ibmcom/guestbook:v1","title":"Lab 2: Scale and Update Deployments"},{"location":"generatedContent/kube101/Lab2/#1-scale-apps-with-replicas","text":"A replica is a copy of a pod that contains a running service. By having multiple replicas of a pod, you can ensure your deployment has the available resources to handle increasing load on your application. kubectl provides a scale subcommand to change the size of an existing deployment. Let's increase our capacity from a single running instance of guestbook up to 10 instances: kubectl scale --replicas = 10 deployment guestbook Kubernetes will now try to make reality match the desired state of 10 replicas by starting 9 new pods with the same configuration as the first. To see your changes being rolled out, you can run: kubectl rollout status deployment guestbook The rollout might occur so quickly that the following messages might not display: $ kubectl rollout status deployment guestbook Waiting for rollout to finish: 1 of 10 updated replicas are available... Waiting for rollout to finish: 2 of 10 updated replicas are available... Waiting for rollout to finish: 3 of 10 updated replicas are available... Waiting for rollout to finish: 4 of 10 updated replicas are available... Waiting for rollout to finish: 5 of 10 updated replicas are available... Waiting for rollout to finish: 6 of 10 updated replicas are available... Waiting for rollout to finish: 7 of 10 updated replicas are available... Waiting for rollout to finish: 8 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... deployment \"guestbook\" successfully rolled out Once the rollout has finished, ensure your pods are running by using: kubectl get pods You should see output listing 10 replicas of your deployment: $ kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-562211614-1tqm7 1 /1 Running 0 1d guestbook-562211614-1zqn4 1 /1 Running 0 2m guestbook-562211614-5htdz 1 /1 Running 0 2m guestbook-562211614-6h04h 1 /1 Running 0 2m guestbook-562211614-ds9hb 1 /1 Running 0 2m guestbook-562211614-nb5qp 1 /1 Running 0 2m guestbook-562211614-vtfp2 1 /1 Running 0 2m guestbook-562211614-vz5qw 1 /1 Running 0 2m guestbook-562211614-zksw3 1 /1 Running 0 2m guestbook-562211614-zsp0j 1 /1 Running 0 2m Tip: Another way to improve availability is to add clusters and regions to your deployment, as shown in the following diagram:","title":"1. Scale apps with replicas"},{"location":"generatedContent/kube101/Lab2/#2-update-and-roll-back-apps","text":"Kubernetes allows you to do rolling upgrade of your application to a new container image. This allows you to easily update the running image and also allows you to easily undo a rollout if a problem is discovered during or after deployment. In the previous lab, we used an image with a v1 tag. For our upgrade we'll use the image with the v2 tag. To update and roll back: Using kubectl , you can now update your deployment to use the v2 image. kubectl allows you to change details about existing resources with the set subcommand. We can use it to change the image being used. kubectl set image deployment/guestbook guestbook = ibmcom/guestbook:v2 Note that a pod could have multiple containers, each with its own name. Each image can be changed individually or all at once by referring to the name. In the case of our guestbook Deployment, the container name is also guestbook . Multiple containers can be updated at the same time. ( More information .) To check the status of the rollout, run: kubectl rollout status deployment/guestbook The rollout might occur so quickly that the following messages might not display: $ kubectl rollout status deployment/guestbook Waiting for rollout to finish: 2 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 9 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... deployment \"guestbook\" successfully rolled out Test the application as before, by accessing <public-IP>:<nodeport> in the browser to confirm your new code is active. Remember, to get the \"nodeport\" and \"public-ip\" use the following commands. Replace $CLUSTER_NAME with the name of your cluster if the environment variable is not set.: kubectl describe service guestbook and kubectl get nodes -o wide To verify that you're running \"v2\" of guestbook, look at the title of the page, it should now be Guestbook - v2 . If you are using a browser, make sure you force refresh (invalidating your cache). If you want to undo your latest rollout, use: kubectl rollout undo deployment guestbook You can then use this command to see the status: kubectl rollout status deployment/guestbook When doing a rollout, you see references to old replicas and new replicas. The old replicas are the original 10 pods deployed when we scaled the application. The new replicas come from the newly created pods with the different image. All of these pods are owned by the Deployment. The deployment manages these two sets of pods with a resource called a ReplicaSet. We can see the guestbook ReplicaSets with: $ kubectl get replicasets -l app = guestbook NAME DESIRED CURRENT READY AGE guestbook-5f5548d4f 10 10 10 21m guestbook-768cc55c78 0 0 0 3h Before we continue, let's delete the application so we can learn about a different way to achieve the same results: To remove the deployment, use kubectl delete deployment guestbook To remove the service, use: kubectl delete service guestbook Congratulations! You deployed the second version of the app. Lab 2 is now complete. Continue to the next lab of this course .","title":"2. Update and roll back apps"},{"location":"generatedContent/kube101/Lab3/","text":"Lab 3: Scale and update apps natively, building multi-tier applications \u00b6 In this lab you'll learn how to deploy the same guestbook application we deployed in the previous labs, however, instead of using the kubectl command line helper functions we'll be deploying the application using configuration files. The configuration file mechanism allows you to have more fine-grained control over all of resources being created within the Kubernetes cluster. Before we work with the application we need to clone a github repo: git clone https://github.com/IBM/guestbook.git This repo contains multiple versions of the guestbook application as well as the configuration files we'll use to deploy the pieces of the application. Change directory by running the command cd guestbook/v1 You will find all the configurations files for this exercise in this directory. 1. Scale apps natively \u00b6 Kubernetes can deploy an individual pod to run an application but when you need to scale it to handle a large number of requests a Deployment is the resource you want to use. A Deployment manages a collection of similar pods. When you ask for a specific number of replicas the Kubernetes Deployment Controller will attempt to maintain that number of replicas at all times. Every Kubernetes object we create should provide two nested object fields that govern the object\u2019s configuration: the object spec and the object status . Object spec defines the desired state, and object status contains Kubernetes system provided information about the actual state of the resource. As described before, Kubernetes will attempt to reconcile your desired state with the actual state of the system. For Object that we create we need to provide the apiVersion you are using to create the object, kind of the object we are creating and the metadata about the object such as a name , set of labels and optionally namespace that this object should belong. Consider the following deployment configuration for guestbook application guestbook-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : guestbook-v1 labels : app : guestbook version : \"1.0\" spec : replicas : 3 selector : matchLabels : app : guestbook template : metadata : labels : app : guestbook version : \"1.0\" spec : containers : - name : guestbook image : ibmcom/guestbook:v1 ports : - name : http-server containerPort : 3000 The above configuration file create a deployment object named 'guestbook' with a pod containing a single container running the image ibmcom/guestbook:v1 . Also the configuration specifies replicas set to 3 and Kubernetes tries to make sure that at least three active pods are running at all times. Create guestbook deployment To create a Deployment using this configuration file we use the following command: kubectl create -f guestbook-deployment.yaml List the pod with label app=guestbook We can then list the pods it created by listing all pods that have a label of \"app\" with a value of \"guestbook\". This matches the labels defined above in the yaml file in the spec.template.metadata.labels section. kubectl get pods -l app = guestbook When you change the number of replicas in the configuration, Kubernetes will try to add, or remove, pods from the system to match your request. To can make these modifications by using the following command: kubectl edit deployment guestbook-v1 This will retrieve the latest configuration for the Deployment from the Kubernetes server and then load it into an editor for you. You'll notice that there are a lot more fields in this version than the original yaml file we used. This is because it contains all of the properties about the Deployment that Kubernetes knows about, not just the ones we chose to specify when we create it. Also notice that it now contains the status section mentioned previously. To exit the vi editor, type :q! , of if you made changes that you want to see reflected, save them using :wq . You can also edit the deployment file we used to create the Deployment to make changes. You should use the following command to make the change effective when you edit the deployment locally. kubectl apply -f guestbook-deployment.yaml This will ask Kubernetes to \"diff\" our yaml file with the current state of the Deployment and apply just those changes. We can now define a Service object to expose the deployment to external clients. guestbook-service.yaml apiVersion : v1 kind : Service metadata : name : guestbook labels : app : guestbook spec : ports : - port : 3000 targetPort : http-server selector : app : guestbook type : LoadBalancer The above configuration creates a Service resource named guestbook. A Service can be used to create a network path for incoming traffic to your running application. In this case, we are setting up a route from port 3000 on the cluster to the \"http-server\" port on our app, which is port 3000 per the Deployment container spec. Let us now create the guestbook service using the same type of command we used when we created the Deployment: kubectl create -f guestbook-service.yaml Test guestbook app using a browser of your choice using the url <your-cluster-ip>:<node-port> Remember, to get the nodeport and public-ip use the following commands, replacing $CLUSTER_NAME with the name of your cluster if the environment variable is not already set. kubectl describe service guestbook and kubectl get nodes -o wide 2. Connect to a back-end service \u00b6 If you look at the guestbook source code, under the guestbook/v1/guestbook directory, you'll notice that it is written to support a variety of data stores. By default it will keep the log of guestbook entries in memory. That's ok for testing purposes, but as you get into a more \"real\" environment where you scale your application that model will not work because based on which instance of the application the user is routed to they'll see very different results. To solve this we need to have all instances of our app share the same data store - in this case we're going to use a redis database that we deploy to our cluster. This instance of redis will be defined in a similar manner to the guestbook. redis-master-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : redis-master labels : app : redis role : master spec : replicas : 1 selector : matchLabels : app : redis role : master template : metadata : labels : app : redis role : master spec : containers : - name : redis-master image : redis:3.2.9 ports : - name : redis-server containerPort : 6379 This yaml creates a redis database in a Deployment named 'redis-master'. It will create a single instance, with replicas set to 1, and the guestbook app instances will connect to it to persist data, as well as read the persisted data back. The image running in the container is 'redis:3.2.9' and exposes the standard redis port 6379. Create a redis Deployment, like we did for guestbook: kubectl create -f redis-master-deployment.yaml Check to see that redis server pod is running: $ kubectl get pods -lapp = redis,role = master NAME READY STATUS RESTARTS AGE redis-master-q9zg7 1 /1 Running 0 2d Let us test the redis standalone. Replace the pod name redis-master-q9zg7 with the name of your pod. kubectl exec -it redis-master-q9zg7 redis-cli The kubectl exec command will start a secondary process in the specified container. In this case we're asking for the \"redis-cli\" command to be executed in the container named \"redis-master-q9zg7\". When this process ends the \"kubectl exec\" command will also exit but the other processes in the container will not be impacted. Once in the container we can use the \"redis-cli\" command to make sure the redis database is running properly, or to configure it if needed. redis-cli> ping PONG redis-cli> exit Now we need to expose the redis-master Deployment as a Service so that the guestbook application can connect to it through DNS lookup. redis-master-service.yaml apiVersion : v1 kind : Service metadata : name : redis-master labels : app : redis role : master spec : ports : - port : 6379 targetPort : redis-server selector : app : redis role : master This creates a Service object named 'redis-master' and configures it to target port 6379 on the pods selected by the selectors \"app=redis\" and \"role=master\". Create the service to access redis master: kubectl create -f redis-master-service.yaml Restart guestbook so that it will find the redis service to use database: kubectl delete deploy guestbook-v1 kubectl create -f guestbook-deployment.yaml Test guestbook app using a browser of your choice using the url <your-cluster-ip>:<node-port> , or by refreshing the page if you already have the app open in another window. You can see now that if you open up multiple browsers and refresh the page to access the different copies of guestbook that they all have a consistent state. All instances write to the same backing persistent storage, and all instances read from that storage to display the guestbook entries that have been stored. We have our simple 3-tier application running but we need to scale the application if traffic increases. Our main bottleneck is that we only have one database server to process each request coming though guestbook. One simple solution is to separate the reads and write such that they go to different databases that are replicated properly to achieve data consistency. Create a deployment named 'redis-slave' that can talk to redis database to manage data reads. In order to scale the database we use the pattern where we can scale the reads using redis slave deployment which can run several instances to read. Redis slave deployments is configured to run two replicas. redis-slave-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : redis-slave labels : app : redis role : slave spec : replicas : 2 selector : matchLabels : app : redis role : slave template : metadata : labels : app : redis role : slave spec : containers : - name : redis-slave image : ibmcom/guestbook-redis-slave:v2 ports : - name : redis-server containerPort : 6379 Create the pod running redis slave deployment. kubectl create -f redis-slave-deployment.yaml Check if all the slave replicas are running $ kubectl get pods -lapp = redis,role = slave NAME READY STATUS RESTARTS AGE redis-slave-kd7vx 1 /1 Running 0 2d redis-slave-wwcxw 1 /1 Running 0 2d And then go into one of those pods and look at the database to see that everything looks right. Replace the pod name redis-slave-kd7vx with your own pod name. If you get the back (empty list or set) when you print the keys, go to the guestbook application and add an entry! $ kubectl exec -it redis-slave-kd7vx redis-cli 127 .0.0.1:6379> keys * 1 ) \"guestbook\" 127 .0.0.1:6379> lrange guestbook 0 10 1 ) \"hello world\" 2 ) \"welcome to the Kube workshop\" 127 .0.0.1:6379> exit Deploy redis slave service so we can access it by DNS name. Once redeployed, the application will send \"read\" operations to the redis-slave pods while \"write\" operations will go to the redis-master pods. redis-slave-service.yaml apiVersion : v1 kind : Service metadata : name : redis-slave labels : app : redis role : slave spec : ports : - port : 6379 targetPort : redis-server selector : app : redis role : slave Create the service to access redis slaves. kubectl create -f redis-slave-service.yaml Restart guestbook so that it will find the slave service to read from. kubectl delete deploy guestbook-v1 kubectl create -f guestbook-deployment.yaml Test guestbook app using a browser of your choice using the url <your-cluster-ip>:<node-port> , or by refreshing the page if you have the app open in another window. That's the end of the lab. Now let's clean-up our environment: kubectl delete -f guestbook-deployment.yaml kubectl delete -f guestbook-service.yaml kubectl delete -f redis-slave-service.yaml kubectl delete -f redis-slave-deployment.yaml kubectl delete -f redis-master-service.yaml kubectl delete -f redis-master-deployment.yaml","title":"Lab 3. Build multi-tier applications"},{"location":"generatedContent/kube101/Lab3/#lab-3-scale-and-update-apps-natively-building-multi-tier-applications","text":"In this lab you'll learn how to deploy the same guestbook application we deployed in the previous labs, however, instead of using the kubectl command line helper functions we'll be deploying the application using configuration files. The configuration file mechanism allows you to have more fine-grained control over all of resources being created within the Kubernetes cluster. Before we work with the application we need to clone a github repo: git clone https://github.com/IBM/guestbook.git This repo contains multiple versions of the guestbook application as well as the configuration files we'll use to deploy the pieces of the application. Change directory by running the command cd guestbook/v1 You will find all the configurations files for this exercise in this directory.","title":"Lab 3: Scale and update apps natively, building multi-tier applications"},{"location":"generatedContent/kube101/Lab3/#1-scale-apps-natively","text":"Kubernetes can deploy an individual pod to run an application but when you need to scale it to handle a large number of requests a Deployment is the resource you want to use. A Deployment manages a collection of similar pods. When you ask for a specific number of replicas the Kubernetes Deployment Controller will attempt to maintain that number of replicas at all times. Every Kubernetes object we create should provide two nested object fields that govern the object\u2019s configuration: the object spec and the object status . Object spec defines the desired state, and object status contains Kubernetes system provided information about the actual state of the resource. As described before, Kubernetes will attempt to reconcile your desired state with the actual state of the system. For Object that we create we need to provide the apiVersion you are using to create the object, kind of the object we are creating and the metadata about the object such as a name , set of labels and optionally namespace that this object should belong. Consider the following deployment configuration for guestbook application guestbook-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : guestbook-v1 labels : app : guestbook version : \"1.0\" spec : replicas : 3 selector : matchLabels : app : guestbook template : metadata : labels : app : guestbook version : \"1.0\" spec : containers : - name : guestbook image : ibmcom/guestbook:v1 ports : - name : http-server containerPort : 3000 The above configuration file create a deployment object named 'guestbook' with a pod containing a single container running the image ibmcom/guestbook:v1 . Also the configuration specifies replicas set to 3 and Kubernetes tries to make sure that at least three active pods are running at all times. Create guestbook deployment To create a Deployment using this configuration file we use the following command: kubectl create -f guestbook-deployment.yaml List the pod with label app=guestbook We can then list the pods it created by listing all pods that have a label of \"app\" with a value of \"guestbook\". This matches the labels defined above in the yaml file in the spec.template.metadata.labels section. kubectl get pods -l app = guestbook When you change the number of replicas in the configuration, Kubernetes will try to add, or remove, pods from the system to match your request. To can make these modifications by using the following command: kubectl edit deployment guestbook-v1 This will retrieve the latest configuration for the Deployment from the Kubernetes server and then load it into an editor for you. You'll notice that there are a lot more fields in this version than the original yaml file we used. This is because it contains all of the properties about the Deployment that Kubernetes knows about, not just the ones we chose to specify when we create it. Also notice that it now contains the status section mentioned previously. To exit the vi editor, type :q! , of if you made changes that you want to see reflected, save them using :wq . You can also edit the deployment file we used to create the Deployment to make changes. You should use the following command to make the change effective when you edit the deployment locally. kubectl apply -f guestbook-deployment.yaml This will ask Kubernetes to \"diff\" our yaml file with the current state of the Deployment and apply just those changes. We can now define a Service object to expose the deployment to external clients. guestbook-service.yaml apiVersion : v1 kind : Service metadata : name : guestbook labels : app : guestbook spec : ports : - port : 3000 targetPort : http-server selector : app : guestbook type : LoadBalancer The above configuration creates a Service resource named guestbook. A Service can be used to create a network path for incoming traffic to your running application. In this case, we are setting up a route from port 3000 on the cluster to the \"http-server\" port on our app, which is port 3000 per the Deployment container spec. Let us now create the guestbook service using the same type of command we used when we created the Deployment: kubectl create -f guestbook-service.yaml Test guestbook app using a browser of your choice using the url <your-cluster-ip>:<node-port> Remember, to get the nodeport and public-ip use the following commands, replacing $CLUSTER_NAME with the name of your cluster if the environment variable is not already set. kubectl describe service guestbook and kubectl get nodes -o wide","title":"1. Scale apps natively"},{"location":"generatedContent/kube101/Lab3/#2-connect-to-a-back-end-service","text":"If you look at the guestbook source code, under the guestbook/v1/guestbook directory, you'll notice that it is written to support a variety of data stores. By default it will keep the log of guestbook entries in memory. That's ok for testing purposes, but as you get into a more \"real\" environment where you scale your application that model will not work because based on which instance of the application the user is routed to they'll see very different results. To solve this we need to have all instances of our app share the same data store - in this case we're going to use a redis database that we deploy to our cluster. This instance of redis will be defined in a similar manner to the guestbook. redis-master-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : redis-master labels : app : redis role : master spec : replicas : 1 selector : matchLabels : app : redis role : master template : metadata : labels : app : redis role : master spec : containers : - name : redis-master image : redis:3.2.9 ports : - name : redis-server containerPort : 6379 This yaml creates a redis database in a Deployment named 'redis-master'. It will create a single instance, with replicas set to 1, and the guestbook app instances will connect to it to persist data, as well as read the persisted data back. The image running in the container is 'redis:3.2.9' and exposes the standard redis port 6379. Create a redis Deployment, like we did for guestbook: kubectl create -f redis-master-deployment.yaml Check to see that redis server pod is running: $ kubectl get pods -lapp = redis,role = master NAME READY STATUS RESTARTS AGE redis-master-q9zg7 1 /1 Running 0 2d Let us test the redis standalone. Replace the pod name redis-master-q9zg7 with the name of your pod. kubectl exec -it redis-master-q9zg7 redis-cli The kubectl exec command will start a secondary process in the specified container. In this case we're asking for the \"redis-cli\" command to be executed in the container named \"redis-master-q9zg7\". When this process ends the \"kubectl exec\" command will also exit but the other processes in the container will not be impacted. Once in the container we can use the \"redis-cli\" command to make sure the redis database is running properly, or to configure it if needed. redis-cli> ping PONG redis-cli> exit Now we need to expose the redis-master Deployment as a Service so that the guestbook application can connect to it through DNS lookup. redis-master-service.yaml apiVersion : v1 kind : Service metadata : name : redis-master labels : app : redis role : master spec : ports : - port : 6379 targetPort : redis-server selector : app : redis role : master This creates a Service object named 'redis-master' and configures it to target port 6379 on the pods selected by the selectors \"app=redis\" and \"role=master\". Create the service to access redis master: kubectl create -f redis-master-service.yaml Restart guestbook so that it will find the redis service to use database: kubectl delete deploy guestbook-v1 kubectl create -f guestbook-deployment.yaml Test guestbook app using a browser of your choice using the url <your-cluster-ip>:<node-port> , or by refreshing the page if you already have the app open in another window. You can see now that if you open up multiple browsers and refresh the page to access the different copies of guestbook that they all have a consistent state. All instances write to the same backing persistent storage, and all instances read from that storage to display the guestbook entries that have been stored. We have our simple 3-tier application running but we need to scale the application if traffic increases. Our main bottleneck is that we only have one database server to process each request coming though guestbook. One simple solution is to separate the reads and write such that they go to different databases that are replicated properly to achieve data consistency. Create a deployment named 'redis-slave' that can talk to redis database to manage data reads. In order to scale the database we use the pattern where we can scale the reads using redis slave deployment which can run several instances to read. Redis slave deployments is configured to run two replicas. redis-slave-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : redis-slave labels : app : redis role : slave spec : replicas : 2 selector : matchLabels : app : redis role : slave template : metadata : labels : app : redis role : slave spec : containers : - name : redis-slave image : ibmcom/guestbook-redis-slave:v2 ports : - name : redis-server containerPort : 6379 Create the pod running redis slave deployment. kubectl create -f redis-slave-deployment.yaml Check if all the slave replicas are running $ kubectl get pods -lapp = redis,role = slave NAME READY STATUS RESTARTS AGE redis-slave-kd7vx 1 /1 Running 0 2d redis-slave-wwcxw 1 /1 Running 0 2d And then go into one of those pods and look at the database to see that everything looks right. Replace the pod name redis-slave-kd7vx with your own pod name. If you get the back (empty list or set) when you print the keys, go to the guestbook application and add an entry! $ kubectl exec -it redis-slave-kd7vx redis-cli 127 .0.0.1:6379> keys * 1 ) \"guestbook\" 127 .0.0.1:6379> lrange guestbook 0 10 1 ) \"hello world\" 2 ) \"welcome to the Kube workshop\" 127 .0.0.1:6379> exit Deploy redis slave service so we can access it by DNS name. Once redeployed, the application will send \"read\" operations to the redis-slave pods while \"write\" operations will go to the redis-master pods. redis-slave-service.yaml apiVersion : v1 kind : Service metadata : name : redis-slave labels : app : redis role : slave spec : ports : - port : 6379 targetPort : redis-server selector : app : redis role : slave Create the service to access redis slaves. kubectl create -f redis-slave-service.yaml Restart guestbook so that it will find the slave service to read from. kubectl delete deploy guestbook-v1 kubectl create -f guestbook-deployment.yaml Test guestbook app using a browser of your choice using the url <your-cluster-ip>:<node-port> , or by refreshing the page if you have the app open in another window. That's the end of the lab. Now let's clean-up our environment: kubectl delete -f guestbook-deployment.yaml kubectl delete -f guestbook-service.yaml kubectl delete -f redis-slave-service.yaml kubectl delete -f redis-slave-deployment.yaml kubectl delete -f redis-master-service.yaml kubectl delete -f redis-master-deployment.yaml","title":"2. Connect to a back-end service"},{"location":"generatedContent/kube101/Lab4/","text":"UNDER CONSTRUCTION \u00b6 1. Check the health of apps \u00b6 Kubernetes uses availability checks (liveness probes) to know when to restart a container. For example, liveness probes could catch a deadlock, where an application is running, but unable to make progress. Restarting a container in such a state can help to make the application more available despite bugs. Also, Kubernetes uses readiness checks to know when a container is ready to start accepting traffic. A pod is considered ready when all of its containers are ready. One use of this check is to control which pods are used as backends for services. When a pod is not ready, it is removed from load balancers. In this example, we have defined a HTTP liveness probe to check health of the container every five seconds. For the first 10-15 seconds the /healthz returns a 200 response and will fail afterward. Kubernetes will automatically restart the service. Open the healthcheck.yml file with a text editor. This configuration script combines a few steps from the previous lesson to create a deployment and a service at the same time. App developers can use these scripts when updates are made or to troubleshoot issues by re-creating the pods: Update the details for the image in your private registry namespace: image : \"ibmcom/guestbook:v2\" Note the HTTP liveness probe that checks the health of the container every five seconds. livenessProbe : httpGet : path : /healthz port : 3000 initialDelaySeconds : 5 periodSeconds : 5 In the Service section, note the NodePort . Rather than generating a random NodePort like you did in the previous lesson, you can specify a port in the 30000 - 32767 range. This example uses 30072. Run the configuration script in the cluster. When the deployment and the service are created, the app is available for anyone to see: kubectl apply -f healthcheck.yml Now that all the deployment work is done, check how everything turned out. You might notice that because more instances are running, things might run a bit slower. Open a browser and check out the app. To form the URL, combine the IP with the NodePort that was specified in the configuration script. To get the public IP address for the worker node: ibmcloud cs workers <cluster-name> In a browser, you'll see a success message. If you do not see this text, don't worry. This app is designed to go up and down. For the first 10 - 15 seconds, a 200 message is returned, so you know that the app is running successfully. After those 15 seconds, a timeout message is displayed, as is designed in the app. Launch your Kubernetes dashboard: Get your credentials for Kubernetes. kubectl config view -o jsonpath = '{.users[0].user.auth-provider.config.id-token}' Copy the id-token value that is shown in the output. Set the proxy with the default port number. kubectl proxy Output: Starting to serve on 127 .0.0.1:8001 Sign in to the dashboard. Open the following URL in a web browser. http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/ In the sign-on page, select the Token authentication method. Then, paste the id-token value that you previously copied into the Token field and click SIGN IN . In the Workloads tab, you can see the resources that you created. From this tab, you can continually refresh and see that the health check is working. In the Pods section, you can see how many times the pods are restarted when the containers in them are re-created. You might happen to catch errors in the dashboard, indicating that the health check caught a problem. Give it a few minutes and refresh again. You see the number of restarts changes for each pod. Ready to delete what you created before you continue? This time, you can use the same configuration script to delete both of the resources you created. kubectl delete -f healthcheck.yml When you are done exploring the Kubernetes dashboard, in your CLI, enter CTRL+C to exit the proxy command.","title":"***UNDER CONSTRUCTION***"},{"location":"generatedContent/kube101/Lab4/#under-construction","text":"","title":"UNDER CONSTRUCTION"},{"location":"generatedContent/kube101/Lab4/#1-check-the-health-of-apps","text":"Kubernetes uses availability checks (liveness probes) to know when to restart a container. For example, liveness probes could catch a deadlock, where an application is running, but unable to make progress. Restarting a container in such a state can help to make the application more available despite bugs. Also, Kubernetes uses readiness checks to know when a container is ready to start accepting traffic. A pod is considered ready when all of its containers are ready. One use of this check is to control which pods are used as backends for services. When a pod is not ready, it is removed from load balancers. In this example, we have defined a HTTP liveness probe to check health of the container every five seconds. For the first 10-15 seconds the /healthz returns a 200 response and will fail afterward. Kubernetes will automatically restart the service. Open the healthcheck.yml file with a text editor. This configuration script combines a few steps from the previous lesson to create a deployment and a service at the same time. App developers can use these scripts when updates are made or to troubleshoot issues by re-creating the pods: Update the details for the image in your private registry namespace: image : \"ibmcom/guestbook:v2\" Note the HTTP liveness probe that checks the health of the container every five seconds. livenessProbe : httpGet : path : /healthz port : 3000 initialDelaySeconds : 5 periodSeconds : 5 In the Service section, note the NodePort . Rather than generating a random NodePort like you did in the previous lesson, you can specify a port in the 30000 - 32767 range. This example uses 30072. Run the configuration script in the cluster. When the deployment and the service are created, the app is available for anyone to see: kubectl apply -f healthcheck.yml Now that all the deployment work is done, check how everything turned out. You might notice that because more instances are running, things might run a bit slower. Open a browser and check out the app. To form the URL, combine the IP with the NodePort that was specified in the configuration script. To get the public IP address for the worker node: ibmcloud cs workers <cluster-name> In a browser, you'll see a success message. If you do not see this text, don't worry. This app is designed to go up and down. For the first 10 - 15 seconds, a 200 message is returned, so you know that the app is running successfully. After those 15 seconds, a timeout message is displayed, as is designed in the app. Launch your Kubernetes dashboard: Get your credentials for Kubernetes. kubectl config view -o jsonpath = '{.users[0].user.auth-provider.config.id-token}' Copy the id-token value that is shown in the output. Set the proxy with the default port number. kubectl proxy Output: Starting to serve on 127 .0.0.1:8001 Sign in to the dashboard. Open the following URL in a web browser. http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/ In the sign-on page, select the Token authentication method. Then, paste the id-token value that you previously copied into the Token field and click SIGN IN . In the Workloads tab, you can see the resources that you created. From this tab, you can continually refresh and see that the health check is working. In the Pods section, you can see how many times the pods are restarted when the containers in them are re-created. You might happen to catch errors in the dashboard, indicating that the health check caught a problem. Give it a few minutes and refresh again. You see the number of restarts changes for each pod. Ready to delete what you created before you continue? This time, you can use the same configuration script to delete both of the resources you created. kubectl delete -f healthcheck.yml When you are done exploring the Kubernetes dashboard, in your CLI, enter CTRL+C to exit the proxy command.","title":"1. Check the health of apps"},{"location":"generatedContent/kube101/LabD/","text":"Optional Debugging Lab - Tips and Tricks for Debugging Applications in Kubernetes \u00b6 Advanced debugging techniques to reach your pods. Pod Logs \u00b6 You can look at the logs of any of the pods running under your deployments as follows kubectl logs <podname> Remember that if you have multiple containers running in your pod, you have to specify the specific container you want to see logs from. kubectl logs <pod-name> <container-name> This subcommand operates like tail . Including the -f flag will continue to stream the logs live once the current time is reached. kubectl edit and vi \u00b6 By default, on many Linux and macOS systems, you will be dropped into the editor vi . export EDITOR = nano On Windows, a copy of notepad.exe will be opened with the contents of the file. busybox pod \u00b6 For debugging live, this command frequently helps me: kubectl create deployment bb --image busybox --restart = Never -it --rm In the busybox image is a basic shell that contains useful utilities. Utils I often use are nslookup and wget . nslookup is useful for testing DNS resolution in a pod. wget is useful for trying to do network requests. Service Endpoints \u00b6 Endpoint resource can be used to see all the service endpoints. kubectl get endpoints <service> ImagePullPolicy \u00b6 By default Kubernetes will only pull the image on first use. This can be confusing during development when you expect changes to show up. You should be aware of the three ImagePullPolicy s: IfNotPresent - the default, only request the image if not present. Always - always request the image. Never More details on image management may be found here .","title":"Optional Debugging Lab - Tips and Tricks for Debugging Applications in Kubernetes"},{"location":"generatedContent/kube101/LabD/#optional-debugging-lab-tips-and-tricks-for-debugging-applications-in-kubernetes","text":"Advanced debugging techniques to reach your pods.","title":"Optional Debugging Lab - Tips and Tricks for Debugging Applications in Kubernetes"},{"location":"generatedContent/kube101/LabD/#pod-logs","text":"You can look at the logs of any of the pods running under your deployments as follows kubectl logs <podname> Remember that if you have multiple containers running in your pod, you have to specify the specific container you want to see logs from. kubectl logs <pod-name> <container-name> This subcommand operates like tail . Including the -f flag will continue to stream the logs live once the current time is reached.","title":"Pod Logs"},{"location":"generatedContent/kube101/LabD/#kubectl-edit-and-vi","text":"By default, on many Linux and macOS systems, you will be dropped into the editor vi . export EDITOR = nano On Windows, a copy of notepad.exe will be opened with the contents of the file.","title":"kubectl edit and vi"},{"location":"generatedContent/kube101/LabD/#busybox-pod","text":"For debugging live, this command frequently helps me: kubectl create deployment bb --image busybox --restart = Never -it --rm In the busybox image is a basic shell that contains useful utilities. Utils I often use are nslookup and wget . nslookup is useful for testing DNS resolution in a pod. wget is useful for trying to do network requests.","title":"busybox pod"},{"location":"generatedContent/kube101/LabD/#service-endpoints","text":"Endpoint resource can be used to see all the service endpoints. kubectl get endpoints <service>","title":"Service Endpoints"},{"location":"generatedContent/kube101/LabD/#imagepullpolicy","text":"By default Kubernetes will only pull the image on first use. This can be confusing during development when you expect changes to show up. You should be aware of the three ImagePullPolicy s: IfNotPresent - the default, only request the image if not present. Always - always request the image. Never More details on image management may be found here .","title":"ImagePullPolicy"},{"location":"generatedContent/kubernetes-operators/","text":"Kubernetes Operators \u00b6 Kubernetes Operators is a series of labs about deploying applications to Kubernetes using the Extensions API to create Custom Resources (CR) and customize controllers using the Operator Pattern. This workshop uses the Operator Framework to create operators. Pre-requirements \u00b6 a Free IBM Cloud account, to create a new IBM Cloud account, follow the instructions here . a Red Hat OpenShift Kubernetes Service (ROKS) v4.5 using a cluster with admin rights, CognitiveLabs.ai account, to access a client terminal at CognitiveLabs.ai, follow the instructions here . Labs \u00b6 Setup , Lab 1 Create a Custom Resource (CR) , Lab2 Create an Operator of Type Go using the Operator SDK , Lab3 Create an Operator using an Existing Helm Chart Contributors \u00b6 Rojan Jose, rojanjose Remko de Knikker, remkohdev","title":"Kubernetes Operators"},{"location":"generatedContent/kubernetes-operators/#kubernetes-operators","text":"Kubernetes Operators is a series of labs about deploying applications to Kubernetes using the Extensions API to create Custom Resources (CR) and customize controllers using the Operator Pattern. This workshop uses the Operator Framework to create operators.","title":"Kubernetes Operators"},{"location":"generatedContent/kubernetes-operators/#pre-requirements","text":"a Free IBM Cloud account, to create a new IBM Cloud account, follow the instructions here . a Red Hat OpenShift Kubernetes Service (ROKS) v4.5 using a cluster with admin rights, CognitiveLabs.ai account, to access a client terminal at CognitiveLabs.ai, follow the instructions here .","title":"Pre-requirements"},{"location":"generatedContent/kubernetes-operators/#labs","text":"Setup , Lab 1 Create a Custom Resource (CR) , Lab2 Create an Operator of Type Go using the Operator SDK , Lab3 Create an Operator using an Existing Helm Chart","title":"Labs"},{"location":"generatedContent/kubernetes-operators/#contributors","text":"Rojan Jose, rojanjose Remko de Knikker, remkohdev","title":"Contributors"},{"location":"generatedContent/kubernetes-operators/lab1/","text":"Create a Custom Resource \u00b6 Create a Custom Resource Operators Ready Made Operators Create a Custom Resource and Operator using the Operator SDK Install sdk-operator Create the Operator Cleanup Application CRD Create a Custom Resource (CR) \u00b6 https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/ Custom Resource Definitions (CRD) were added in Kubernetes v1.7 in June 2017. A CRD defines Custom Resources (CR). A CR is an extension of the Kubernetes API that allows you to store your own API Objects and lets the API Server handle the lifecycle of a CR. On their own, CRs simply let you store and retrieve structured data. For instance, our Guestbook application consists of an object Guestbook with attributes GuestbookTitle and GuestbookSubtitle , and a Guestbook handles objectes of type GuestbookMessage with attributes Message , Sender . You have to ask yourself if it makes sense if your objects are added as a Custom Resource to Kubernetes or not. If your API is a Declarative API you can consider adding a CR. Your API has a small number of small objects (resources). The objects define configuration of applications or infrastructure. The objects are updated relatively infrequently. Users often need to read and write the objects. main operations on the objects are CRUD (create, read, update and delete). Transactions between objects are not required. It doesn't immediately make sense to store messages by Guestbook users in Kubernetes, but it might make sense to store meta-data about a Guestbook deployment, for instance the title and subtitle of a Guestbook deployment, assigned resources or replicas. Another benefit of adding a Custom Resource is to view your types in the Kubernetes Dashboard. If you want to deploy a Guestbook instance as a Kubernetes API object and let the Kubernetes API Server handle the lifecycle events of the Guestbook deployment, you can create a Custom Resource Definition (CRD) for the Guestbook object as follows. That way you can deploy multiple Guestbooks with different titles and let each be managed by Kubernetes. cat <<EOF >>guestbook-crd.yaml apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: guestbooks.apps.ibm.com spec: group: apps.ibm.com versions: - name: v1 served: true storage: true schema: openAPIV3Schema: type: object properties: spec: type: object properties: guestbookTitle: type: string guestbookSubtitle: type: string scope: Namespaced names: plural: guestbooks singular: guestbook kind: Guestbook shortNames: - gb EOF You can see that the apiVersion is part of the apiextensions.k8s.io/v1 API Group in Kubernetes, which is the API that enables extensions, and the kind is set to CustomResourceDefinition . The served flag can disable and enable a version. Only 1 version can be flagged as the storage version. The spec.names.kind is used by your resource manifests and should be CamelCased. Create the Custom Resource for the Guestbook witht he command, oc create -f guestbook-crd.yaml When run in the terminal, $ oc create -f guestbook-crd.yaml customresourcedefinition.apiextensions.k8s.io/guestbooks.apps.ibm.com created You have now added a CR to the Kubernetes API, but you have not yet created a deployment of type Guestbook yet. Create a resource specification of type Guestbook named my-guestbook , cat <<EOF >>my-guestbook.yaml apiVersion: \"apps.ibm.com/v1\" kind: Guestbook metadata: name: my-guestbook spec: guestbookTitle: \"The Chemical Wedding of Remko\" guestbookSubtitle: \"First Day of Many\" EOF And to create the my-guestbook resource, run the command oc create -f my-guestbook.yaml When run in the terminal, $ oc create -f my-guestbook.yaml guestbook.apps.ibm.com/my-guestbook created If you list all Kubernetes resources, only the default Kubernetes service is listed. To list your Custom Resources, add the extended type to your command. $ oc get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 172 .21.0.1 <none> 443 /TCP 5d14h service/openshift ExternalName <none> kubernetes.default.svc.cluster.local <none> 5d14h service/openshift-apiserver ClusterIP 172 .21.6.8 <none> = 443 /TCP 5d14h $ oc get guestbook NAME AGE my-guestbook 8m32s To read the details for the my-guestbook of type Guestbook , describe the instance, $ oc describe guestbook my-guestbook Name: my-guestbook Namespace: default Labels: <none> Annotations: <none> API Version: apps.ibm.com/v1 Kind: Guestbook Metadata: Creation Timestamp: 2020 -06-30T20:31:36Z Generation: 1 Resource Version: 1081471 Self Link: /apis/apps.ibm.com/v1/namespaces/default/guestbooks/my-guestbook UID: dcbdcafc-999d-4051-9244-0315093357e7 Spec: Guestbook Subtitle: First Day of Many Guestbook Title: The Chemical Wedding of Remko Events: <none> Or retrieve the resource information by specifying the type, $ oc get Guestbook -o yaml apiVersion: v1 items: - apiVersion: apps.ibm.com/v1 kind: Guestbook metadata: creationTimestamp: \"2020-07-02T04:41:57Z\" generation: 1 name: my-guestbook namespace: default resourceVersion: \"1903244\" selfLink: /apis/apps.ibm.com/v1/namespaces/default/guestbooks/my-guestbook uid: 3f774899-3070-4e00-b74c-a6a14654faeb spec: guestbookSubtitle: First Day of Many guestbookTitle: The Chemical Wedding of Remko kind: List metadata: resourceVersion: \"\" selfLink: \"\" In the OpenShift web console, you can browse to Administration > Custom Resource Definitions and find the Guestbook CRD at /k8s/cluster/customresourcedefinitions/guestbooks.apps.ibm.com . You have now created a new type or Custom Resource (CR) and created an instance of your new type. But just having a new type and a new instance of the type, does not add as much control over the instances yet, we can basically only create and delete a static type with some descriptive meta-data. With a custom controller or Operator you can over-write the methods that are triggered at certain lifecycle events.","title":"Lab 1. Custom Resource (CR)"},{"location":"generatedContent/kubernetes-operators/lab1/#create-a-custom-resource","text":"Create a Custom Resource Operators Ready Made Operators Create a Custom Resource and Operator using the Operator SDK Install sdk-operator Create the Operator Cleanup Application CRD","title":"Create a Custom Resource"},{"location":"generatedContent/kubernetes-operators/lab1/#create-a-custom-resource-cr","text":"https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/ Custom Resource Definitions (CRD) were added in Kubernetes v1.7 in June 2017. A CRD defines Custom Resources (CR). A CR is an extension of the Kubernetes API that allows you to store your own API Objects and lets the API Server handle the lifecycle of a CR. On their own, CRs simply let you store and retrieve structured data. For instance, our Guestbook application consists of an object Guestbook with attributes GuestbookTitle and GuestbookSubtitle , and a Guestbook handles objectes of type GuestbookMessage with attributes Message , Sender . You have to ask yourself if it makes sense if your objects are added as a Custom Resource to Kubernetes or not. If your API is a Declarative API you can consider adding a CR. Your API has a small number of small objects (resources). The objects define configuration of applications or infrastructure. The objects are updated relatively infrequently. Users often need to read and write the objects. main operations on the objects are CRUD (create, read, update and delete). Transactions between objects are not required. It doesn't immediately make sense to store messages by Guestbook users in Kubernetes, but it might make sense to store meta-data about a Guestbook deployment, for instance the title and subtitle of a Guestbook deployment, assigned resources or replicas. Another benefit of adding a Custom Resource is to view your types in the Kubernetes Dashboard. If you want to deploy a Guestbook instance as a Kubernetes API object and let the Kubernetes API Server handle the lifecycle events of the Guestbook deployment, you can create a Custom Resource Definition (CRD) for the Guestbook object as follows. That way you can deploy multiple Guestbooks with different titles and let each be managed by Kubernetes. cat <<EOF >>guestbook-crd.yaml apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: guestbooks.apps.ibm.com spec: group: apps.ibm.com versions: - name: v1 served: true storage: true schema: openAPIV3Schema: type: object properties: spec: type: object properties: guestbookTitle: type: string guestbookSubtitle: type: string scope: Namespaced names: plural: guestbooks singular: guestbook kind: Guestbook shortNames: - gb EOF You can see that the apiVersion is part of the apiextensions.k8s.io/v1 API Group in Kubernetes, which is the API that enables extensions, and the kind is set to CustomResourceDefinition . The served flag can disable and enable a version. Only 1 version can be flagged as the storage version. The spec.names.kind is used by your resource manifests and should be CamelCased. Create the Custom Resource for the Guestbook witht he command, oc create -f guestbook-crd.yaml When run in the terminal, $ oc create -f guestbook-crd.yaml customresourcedefinition.apiextensions.k8s.io/guestbooks.apps.ibm.com created You have now added a CR to the Kubernetes API, but you have not yet created a deployment of type Guestbook yet. Create a resource specification of type Guestbook named my-guestbook , cat <<EOF >>my-guestbook.yaml apiVersion: \"apps.ibm.com/v1\" kind: Guestbook metadata: name: my-guestbook spec: guestbookTitle: \"The Chemical Wedding of Remko\" guestbookSubtitle: \"First Day of Many\" EOF And to create the my-guestbook resource, run the command oc create -f my-guestbook.yaml When run in the terminal, $ oc create -f my-guestbook.yaml guestbook.apps.ibm.com/my-guestbook created If you list all Kubernetes resources, only the default Kubernetes service is listed. To list your Custom Resources, add the extended type to your command. $ oc get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 172 .21.0.1 <none> 443 /TCP 5d14h service/openshift ExternalName <none> kubernetes.default.svc.cluster.local <none> 5d14h service/openshift-apiserver ClusterIP 172 .21.6.8 <none> = 443 /TCP 5d14h $ oc get guestbook NAME AGE my-guestbook 8m32s To read the details for the my-guestbook of type Guestbook , describe the instance, $ oc describe guestbook my-guestbook Name: my-guestbook Namespace: default Labels: <none> Annotations: <none> API Version: apps.ibm.com/v1 Kind: Guestbook Metadata: Creation Timestamp: 2020 -06-30T20:31:36Z Generation: 1 Resource Version: 1081471 Self Link: /apis/apps.ibm.com/v1/namespaces/default/guestbooks/my-guestbook UID: dcbdcafc-999d-4051-9244-0315093357e7 Spec: Guestbook Subtitle: First Day of Many Guestbook Title: The Chemical Wedding of Remko Events: <none> Or retrieve the resource information by specifying the type, $ oc get Guestbook -o yaml apiVersion: v1 items: - apiVersion: apps.ibm.com/v1 kind: Guestbook metadata: creationTimestamp: \"2020-07-02T04:41:57Z\" generation: 1 name: my-guestbook namespace: default resourceVersion: \"1903244\" selfLink: /apis/apps.ibm.com/v1/namespaces/default/guestbooks/my-guestbook uid: 3f774899-3070-4e00-b74c-a6a14654faeb spec: guestbookSubtitle: First Day of Many guestbookTitle: The Chemical Wedding of Remko kind: List metadata: resourceVersion: \"\" selfLink: \"\" In the OpenShift web console, you can browse to Administration > Custom Resource Definitions and find the Guestbook CRD at /k8s/cluster/customresourcedefinitions/guestbooks.apps.ibm.com . You have now created a new type or Custom Resource (CR) and created an instance of your new type. But just having a new type and a new instance of the type, does not add as much control over the instances yet, we can basically only create and delete a static type with some descriptive meta-data. With a custom controller or Operator you can over-write the methods that are triggered at certain lifecycle events.","title":"Create a Custom Resource (CR)"},{"location":"generatedContent/kubernetes-operators/lab2/","text":"Create an Operator of Type Go using the Operator SDK \u00b6 About Operators \u00b6 See https://kubernetes.io/docs/concepts/extend-kubernetes/operator/ . Operators are clients of the Kubernetes API that act as controllers for a Custom Resource. Operators are extensions to Kubernetes that use custom resources to manage applications and their components. They follow the Kubernetes principle of the control loop. About the Operator Framework \u00b6 The Operator Framework is an open source toolkit to manage Operators. The Operator SDK provides the following workflow to develop a new Operator: The following workflow is for a new Go operator: Create a new operator project using the SDK Command Line Interface(CLI) Define new resource APIs by adding Custom Resource Definitions(CRD) Define Controllers to watch and reconcile resources Write the reconciling logic for your Controller using the SDK and controller-runtime APIs Use the SDK CLI to build and generate the operator deployment manifests Install sdk-operator \u00b6 For detailed installation instructions go here . To install the Operator SDK in Ubuntu, you need to install the Go tools and the Operator SDK. curl -LO https://golang.org/dl/go1.14.4.linux-amd64.tar.gz tar -C /usr/local -xzf go1.14.4.linux-amd64.tar.gz export PATH = $PATH :/usr/local/go/bin curl -LO https://github.com/operator-framework/operator-sdk/releases/download/v0.18.2/operator-sdk-v0.18.2-x86_64-linux-gnu chmod +x operator-sdk-v0.18.2-x86_64-linux-gnu sudo mkdir -p /usr/local/bin/ sudo cp operator-sdk-v0.18.2-x86_64-linux-gnu /usr/local/bin/operator-sdk rm operator-sdk-v0.18.2-x86_64-linux-gnu go version operator-sdk version 1. Create a New Project \u00b6 Create a new Operator project, export DOCKER_USERNAME = <your-docker-username> export OPERATOR_NAME = guestbook-operator export OPERATOR_PROJECT = guestbook-project export OPERATOR_GROUP = guestbook.remkoh.dev export OPERATOR_VERSION = v1 export CRD_KIND = Guestbook go version operator-sdk version operator-sdk new $OPERATOR_PROJECT --type go --repo github.com/ $DOCKER_USERNAME / $OPERATOR_NAME cd $OPERATOR_PROJECT The scaffolding of a new project will create an operator, an api and a controller. 2. Create a new API \u00b6 Add a new API definition for a new Custom Resource under pkg/apis and generate the Custom Resource Definition (CRD) and Custom Resource (CR) files under deploy/crds . operator-sdk add api --api-version = $OPERATOR_GROUP / $OPERATOR_VERSION --kind = $CRD_KIND The command will create a new API, a Custom Resource (CR), a Custom Resource Definition (CRD). One file is created in pkg/apis called addtoscheme_guestbook_v1.go that registers the new schema. One new file is created in pkg/apis/guestbook called group.go that defines the package. Four new files are created in pkg/apis/guestbook/v1 : doc.go, guestbook_types.go, register.go, zz_generated.deepcopy.go. The guestbook_types.go file, package v1 import ( metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" ) // GuestbookSpec defines the desired state of Guestbook type GuestbookSpec struct { } // GuestbookStatus defines the observed state of Guestbook type GuestbookStatus struct { } type Guestbook struct { metav1 . TypeMeta `json:\",inline\"` metav1 . ObjectMeta `json:\"metadata,omitempty\"` Spec GuestbookSpec `json:\"spec,omitempty\"` Status GuestbookStatus `json:\"status,omitempty\"` } // GuestbookList contains a list of Guestbook type GuestbookList struct { metav1 . TypeMeta `json:\",inline\"` metav1 . ListMeta `json:\"metadata,omitempty\"` Items [] Guestbook `json:\"items\"` } func init () { SchemeBuilder . Register ( & Guestbook {}, & GuestbookList {}) } The Custom Resource (CR) in file deploy/crds/guestbook.remkoh.dev_v1_guestbook_cr , apiVersion : guestbook.remkoh.dev/v1 kind : Guestbook metadata : name : example-guestbook spec : # Add fields here size : 3 The Custom Resource Definition (CRD) in file deploy/crds/guestbook.remkoh.dev_guestbooks_crd.yaml , apiVersion : apiextensions.k8s.io/v1 kind : CustomResourceDefinition metadata : name : guestbooks.guestbook.remkoh.dev spec : group : guestbook.remkoh.dev names : kind : Guestbook listKind : GuestbookList plural : guestbooks singular : guestbook scope : Namespaced versions : - name : v1 schema : openAPIV3Schema : description : Guestbook is the Schema for the guestbooks API properties : apiVersion : description : 'APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources' type : string kind : description : 'Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds' type : string metadata : type : object spec : description : GuestbookSpec defines the desired state of Guestbook type : object status : description : GuestbookStatus defines the observed state of Guestbook type : object type : object served : true storage : true subresources : status : {} 3. Create a new Controller \u00b6 Add a new controller under pkg/controller/<kind> . operator-sdk add controller --api-version = $OPERATOR_GROUP / $OPERATOR_VERSION --kind = $CRD_KIND This command creates two files in pkg/controller : add_guestbook.go , which registers the new controller, and guestbook/guestbook_controller.go , which is the actual custom controller logic. The file guestbook/guestbook_controller.go defines the Reconcile function, // Reconcile reads state of the cluster for a Guestbook object and makes changes based on the state read and what is in the Guestbook.Spec // TODO(user): User must modify this Reconcile function to implement their own Controller logic. This example creates a Pod as an example func ( r * ReconcileGuestbook ) Reconcile ( request reconcile . Request ) ( reconcile . Result , error ) { ... // Fetch the Guestbook instance instance := & guestbookv1 . Guestbook {} ... // Define a new Pod object pod := newPodForCR ( instance ) ... } 4. Compile and Build the Code \u00b6 The operator-sdk build command compiles the code and builds the executables. fter you built the image, push it to your image registry, e.g. Docker hub. operator-sdk build docker.io/ $DOCKER_USERNAME / $OPERATOR_NAME docker login docker.io -u $DOCKER_USERNAME docker push docker.io/ $DOCKER_USERNAME / $OPERATOR_NAME 5. Deploy the Operator \u00b6 First replace the image attribute in the operator resource with the built image, sed -i \"s|REPLACE_IMAGE|docker.io/ $DOCKER_USERNAME / $OPERATOR_NAME |g\" deploy/operator.yaml Make sure you are connected to the OpenShift cluster (see above how to connect), and deploy the operator with the following template code. oc create sa $OPERATOR_PROJECT oc create -f deploy/role.yaml oc create -f deploy/role_binding.yaml oc create -f deploy/crds/ ${ OPERATOR_GROUP } _ ${ CRD_KIND ,, } s_crd.yaml oc create -f deploy/operator.yaml oc create -f deploy/crds/ ${ OPERATOR_GROUP } _ ${ OPERATOR_VERSION } _ ${ CRD_KIND ,, } _cr.yaml oc get deployment $OPERATOR_PROJECT oc get pod -l app = example- ${ CRD_KIND ,, } oc describe ${ CRD_KIND ,, } s. ${ OPERATOR_GROUP } example- ${ CRD_KIND ,, } For our example Guestbook project the above templates should resolve as follows, oc create sa guestbook-project oc create -f deploy/role.yaml oc create -f deploy/role_binding.yaml oc create -f deploy/crds/guestbook.remkoh.dev_guestbooks_crd.yaml oc create -f deploy/operator.yaml oc create -f deploy/crds/guestbook.remkoh.dev_v1_guestbook_cr.yaml oc get deployment guestbook-project oc get pod -l app = example-guestbook oc describe guestbooks.guestbook.remkoh.dev example-guestbook Cleanup \u00b6 oc delete sa $OPERATOR_PROJECT oc delete role $OPERATOR_PROJECT oc delete rolebinding $OPERATOR_PROJECT oc delete customresourcedefinition ${ CRD_KIND ,, } s. ${ OPERATOR_GROUP } oc delete deployment $OPERATOR_PROJECT","title":"Lab 2. Go Operator with Operator SDK"},{"location":"generatedContent/kubernetes-operators/lab2/#create-an-operator-of-type-go-using-the-operator-sdk","text":"","title":"Create an Operator of Type Go using the Operator SDK"},{"location":"generatedContent/kubernetes-operators/lab2/#about-operators","text":"See https://kubernetes.io/docs/concepts/extend-kubernetes/operator/ . Operators are clients of the Kubernetes API that act as controllers for a Custom Resource. Operators are extensions to Kubernetes that use custom resources to manage applications and their components. They follow the Kubernetes principle of the control loop.","title":"About Operators"},{"location":"generatedContent/kubernetes-operators/lab2/#about-the-operator-framework","text":"The Operator Framework is an open source toolkit to manage Operators. The Operator SDK provides the following workflow to develop a new Operator: The following workflow is for a new Go operator: Create a new operator project using the SDK Command Line Interface(CLI) Define new resource APIs by adding Custom Resource Definitions(CRD) Define Controllers to watch and reconcile resources Write the reconciling logic for your Controller using the SDK and controller-runtime APIs Use the SDK CLI to build and generate the operator deployment manifests","title":"About the Operator Framework"},{"location":"generatedContent/kubernetes-operators/lab2/#install-sdk-operator","text":"For detailed installation instructions go here . To install the Operator SDK in Ubuntu, you need to install the Go tools and the Operator SDK. curl -LO https://golang.org/dl/go1.14.4.linux-amd64.tar.gz tar -C /usr/local -xzf go1.14.4.linux-amd64.tar.gz export PATH = $PATH :/usr/local/go/bin curl -LO https://github.com/operator-framework/operator-sdk/releases/download/v0.18.2/operator-sdk-v0.18.2-x86_64-linux-gnu chmod +x operator-sdk-v0.18.2-x86_64-linux-gnu sudo mkdir -p /usr/local/bin/ sudo cp operator-sdk-v0.18.2-x86_64-linux-gnu /usr/local/bin/operator-sdk rm operator-sdk-v0.18.2-x86_64-linux-gnu go version operator-sdk version","title":"Install sdk-operator"},{"location":"generatedContent/kubernetes-operators/lab2/#1-create-a-new-project","text":"Create a new Operator project, export DOCKER_USERNAME = <your-docker-username> export OPERATOR_NAME = guestbook-operator export OPERATOR_PROJECT = guestbook-project export OPERATOR_GROUP = guestbook.remkoh.dev export OPERATOR_VERSION = v1 export CRD_KIND = Guestbook go version operator-sdk version operator-sdk new $OPERATOR_PROJECT --type go --repo github.com/ $DOCKER_USERNAME / $OPERATOR_NAME cd $OPERATOR_PROJECT The scaffolding of a new project will create an operator, an api and a controller.","title":"1. Create a New Project"},{"location":"generatedContent/kubernetes-operators/lab2/#2-create-a-new-api","text":"Add a new API definition for a new Custom Resource under pkg/apis and generate the Custom Resource Definition (CRD) and Custom Resource (CR) files under deploy/crds . operator-sdk add api --api-version = $OPERATOR_GROUP / $OPERATOR_VERSION --kind = $CRD_KIND The command will create a new API, a Custom Resource (CR), a Custom Resource Definition (CRD). One file is created in pkg/apis called addtoscheme_guestbook_v1.go that registers the new schema. One new file is created in pkg/apis/guestbook called group.go that defines the package. Four new files are created in pkg/apis/guestbook/v1 : doc.go, guestbook_types.go, register.go, zz_generated.deepcopy.go. The guestbook_types.go file, package v1 import ( metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" ) // GuestbookSpec defines the desired state of Guestbook type GuestbookSpec struct { } // GuestbookStatus defines the observed state of Guestbook type GuestbookStatus struct { } type Guestbook struct { metav1 . TypeMeta `json:\",inline\"` metav1 . ObjectMeta `json:\"metadata,omitempty\"` Spec GuestbookSpec `json:\"spec,omitempty\"` Status GuestbookStatus `json:\"status,omitempty\"` } // GuestbookList contains a list of Guestbook type GuestbookList struct { metav1 . TypeMeta `json:\",inline\"` metav1 . ListMeta `json:\"metadata,omitempty\"` Items [] Guestbook `json:\"items\"` } func init () { SchemeBuilder . Register ( & Guestbook {}, & GuestbookList {}) } The Custom Resource (CR) in file deploy/crds/guestbook.remkoh.dev_v1_guestbook_cr , apiVersion : guestbook.remkoh.dev/v1 kind : Guestbook metadata : name : example-guestbook spec : # Add fields here size : 3 The Custom Resource Definition (CRD) in file deploy/crds/guestbook.remkoh.dev_guestbooks_crd.yaml , apiVersion : apiextensions.k8s.io/v1 kind : CustomResourceDefinition metadata : name : guestbooks.guestbook.remkoh.dev spec : group : guestbook.remkoh.dev names : kind : Guestbook listKind : GuestbookList plural : guestbooks singular : guestbook scope : Namespaced versions : - name : v1 schema : openAPIV3Schema : description : Guestbook is the Schema for the guestbooks API properties : apiVersion : description : 'APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources' type : string kind : description : 'Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds' type : string metadata : type : object spec : description : GuestbookSpec defines the desired state of Guestbook type : object status : description : GuestbookStatus defines the observed state of Guestbook type : object type : object served : true storage : true subresources : status : {}","title":"2. Create a new API"},{"location":"generatedContent/kubernetes-operators/lab2/#3-create-a-new-controller","text":"Add a new controller under pkg/controller/<kind> . operator-sdk add controller --api-version = $OPERATOR_GROUP / $OPERATOR_VERSION --kind = $CRD_KIND This command creates two files in pkg/controller : add_guestbook.go , which registers the new controller, and guestbook/guestbook_controller.go , which is the actual custom controller logic. The file guestbook/guestbook_controller.go defines the Reconcile function, // Reconcile reads state of the cluster for a Guestbook object and makes changes based on the state read and what is in the Guestbook.Spec // TODO(user): User must modify this Reconcile function to implement their own Controller logic. This example creates a Pod as an example func ( r * ReconcileGuestbook ) Reconcile ( request reconcile . Request ) ( reconcile . Result , error ) { ... // Fetch the Guestbook instance instance := & guestbookv1 . Guestbook {} ... // Define a new Pod object pod := newPodForCR ( instance ) ... }","title":"3. Create a new Controller"},{"location":"generatedContent/kubernetes-operators/lab2/#4-compile-and-build-the-code","text":"The operator-sdk build command compiles the code and builds the executables. fter you built the image, push it to your image registry, e.g. Docker hub. operator-sdk build docker.io/ $DOCKER_USERNAME / $OPERATOR_NAME docker login docker.io -u $DOCKER_USERNAME docker push docker.io/ $DOCKER_USERNAME / $OPERATOR_NAME","title":"4. Compile and Build the Code"},{"location":"generatedContent/kubernetes-operators/lab2/#5-deploy-the-operator","text":"First replace the image attribute in the operator resource with the built image, sed -i \"s|REPLACE_IMAGE|docker.io/ $DOCKER_USERNAME / $OPERATOR_NAME |g\" deploy/operator.yaml Make sure you are connected to the OpenShift cluster (see above how to connect), and deploy the operator with the following template code. oc create sa $OPERATOR_PROJECT oc create -f deploy/role.yaml oc create -f deploy/role_binding.yaml oc create -f deploy/crds/ ${ OPERATOR_GROUP } _ ${ CRD_KIND ,, } s_crd.yaml oc create -f deploy/operator.yaml oc create -f deploy/crds/ ${ OPERATOR_GROUP } _ ${ OPERATOR_VERSION } _ ${ CRD_KIND ,, } _cr.yaml oc get deployment $OPERATOR_PROJECT oc get pod -l app = example- ${ CRD_KIND ,, } oc describe ${ CRD_KIND ,, } s. ${ OPERATOR_GROUP } example- ${ CRD_KIND ,, } For our example Guestbook project the above templates should resolve as follows, oc create sa guestbook-project oc create -f deploy/role.yaml oc create -f deploy/role_binding.yaml oc create -f deploy/crds/guestbook.remkoh.dev_guestbooks_crd.yaml oc create -f deploy/operator.yaml oc create -f deploy/crds/guestbook.remkoh.dev_v1_guestbook_cr.yaml oc get deployment guestbook-project oc get pod -l app = example-guestbook oc describe guestbooks.guestbook.remkoh.dev example-guestbook","title":"5. Deploy the Operator"},{"location":"generatedContent/kubernetes-operators/lab2/#cleanup","text":"oc delete sa $OPERATOR_PROJECT oc delete role $OPERATOR_PROJECT oc delete rolebinding $OPERATOR_PROJECT oc delete customresourcedefinition ${ CRD_KIND ,, } s. ${ OPERATOR_GROUP } oc delete deployment $OPERATOR_PROJECT","title":"Cleanup"},{"location":"generatedContent/kubernetes-operators/lab3-v0.19/","text":"Create an Operator using an Existing Helm Chart \u00b6 The Operator Framework is an open source project that provides developer and runtime Kubernetes tools, enabling you to accelerate the development of an Operator. The Operator SDK provides the tools to build, test and package Operators. The following workflow is for a Helm operator using existing chart : Create a new operator project using the SDK Command Line Interface(CLI) Create a new (or add your existing) Helm chart for use by the operator\u2019s reconciling logic Use the SDK CLI to build and generate the operator deployment manifests Optionally add additional CRD\u2019s using the SDK CLI and repeat steps 2 and 3 Use the SDK bundle feature to package the operator for OLM deployment. Deploy, test and publish. In this lab, we will use the IBM Guestbook helm chart available here as the base to scaffold a new operator. Information of creating a new operator can be found here Setup \u00b6 The lab requires you to have the operator-sdk installed. Login into the client CLI following these instructions . Run the command shown below to install the prerequisites: source < ( curl -s https://raw.githubusercontent.com/rojanjose/guestbook-helm-operator/master/scripts/operatorInstall.sh ) Check the command output to ensure the SDK version is correct. ... Checking prereqs version ... Go version: go version go1.14.4 linux/amd64 ----------------------------- Helm version: version.BuildInfo { Version: \"v3.0.3\" , GitCommit: \"ac925eb7279f4a6955df663a0128044a8a6b7593\" , GitTreeState: \"clean\" , GoVersion: \"go1.13.6\" } ----------------------------- Operator-sdk version: operator-sdk version: \"v0.19.2\" , commit: \"4282ce9acdef6d7a1e9f90832db4dc5a212ae850\" , kubernetes version: \"v1.18.2\" , go version: \"go1.13.10 linux/amd64\" Log into your OpenShift cluster . oc login --token = YQ2-mTJIWlz1gsWeI2tsO4CzHBbRSCQbH-IdA3tEFrM --server = https://c100-e.us-east.containers.cloud.ibm.com:32055 Create the operator \u00b6 1. Create a new project \u00b6 Export these environment variables prior to starting the project. export DOCKER_USERNAME = <your-docker-username> export OPERATOR_NAME = guestbook-operator export OPERATOR_PROJECT = guestbook-operator-project export OPERATOR_VERSION = v1.0.0 export IMG = docker.io/ ${ DOCKER_USERNAME } / ${ OPERATOR_NAME } : ${ OPERATOR_VERSION } Create a new project called guestbook-operator using the existing guestbook helm chart. The guestbook chart is available at the repo https://ibm.github.io/helm101/ . operator-sdk new $OPERATOR_PROJECT --type = helm --helm-chart = guestbook --helm-chart-repo = https://ibm.github.io/helm101/ cd $OPERATOR_PROJECT Output: INFO [ 0000 ] Creating new Helm operator 'guestbook-operator-project' . INFO [ 0000 ] Created helm-charts/guestbook INFO [ 0000 ] Generating RBAC rules I0813 23 :07:00.995286 11211 request.go:621 ] Throttling request took 1 .031369076s, request: GET:https://c100-e.us-east.containers.cloud.ibm.com:31941/apis/scheduling.k8s.io/v1?timeout = 32s WARN [ 0002 ] The RBAC rules generated in deploy/role.yaml are based on the chart 's default manifest. Some rules may be missing for resources that are only enabled with custom values, and some existing rules may be overly broad. Double check the rules generated in deploy/role.yaml to ensure they meet the operator' s permission requirements. INFO [ 0002 ] Created build/Dockerfile INFO [ 0002 ] Created deploy/service_account.yaml INFO [ 0002 ] Created deploy/role.yaml INFO [ 0002 ] Created deploy/role_binding.yaml INFO [ 0002 ] Created deploy/operator.yaml INFO [ 0002 ] Created deploy/crds/helm.operator-sdk_v1alpha1_guestbook_cr.yaml INFO [ 0002 ] Generated CustomResourceDefinition manifests. INFO [ 0002 ] Project creation complete. Review the code and customize the operator logic as required to obtain the desired results. By default, the Guestbook operator installs the configured helm chart watches the events shown in the watches.yaml . - group : helm.operator-sdk version : v1alpha1 kind : Guestbook chart : helm-charts/guestbook The custom resource (CR) file defines the properties used by operator while it creates an instance of the Guestbook application. These properties are derived from the values.yaml file in the Helm chart. apiVersion : helm.operator-sdk/v1alpha1 kind : Guestbook metadata : name : example-guestbook spec : # Default values copied from <project_dir>/helm-charts/guestbook/values.yaml image : pullPolicy : Always repository : ibmcom/guestbook tag : v1 redis : port : 6379 slaveEnabled : true replicaCount : 2 service : port : 3000 type : LoadBalancer 2. Deploy the CRD \u00b6 Let Kubernetes know about the new custom resource definition (CRD) the operator will be watching. oc create -f deploy/crds/helm.operator-sdk_guestbooks_crd.yaml Verify the CRD install in OpenShift console: Alternatively, query using the following CLI commands: oc get crd guestbooks.helm.operator-sdk oc describe crd guestbooks.helm.operator-sdk 3. Build the code \u00b6 Use the generated Dockerfile under build directory for image build. FROM quay.io/operator-framework/helm-operator:v0.19.2 COPY watches.yaml ${ HOME } /watches.yaml COPY helm-charts/ ${ HOME } /helm-charts/ Run the operator sdk build command to build the image for the helm operator. operator-sdk build ${ IMG } INFO [ 0000 ] Building OCI image docker.io/rojanjose/guestbook-operator:v1.0.0 Sending build context to Docker daemon 41 .98kB Step 1 /3 : FROM quay.io/operator-framework/helm-operator:v0.19.2 v0.19.2: Pulling from operator-framework/helm-operator 41ae95b593e0: Pull complete f20f68829d13: Pull complete 05c2e7d4212e: Pull complete 66213365a0c9: Pull complete 09e5a7e28c6f: Pull complete Digest: sha256:0f1e104719267f687280d8640a6958c61510fae27a6937369c419b0dd2b91564 .... Verify the built image: $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE rojanjose/guestbook-operator v1.0.0 d05f5e2c441e 7 seconds ago 200MB quay.io/operator-framework/helm-operator v0.19.2 11862329f28c 2 weeks ago 200MB Log into the docker registry and push image: docker login docker.io -u $DOCKER_USERNAME docker push ${ IMG } Replace the image name string in the operator.yaml file: sed -i 's|REPLACE_IMAGE|' ${ IMG } '|g' deploy/operator.yaml ( MacOS: ) sed -i \"\" 's|REPLACE_IMAGE|' ${ IMG } '|g' deploy/operator.yaml At this stage, the operator can be deployed with the available manifest files, however, we will explore the operator deloy with OLM features. 4. Deploy the Operator with the Operator Lifecycle Manager (OLM) \u00b6 Ensure OLM is enabled on the cluster by running this command: operator-sdk olm status --olm-namespace openshift-operator-lifecycle-manager Expected result: operator-sdk olm status --olm-namespace openshift-operator-lifecycle-manager I0813 23 :36:41.881438 14844 request.go:621 ] Throttling request took 1 .020925705s, request: GET:https://c100-e.us-east.containers.cloud.ibm.com:31941/apis/rbac.authorization.k8s.io/v1beta1?timeout = 32s INFO [ 0002 ] Fetching CRDs for version \"0.13.0\" INFO [ 0002 ] Fetching resources for version \"0.13.0\" INFO [ 0003 ] Successfully got OLM status for version \"0.13.0\" NAME NAMESPACE KIND STATUS installplans.operators.coreos.com CustomResourceDefinition Installed clusterserviceversions.operators.coreos.com CustomResourceDefinition Installed aggregate-olm-view ClusterRole Installed operatorgroups.operators.coreos.com CustomResourceDefinition Installed catalogsources.operators.coreos.com CustomResourceDefinition Installed subscriptions.operators.coreos.com CustomResourceDefinition Installed system:controller:operator-lifecycle-manager ClusterRole Installed aggregate-olm-edit ClusterRole Installed olm-operator-binding-olm ClusterRoleBinding clusterrolebindings.rbac.authorization.k8s.io \"olm-operator-binding-olm\" not found olm-operator-serviceaccount olm ServiceAccount serviceaccounts \"olm-operator-serviceaccount\" not found olm-operator olm Deployment deployments.apps \"olm-operator\" not found catalog-operator olm Deployment deployments.apps \"catalog-operator\" not found operators Namespace namespaces \"operators\" not found olm Namespace namespaces \"olm\" not found global-operators operators OperatorGroup operatorgroups.operators.coreos.com \"global-operators\" not found olm-operators olm OperatorGroup operatorgroups.operators.coreos.com \"olm-operators\" not found packageserver olm ClusterServiceVersion clusterserviceversions.operators.coreos.com \"packageserver\" not found operatorhubio-catalog olm CatalogSource catalogsources.operators.coreos.com \"operatorhubio-catalog\" not found [Note: OLM is partially enabled which is sufficient to complete this lab.] Create a bundle: operator-sdk generate bundle --version 1 .0.0 Output of the command: INFO [ 0000 ] Generating bundle manifests version 1 .0.0 Display name for the operator ( required ) : > Guestbook Operator Description for the operator ( required ) : > Demo helm operator for Guestbook Provider 's name for the operator (required): > IBM Any relevant URL for the provider name (optional): > https://github.com/rojanjose/guestbook-helm-operator Comma-separated list of keywords for your operator (required): > helm,operator,kubernetes,openshift Comma-separated list of maintainers and their emails (e.g. ' name1:email1, name2:email2 ' ) ( required ) : > Rojan:rojanjose@gmail.com INFO [ 0164 ] Bundle manifests generated successfully in deploy/olm-catalog/guestbook-operator-project INFO [ 0164 ] Building annotations.yaml INFO [ 0164 ] Writing annotations.yaml in /Users/operator/guestbook-operator-project/deploy/olm-catalog/guestbook-operator-project/metadata INFO [ 0164 ] Building Dockerfile INFO [ 0164 ] Writing bundle.Dockerfile in /Users/operator/guestbook-operator-project A bundle manifests directory deploy/olm-catalog/guestbook-operator-project/manifests containing a CSV and all CRDs in deploy/crds and a bundle metadata directory deploy/olm-catalog/guestbook-operator-project/metadata are generated. Create Project where operator OLM should be installed: oc new-project guest-operator-ns Output: Now using project \"guest-operator-ns\" on server \"https://c100-e.us-east.containers.cloud.ibm.com:31941\" . You can add applications to this project with the 'new-app' command. For example, try: oc new-app django-psql-example ... Create an OperatorGroup yaml definition: cat <<EOF >>deploy/operator_group.yaml apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: guestbook-og namespace: guest-operator-ns spec: targetNamespaces: - guest-operator-ns EOF Replace placeholder string with project guest-operator-ns in guestbook-operator.clusterserviceversion.yaml sed -i 's#namespace: placeholder#namespace: guest-operator-ns#' deploy/olm-catalog/guestbook-operator-project/manifests/guestbook-operator-project.clusterserviceversion.yaml or on Mac, sed -i \"\" 's#namespace: placeholder#namespace: guest-operator-ns#' deploy/olm-catalog/guestbook-operator-project/manifests/guestbook-operator-project.clusterserviceversion.yaml 5. Install the operator \u00b6 Create the Operator group: oc create -f deploy/operator_group.yaml Apply the Operator\u2019s CSV manifest to the specified namespace in the cluster: oc create -f deploy/olm-catalog/guestbook-operator-project/manifests/guestbook-operator-project.clusterserviceversion.yaml Create the role, role binding, and service account to grant resource permissions to the Operator to create the Guestbook type that the Operator manages: oc create -f deploy/service_account.yaml oc create -f deploy/role.yaml oc create -f deploy/role_binding.yaml Wait for few minutes for the Guestbook operator to complete the installation. oc get ClusterServiceVersion ClusterServiceVersion should show a PHASE value of Succeeded , $ oc get ClusterServiceVersion NAME DISPLAY VERSION REPLACES PHASE guestbook-operator-project.v1.0.0 Guestbook Operator 1 .0.0 Succeeded Check the list of Installed Operators under the project guest-operator-ns . Open the operator and validate that install succeeded. Now, create an instance of Guestbook helm chart. Click on Create instance icon. Goto Workloads > Pods to view the pods. You should see 2 frontend pods, 1 Redis master, 2 Redis slave and pod supporting the Guestbook operator OLM. 6. Update the Guestbook application instance \u00b6 Open the deploy/crds/helm.operator-sdk_v1alpha1_guestbook_cr.yaml and change the value of replicaCount to 4. Save the file and run the oc apply command: oc apply -f deploy/crds/helm.operator-sdk_v1alpha1_guestbook_cr.yaml Run oc get pods to validate the guesbook pod count: oc get podsNAME READY STATUS RESTARTS AGE example-guestbook-6fdb6776b-hdq64 1 /1 Running 0 41m example-guestbook-6fdb6776b-pktr8 1 /1 Running 0 8s example-guestbook-6fdb6776b-x2nqw 1 /1 Running 0 41m example-guestbook-6fdb6776b-xz8lp 1 /1 Running 0 77s guestbook-operator-project-767cc5686c-ksmxq 1 /1 Running 0 52m redis-master-68857cd57c-pwctp 1 /1 Running 0 41m redis-slave-bbd8d8545-6jk8m 1 /1 Running 0 41m redis-slave-bbd8d8545-k65wz 1 /1 Running 0 41m 7. Clean up \u00b6 Run the oc delete commands to remove the operator. oc delete -f deploy/crds/helm.operator-sdk_v1alpha1_guestbook_cr.yaml oc delete -f deploy/service_account.yaml oc delete -f deploy/role.yaml oc delete -f deploy/role_binding.yaml oc delete -f deploy/olm-catalog/guestbook-operator-project/manifests/guestbook-operator-project.clusterserviceversion.yaml oc delete -f deploy/operator_group.yaml oc delete -f deploy/crds/helm.operator-sdk_guestbooks_crd.yaml","title":"Create an Operator using an Existing Helm Chart"},{"location":"generatedContent/kubernetes-operators/lab3-v0.19/#create-an-operator-using-an-existing-helm-chart","text":"The Operator Framework is an open source project that provides developer and runtime Kubernetes tools, enabling you to accelerate the development of an Operator. The Operator SDK provides the tools to build, test and package Operators. The following workflow is for a Helm operator using existing chart : Create a new operator project using the SDK Command Line Interface(CLI) Create a new (or add your existing) Helm chart for use by the operator\u2019s reconciling logic Use the SDK CLI to build and generate the operator deployment manifests Optionally add additional CRD\u2019s using the SDK CLI and repeat steps 2 and 3 Use the SDK bundle feature to package the operator for OLM deployment. Deploy, test and publish. In this lab, we will use the IBM Guestbook helm chart available here as the base to scaffold a new operator. Information of creating a new operator can be found here","title":"Create an Operator using an Existing Helm Chart"},{"location":"generatedContent/kubernetes-operators/lab3-v0.19/#setup","text":"The lab requires you to have the operator-sdk installed. Login into the client CLI following these instructions . Run the command shown below to install the prerequisites: source < ( curl -s https://raw.githubusercontent.com/rojanjose/guestbook-helm-operator/master/scripts/operatorInstall.sh ) Check the command output to ensure the SDK version is correct. ... Checking prereqs version ... Go version: go version go1.14.4 linux/amd64 ----------------------------- Helm version: version.BuildInfo { Version: \"v3.0.3\" , GitCommit: \"ac925eb7279f4a6955df663a0128044a8a6b7593\" , GitTreeState: \"clean\" , GoVersion: \"go1.13.6\" } ----------------------------- Operator-sdk version: operator-sdk version: \"v0.19.2\" , commit: \"4282ce9acdef6d7a1e9f90832db4dc5a212ae850\" , kubernetes version: \"v1.18.2\" , go version: \"go1.13.10 linux/amd64\" Log into your OpenShift cluster . oc login --token = YQ2-mTJIWlz1gsWeI2tsO4CzHBbRSCQbH-IdA3tEFrM --server = https://c100-e.us-east.containers.cloud.ibm.com:32055","title":"Setup"},{"location":"generatedContent/kubernetes-operators/lab3-v0.19/#create-the-operator","text":"","title":"Create the operator"},{"location":"generatedContent/kubernetes-operators/lab3-v0.19/#1-create-a-new-project","text":"Export these environment variables prior to starting the project. export DOCKER_USERNAME = <your-docker-username> export OPERATOR_NAME = guestbook-operator export OPERATOR_PROJECT = guestbook-operator-project export OPERATOR_VERSION = v1.0.0 export IMG = docker.io/ ${ DOCKER_USERNAME } / ${ OPERATOR_NAME } : ${ OPERATOR_VERSION } Create a new project called guestbook-operator using the existing guestbook helm chart. The guestbook chart is available at the repo https://ibm.github.io/helm101/ . operator-sdk new $OPERATOR_PROJECT --type = helm --helm-chart = guestbook --helm-chart-repo = https://ibm.github.io/helm101/ cd $OPERATOR_PROJECT Output: INFO [ 0000 ] Creating new Helm operator 'guestbook-operator-project' . INFO [ 0000 ] Created helm-charts/guestbook INFO [ 0000 ] Generating RBAC rules I0813 23 :07:00.995286 11211 request.go:621 ] Throttling request took 1 .031369076s, request: GET:https://c100-e.us-east.containers.cloud.ibm.com:31941/apis/scheduling.k8s.io/v1?timeout = 32s WARN [ 0002 ] The RBAC rules generated in deploy/role.yaml are based on the chart 's default manifest. Some rules may be missing for resources that are only enabled with custom values, and some existing rules may be overly broad. Double check the rules generated in deploy/role.yaml to ensure they meet the operator' s permission requirements. INFO [ 0002 ] Created build/Dockerfile INFO [ 0002 ] Created deploy/service_account.yaml INFO [ 0002 ] Created deploy/role.yaml INFO [ 0002 ] Created deploy/role_binding.yaml INFO [ 0002 ] Created deploy/operator.yaml INFO [ 0002 ] Created deploy/crds/helm.operator-sdk_v1alpha1_guestbook_cr.yaml INFO [ 0002 ] Generated CustomResourceDefinition manifests. INFO [ 0002 ] Project creation complete. Review the code and customize the operator logic as required to obtain the desired results. By default, the Guestbook operator installs the configured helm chart watches the events shown in the watches.yaml . - group : helm.operator-sdk version : v1alpha1 kind : Guestbook chart : helm-charts/guestbook The custom resource (CR) file defines the properties used by operator while it creates an instance of the Guestbook application. These properties are derived from the values.yaml file in the Helm chart. apiVersion : helm.operator-sdk/v1alpha1 kind : Guestbook metadata : name : example-guestbook spec : # Default values copied from <project_dir>/helm-charts/guestbook/values.yaml image : pullPolicy : Always repository : ibmcom/guestbook tag : v1 redis : port : 6379 slaveEnabled : true replicaCount : 2 service : port : 3000 type : LoadBalancer","title":"1. Create a new project"},{"location":"generatedContent/kubernetes-operators/lab3-v0.19/#2-deploy-the-crd","text":"Let Kubernetes know about the new custom resource definition (CRD) the operator will be watching. oc create -f deploy/crds/helm.operator-sdk_guestbooks_crd.yaml Verify the CRD install in OpenShift console: Alternatively, query using the following CLI commands: oc get crd guestbooks.helm.operator-sdk oc describe crd guestbooks.helm.operator-sdk","title":"2. Deploy the CRD"},{"location":"generatedContent/kubernetes-operators/lab3-v0.19/#3-build-the-code","text":"Use the generated Dockerfile under build directory for image build. FROM quay.io/operator-framework/helm-operator:v0.19.2 COPY watches.yaml ${ HOME } /watches.yaml COPY helm-charts/ ${ HOME } /helm-charts/ Run the operator sdk build command to build the image for the helm operator. operator-sdk build ${ IMG } INFO [ 0000 ] Building OCI image docker.io/rojanjose/guestbook-operator:v1.0.0 Sending build context to Docker daemon 41 .98kB Step 1 /3 : FROM quay.io/operator-framework/helm-operator:v0.19.2 v0.19.2: Pulling from operator-framework/helm-operator 41ae95b593e0: Pull complete f20f68829d13: Pull complete 05c2e7d4212e: Pull complete 66213365a0c9: Pull complete 09e5a7e28c6f: Pull complete Digest: sha256:0f1e104719267f687280d8640a6958c61510fae27a6937369c419b0dd2b91564 .... Verify the built image: $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE rojanjose/guestbook-operator v1.0.0 d05f5e2c441e 7 seconds ago 200MB quay.io/operator-framework/helm-operator v0.19.2 11862329f28c 2 weeks ago 200MB Log into the docker registry and push image: docker login docker.io -u $DOCKER_USERNAME docker push ${ IMG } Replace the image name string in the operator.yaml file: sed -i 's|REPLACE_IMAGE|' ${ IMG } '|g' deploy/operator.yaml ( MacOS: ) sed -i \"\" 's|REPLACE_IMAGE|' ${ IMG } '|g' deploy/operator.yaml At this stage, the operator can be deployed with the available manifest files, however, we will explore the operator deloy with OLM features.","title":"3. Build the code"},{"location":"generatedContent/kubernetes-operators/lab3-v0.19/#4-deploy-the-operator-with-the-operator-lifecycle-manager-olm","text":"Ensure OLM is enabled on the cluster by running this command: operator-sdk olm status --olm-namespace openshift-operator-lifecycle-manager Expected result: operator-sdk olm status --olm-namespace openshift-operator-lifecycle-manager I0813 23 :36:41.881438 14844 request.go:621 ] Throttling request took 1 .020925705s, request: GET:https://c100-e.us-east.containers.cloud.ibm.com:31941/apis/rbac.authorization.k8s.io/v1beta1?timeout = 32s INFO [ 0002 ] Fetching CRDs for version \"0.13.0\" INFO [ 0002 ] Fetching resources for version \"0.13.0\" INFO [ 0003 ] Successfully got OLM status for version \"0.13.0\" NAME NAMESPACE KIND STATUS installplans.operators.coreos.com CustomResourceDefinition Installed clusterserviceversions.operators.coreos.com CustomResourceDefinition Installed aggregate-olm-view ClusterRole Installed operatorgroups.operators.coreos.com CustomResourceDefinition Installed catalogsources.operators.coreos.com CustomResourceDefinition Installed subscriptions.operators.coreos.com CustomResourceDefinition Installed system:controller:operator-lifecycle-manager ClusterRole Installed aggregate-olm-edit ClusterRole Installed olm-operator-binding-olm ClusterRoleBinding clusterrolebindings.rbac.authorization.k8s.io \"olm-operator-binding-olm\" not found olm-operator-serviceaccount olm ServiceAccount serviceaccounts \"olm-operator-serviceaccount\" not found olm-operator olm Deployment deployments.apps \"olm-operator\" not found catalog-operator olm Deployment deployments.apps \"catalog-operator\" not found operators Namespace namespaces \"operators\" not found olm Namespace namespaces \"olm\" not found global-operators operators OperatorGroup operatorgroups.operators.coreos.com \"global-operators\" not found olm-operators olm OperatorGroup operatorgroups.operators.coreos.com \"olm-operators\" not found packageserver olm ClusterServiceVersion clusterserviceversions.operators.coreos.com \"packageserver\" not found operatorhubio-catalog olm CatalogSource catalogsources.operators.coreos.com \"operatorhubio-catalog\" not found [Note: OLM is partially enabled which is sufficient to complete this lab.] Create a bundle: operator-sdk generate bundle --version 1 .0.0 Output of the command: INFO [ 0000 ] Generating bundle manifests version 1 .0.0 Display name for the operator ( required ) : > Guestbook Operator Description for the operator ( required ) : > Demo helm operator for Guestbook Provider 's name for the operator (required): > IBM Any relevant URL for the provider name (optional): > https://github.com/rojanjose/guestbook-helm-operator Comma-separated list of keywords for your operator (required): > helm,operator,kubernetes,openshift Comma-separated list of maintainers and their emails (e.g. ' name1:email1, name2:email2 ' ) ( required ) : > Rojan:rojanjose@gmail.com INFO [ 0164 ] Bundle manifests generated successfully in deploy/olm-catalog/guestbook-operator-project INFO [ 0164 ] Building annotations.yaml INFO [ 0164 ] Writing annotations.yaml in /Users/operator/guestbook-operator-project/deploy/olm-catalog/guestbook-operator-project/metadata INFO [ 0164 ] Building Dockerfile INFO [ 0164 ] Writing bundle.Dockerfile in /Users/operator/guestbook-operator-project A bundle manifests directory deploy/olm-catalog/guestbook-operator-project/manifests containing a CSV and all CRDs in deploy/crds and a bundle metadata directory deploy/olm-catalog/guestbook-operator-project/metadata are generated. Create Project where operator OLM should be installed: oc new-project guest-operator-ns Output: Now using project \"guest-operator-ns\" on server \"https://c100-e.us-east.containers.cloud.ibm.com:31941\" . You can add applications to this project with the 'new-app' command. For example, try: oc new-app django-psql-example ... Create an OperatorGroup yaml definition: cat <<EOF >>deploy/operator_group.yaml apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: guestbook-og namespace: guest-operator-ns spec: targetNamespaces: - guest-operator-ns EOF Replace placeholder string with project guest-operator-ns in guestbook-operator.clusterserviceversion.yaml sed -i 's#namespace: placeholder#namespace: guest-operator-ns#' deploy/olm-catalog/guestbook-operator-project/manifests/guestbook-operator-project.clusterserviceversion.yaml or on Mac, sed -i \"\" 's#namespace: placeholder#namespace: guest-operator-ns#' deploy/olm-catalog/guestbook-operator-project/manifests/guestbook-operator-project.clusterserviceversion.yaml","title":"4. Deploy the Operator with the Operator Lifecycle Manager (OLM)"},{"location":"generatedContent/kubernetes-operators/lab3-v0.19/#5-install-the-operator","text":"Create the Operator group: oc create -f deploy/operator_group.yaml Apply the Operator\u2019s CSV manifest to the specified namespace in the cluster: oc create -f deploy/olm-catalog/guestbook-operator-project/manifests/guestbook-operator-project.clusterserviceversion.yaml Create the role, role binding, and service account to grant resource permissions to the Operator to create the Guestbook type that the Operator manages: oc create -f deploy/service_account.yaml oc create -f deploy/role.yaml oc create -f deploy/role_binding.yaml Wait for few minutes for the Guestbook operator to complete the installation. oc get ClusterServiceVersion ClusterServiceVersion should show a PHASE value of Succeeded , $ oc get ClusterServiceVersion NAME DISPLAY VERSION REPLACES PHASE guestbook-operator-project.v1.0.0 Guestbook Operator 1 .0.0 Succeeded Check the list of Installed Operators under the project guest-operator-ns . Open the operator and validate that install succeeded. Now, create an instance of Guestbook helm chart. Click on Create instance icon. Goto Workloads > Pods to view the pods. You should see 2 frontend pods, 1 Redis master, 2 Redis slave and pod supporting the Guestbook operator OLM.","title":"5. Install the operator"},{"location":"generatedContent/kubernetes-operators/lab3-v0.19/#6-update-the-guestbook-application-instance","text":"Open the deploy/crds/helm.operator-sdk_v1alpha1_guestbook_cr.yaml and change the value of replicaCount to 4. Save the file and run the oc apply command: oc apply -f deploy/crds/helm.operator-sdk_v1alpha1_guestbook_cr.yaml Run oc get pods to validate the guesbook pod count: oc get podsNAME READY STATUS RESTARTS AGE example-guestbook-6fdb6776b-hdq64 1 /1 Running 0 41m example-guestbook-6fdb6776b-pktr8 1 /1 Running 0 8s example-guestbook-6fdb6776b-x2nqw 1 /1 Running 0 41m example-guestbook-6fdb6776b-xz8lp 1 /1 Running 0 77s guestbook-operator-project-767cc5686c-ksmxq 1 /1 Running 0 52m redis-master-68857cd57c-pwctp 1 /1 Running 0 41m redis-slave-bbd8d8545-6jk8m 1 /1 Running 0 41m redis-slave-bbd8d8545-k65wz 1 /1 Running 0 41m","title":"6. Update the Guestbook application instance"},{"location":"generatedContent/kubernetes-operators/lab3-v0.19/#7-clean-up","text":"Run the oc delete commands to remove the operator. oc delete -f deploy/crds/helm.operator-sdk_v1alpha1_guestbook_cr.yaml oc delete -f deploy/service_account.yaml oc delete -f deploy/role.yaml oc delete -f deploy/role_binding.yaml oc delete -f deploy/olm-catalog/guestbook-operator-project/manifests/guestbook-operator-project.clusterserviceversion.yaml oc delete -f deploy/operator_group.yaml oc delete -f deploy/crds/helm.operator-sdk_guestbooks_crd.yaml","title":"7. Clean up"},{"location":"generatedContent/kubernetes-operators/lab3/","text":"Create an Operator using an Existing Helm Chart \u00b6 The Operator Framework is an open source project that provides developer and runtime Kubernetes tools, enabling you to accelerate the development of an Operator. The Operator SDK provides the tools to build, test and package Operators. The following workflow is to build an operator using an existing Helm chart : Create a new operator project and initialize it using the SDK Command Line Interface(CLI) Create the API to generate the CRD files for the chart. Build the Operator container image and push it to a registry. Apply the CRD in the cluster and deploy the operator image. Deploy the operand by applying the custom resource (CR) into the cluster. Cleanup the deployment. In this lab, we will use the IBM Guestbook helm chart available here as the base to scaffold a new operator. Information on creating a new operator can be found here Operator SDK made several technology and architecture changes with the release of v1.0 which as listed here . Setup \u00b6 The following must be done before you can get started on the lab: Create your lab environment by following the steps found here The lab requires a newer version of the operator-sdk installed. In the lab terminal, run the commands shown below to install the prerequisites: source < ( curl -s https://raw.githubusercontent.com/ibm/kubernetes-operators/master/src/scripts/operatorInstall.sh ) export PATH = \" ${ HOME } /bin: ${ PATH } \" $ source <(curl -s https://raw.githubusercontent.com/ibm/kubernetes-operators/master/src/scripts/operatorInstall.sh) Downloading operaror-sdk-v1.3.0-linux_amd64 ... % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 632 100 632 0 0 1876 0 --:--:-- --:--:-- --:--:-- 1875 100 64.8M 100 64.8M 0 0 61.9M 0 0:00:01 0:00:01 --:--:-- 61.9M operaror-sdk-v1.3.0-linux_amd64 downloaded. ...... Run a version check after the operator-sdk installation is complete: operator-sdk version $ operator-sdk version operator-sdk version: \"v1.3.0\", commit: \"1abf57985b43bf6a59dcd18147b3c574fa57d3f6\", kubernetes version: \"1.19.4\", go version: \"go1.15.5\", GOOS: \"linux\", GOARCH: \"amd64\" Log into the OpenShift cluster: Scroll down on the Quick Links and Common commands page until you see a terminal command block with green text and a description above it that says Log in to your OpenShift cluster. Click on the command and it will automatically paste into your terminal and execute. This lab uses docker registry to container image storage. Create a new docker hub id, if you do not have one. Create the operator \u00b6 1. Create a new project & initialize it using SDK \u00b6 Certain parameters will be used repetitively. Export these parameters as environment variables prior to starting the project. Replace <your-docker-username> with your docker hub id. export DOCKER_USERNAME = <your-docker-username> Set names for the operator, project and operator version. The operator container images is built using these values. export OPERATOR_NAME = guestbook-operator export OPERATOR_PROJECT = guestbook-operator-project export OPERATOR_VERSION = v1.0.0 export IMAGE = docker.io/ ${ DOCKER_USERNAME } / ${ OPERATOR_NAME } : ${ OPERATOR_VERSION } Create the project directory for the operator. mkdir -p ${ OPERATOR_PROJECT } cd ${ OPERATOR_PROJECT } Use the operator SDK to initialize the project. Specify the plugin and API group as the parameters for this command. operator-sdk init --plugins = helm --domain guestbook.ibm.com $ operator-sdk init --plugins=helm --domain guestbook.ibm.com Next: define a resource with: $ operator-sdk create api The initialization step create a scaffolding with the operator boiler plate code. At high level, this creates the config directory, watches.yaml and the place holder for the helm chart. Use the command tree . to view the complete directory structure as shown in the block below: . \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 PROJECT \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 default \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u2514\u2500\u2500 manager_auth_proxy_patch.yaml \u2502 \u251c\u2500\u2500 manager \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u2514\u2500\u2500 manager.yaml \u2502 \u251c\u2500\u2500 prometheus \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u2514\u2500\u2500 monitor.yaml \u2502 \u251c\u2500\u2500 rbac \u2502 \u2502 \u251c\u2500\u2500 auth_proxy_client_clusterrole.yaml \u2502 \u2502 \u251c\u2500\u2500 auth_proxy_role.yaml \u2502 \u2502 \u251c\u2500\u2500 auth_proxy_role_binding.yaml \u2502 \u2502 \u251c\u2500\u2500 auth_proxy_service.yaml \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u251c\u2500\u2500 leader_election_role.yaml \u2502 \u2502 \u251c\u2500\u2500 leader_election_role_binding.yaml \u2502 \u2502 \u251c\u2500\u2500 role.yaml \u2502 \u2502 \u2514\u2500\u2500 role_binding.yaml \u2502 \u2514\u2500\u2500 scorecard \u2502 \u251c\u2500\u2500 bases \u2502 \u2502 \u2514\u2500\u2500 config.yaml \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2514\u2500\u2500 patches \u2502 \u251c\u2500\u2500 basic.config.yaml \u2502 \u2514\u2500\u2500 olm.config.yaml \u251c\u2500\u2500 helm-charts \u2514\u2500\u2500 watches.yaml Operator SDK uses the kubernetes Kustomize tool for managing the deployment of yaml files, hence you see the kustomization.yaml in all the directories. config/default and confg/manager contains the specification to inject the controller manager container into the operator pod as a side car. The confg/rbac folder contains a set of default access control rules. Review the Makefile to understand the operator-sdk , kustomize and docker commands executed for various tasks. 2. Create the API to generate the CRD files for the chart. \u00b6 Next step, create the API artifacts. Provide the name and the location of the helm chart as input parameters to this command. This command will create the crd folder with the custom resource definition for the Guestbook operator. The command picks the latest version of the helm chart, if the helm version parameter is ignored. operator-sdk create api --helm-chart=guestbook --helm-chart-repo=https://raw.githubusercontent.com/IBM/helm101/master/ operator-sdk create api --helm-chart=guestbook --helm-chart-repo=https://raw.githubusercontent.com/IBM/helm101/master/ Created helm-charts/guestbook Generating RBAC rules I0202 15:46:05.545032 48799 request.go:645] Throttling request took 1.005544854s, request: GET:https://c107-e.us-south.containers.cloud.ibm.com:30606/apis/extensions/v1beta1?timeout=32s WARN[0003] The RBAC rules generated in config/rbac/role.yaml are based on the chart's default manifest. Some rules may be missing for resources that are only enabled with custom values, and some existing rules may be overly broad. Double check the rules generated in config/rbac/role.yaml to ensure they meet the operator's permission requirements. Check the new additions to the scaffolding using the tree . command: . \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 PROJECT \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 crd \u2502 \u2502 \u251c\u2500\u2500 bases \u2502 \u2502 \u2502 \u2514\u2500\u2500 charts.guestbook.ibm.com_guestbooks.yaml \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 default \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u2514\u2500\u2500 manager_auth_proxy_patch.yaml \u2502 \u251c\u2500\u2500 manager \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u2514\u2500\u2500 manager.yaml \u2502 \u251c\u2500\u2500 prometheus \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u2514\u2500\u2500 monitor.yaml \u2502 \u251c\u2500\u2500 rbac \u2502 \u2502 \u251c\u2500\u2500 auth_proxy_client_clusterrole.yaml \u2502 \u2502 \u251c\u2500\u2500 auth_proxy_role.yaml \u2502 \u2502 \u251c\u2500\u2500 auth_proxy_role_binding.yaml \u2502 \u2502 \u251c\u2500\u2500 auth_proxy_service.yaml \u2502 \u2502 \u251c\u2500\u2500 guestbook_editor_role.yaml \u2502 \u2502 \u251c\u2500\u2500 guestbook_viewer_role.yaml \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u251c\u2500\u2500 leader_election_role.yaml \u2502 \u2502 \u251c\u2500\u2500 leader_election_role_binding.yaml \u2502 \u2502 \u251c\u2500\u2500 role.yaml \u2502 \u2502 \u2514\u2500\u2500 role_binding.yaml \u2502 \u251c\u2500\u2500 samples \u2502 \u2502 \u2514\u2500\u2500 charts_v1alpha1_guestbook.yaml \u2502 \u2514\u2500\u2500 scorecard \u2502 \u251c\u2500\u2500 bases \u2502 \u2502 \u2514\u2500\u2500 config.yaml \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2514\u2500\u2500 patches \u2502 \u251c\u2500\u2500 basic.config.yaml \u2502 \u2514\u2500\u2500 olm.config.yaml \u251c\u2500\u2500 helm-charts \u2502 \u2514\u2500\u2500 guestbook \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u251c\u2500\u2500 LICENSE \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 templates \u2502 \u2502 \u251c\u2500\u2500 NOTES.txt \u2502 \u2502 \u251c\u2500\u2500 _helpers.tpl \u2502 \u2502 \u251c\u2500\u2500 guestbook-deployment.yaml \u2502 \u2502 \u251c\u2500\u2500 guestbook-service.yaml \u2502 \u2502 \u251c\u2500\u2500 redis-master-deployment.yaml \u2502 \u2502 \u251c\u2500\u2500 redis-master-service.yaml \u2502 \u2502 \u251c\u2500\u2500 redis-slave-deployment.yaml \u2502 \u2502 \u2514\u2500\u2500 redis-slave-service.yaml \u2502 \u2514\u2500\u2500 values.yaml \u2514\u2500\u2500 watches.yaml View the contents of the CRD. Note the values for names and schema.openAPIV3Schema.properties . more config/crd/bases/charts.guestbook.ibm.com_guestbooks.yaml apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: guestbooks.charts.guestbook.ibm.com spec: group: charts.guestbook.ibm.com names: kind: Guestbook listKind: GuestbookList plural: guestbooks singular: guestbook scope: Namespaced versions: - name: v1alpha1 schema: openAPIV3Schema: ... 3. Build the Operator container image and push it to registry. \u00b6 Login into the docker registry using your personal id and password. docker login docker.io -u $DOCKER_USERNAME $ docker login docker.io -u $DOCKER_USERNAME Password: WARNING! Your password will be stored unencrypted in /home/student/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded Build the Guestbook operator container image and push image to the docker hub registry. make docker-build docker-push IMG = ${ IMAGE } make docker-build docker-push IMG=${IMAGE} docker build . -t docker.io/rojanjose/guestbook-operator:v1.0.0 [+] Building 4.2s (9/9) FINISHED => [internal] load .dockerignore 0.0s => => transferring context: 2B 0.0s => [internal] load build definition from Dockerfile 0.0s => => transferring dockerfile: 237B ........... ........... 753e76240780: Pushed 4a3bef90e857: Pushed d0e9a59c2057: Pushed 1d8db7e222a6: Pushed 00af10937683: Pushed 3aa55ff7bca1: Pushed v1.0.0: digest: sha256:c0724c7f31a748094621b7623a81fae107511c23819b729f25878f7e5a7377dd size: 1984 You can view the local docker images by running: docker images $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE rojanjose/guestbook-operator v1.0.0 590c0196c2b6 10 seconds ago 160MB quay.io/operator-framework/helm-operator v1.3.0 57683a970d10 6 weeks ago 160MB 4. Apply the CRD in the cluster and deploy the operator image. \u00b6 Install the Guestbook customer resource definition using the make install command: make install make install /home/student/guestbook-operator-project/bin/kustomize build config/crd | kubectl apply -f - customresourcedefinition.apiextensions.k8s.io/guestbooks.charts.guestbook.ibm.com created View the deployed CRD oc describe CustomResourceDefinition guestbooks.charts.guestbook.ibm.com Next step is to deploy the operator. Note that the operator is installed in its own namespace guestbook-operator-project-system . make deploy IMG=${IMAGE} $ make deploy IMG=${IMAGE} cd config/manager && /home/student/guestbook-operator-project/bin/kustomize edit set image controller=docker.io/rojanjose/guestbook-operator:v1.0.0 /home/student/guestbook-operator-project/bin/kustomize build config/default | kubectl apply -f - namespace/guestbook-operator-project-system created customresourcedefinition.apiextensions.k8s.io/guestbooks.charts.guestbook.ibm.com unchanged role.rbac.authorization.k8s.io/guestbook-operator-project-leader-election-role created clusterrole.rbac.authorization.k8s.io/guestbook-operator-project-manager-role created clusterrole.rbac.authorization.k8s.io/guestbook-operator-project-metrics-reader created clusterrole.rbac.authorization.k8s.io/guestbook-operator-project-proxy-role created rolebinding.rbac.authorization.k8s.io/guestbook-operator-project-leader-election-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/guestbook-operator-project-manager-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/guestbook-operator-project-proxy-rolebinding created service/guestbook-operator-project-controller-manager-metrics-service created deployment.apps/guestbook-operator-project-controller-manager created View of what got deployed: oc get all -n ${ OPERATOR_PROJECT } -system $ oc get all -n ${OPERATOR_PROJECT}-system NAME READY STATUS RESTARTS AGE pod/guestbook-operator-project-controller-manager-7bc6f986dd-2r898 2/2 Running 0 2m24s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook-operator-project-controller-manager-metrics-service ClusterIP 172.21.110.64 <none> 8443/TCP 2m24s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-operator-project-controller-manager 1/1 1 1 2m24s NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-operator-project-controller-manager-7bc6f986dd 1 1 1 2m24s 5. Deploy the operand by applying the custom resource (CR) into the cluster. \u00b6 Create a new project called guestbook where Guestbook application will be deployed. oc new-project guestbook $ oc new-project guestbook Now using project \"guestbook\" on server \"https://c107-e.us-south.containers.cloud.ibm.com:30606\". You can add applications to this project with the 'new-app' command. For example, try: oc new-app ruby~https://github.com/sclorg/ruby-ex.git to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application: kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node An example custom resource yaml file was automatically generated under the config/samples directory as part of the create API step earlier. This is based on the default values.yaml from the Guestbook helm chart under helm-charts/guestbook/values.yaml . Let's use this file to create the operand. oc apply -f config/samples/charts_v1alpha1_guestbook.yaml $ oc apply -f config/samples/charts_v1alpha1_guestbook.yaml guestbook.charts.guestbook.ibm.com/guestbook-sample created Outcome of the operand deploy can be viewed by running the command: oc get all -n guestbook $ oc get all -n guestbook NAME READY STATUS RESTARTS AGE pod/guestbook-sample-8594c8dc46-bl7sm 1/1 Running 0 75s pod/guestbook-sample-8594c8dc46-qwgkv 1/1 Running 0 75s pod/redis-master-68857cd57c-bjxt5 1/1 Running 0 75s pod/redis-slave-bbd8d8545-57944 1/1 Running 0 75s pod/redis-slave-bbd8d8545-rxqc5 1/1 Running 0 75s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook-sample LoadBalancer 172.21.41.67 169.45.217.90 3000:30940/TCP 75s service/redis-master ClusterIP 172.21.206.242 <none> 6379/TCP 75s service/redis-slave ClusterIP 172.21.133.74 <none> 6379/TCP 75s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-sample 2/2 2 2 75s deployment.apps/redis-master 1/1 1 1 75s deployment.apps/redis-slave 2/2 2 2 75s NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-sample-8594c8dc46 2 2 2 75s replicaset.apps/redis-master-68857cd57c 1 1 1 75s replicaset.apps/redis-slave-bbd8d8545 2 2 2 75s Validate the Guestbook application is running by accessing it with the following commands: HOSTNAME = ` oc get nodes -ojsonpath = '{.items[0].metadata.labels.ibm-cloud\\.kubernetes\\.io\\/external-ip}' ` SERVICEPORT = ` oc get svc guestbook-sample -o = jsonpath = '{.spec.ports[0].nodePort}' ` echo \"http:// $HOSTNAME : $SERVICEPORT \" http://169.45.242.104:30940 Copy and paste the above URL in a new browser tab to get to the Guestbook landing page as shown below: Open the OpenShift console to view the artificats created as part of this exerice. Guestbook operator pod: Guestbook application pods: 6. Cleanup the deployment. \u00b6 Remove the Guestbook application by deleting the customer resource (CR). oc delete -f config/samples/charts_v1alpha1_guestbook.yaml $ oc delete -f config/samples/charts_v1alpha1_guestbook.yaml guestbook.charts.guestbook.ibm.com \"guestbook-sample\" deleted $ oc get all -n guestbook No resources found in guestbook namespace. $ oc delete project guestbook project.project.openshift.io \"guestbook\" deleted Finally, to delete the CRDs, run the make undeploy command: make undeploy $ make undeploy /home/student/guestbook-operator-project/bin/kustomize build config/default | kubectl delete -f - namespace \"gb-helm-operator-system\" deleted customresourcedefinition.apiextensions.k8s.io \"guestbooks.charts.guestbook.ibm.com\" deleted role.rbac.authorization.k8s.io \"gb-helm-operator-leader-election-role\" deleted clusterrole.rbac.authorization.k8s.io \"gb-helm-operator-manager-role\" deleted clusterrole.rbac.authorization.k8s.io \"gb-helm-operator-metrics-reader\" deleted clusterrole.rbac.authorization.k8s.io \"gb-helm-operator-proxy-role\" deleted rolebinding.rbac.authorization.k8s.io \"gb-helm-operator-leader-election-rolebinding\" deleted clusterrolebinding.rbac.authorization.k8s.io \"gb-helm-operator-manager-rolebinding\" deleted clusterrolebinding.rbac.authorization.k8s.io \"gb-helm-operator-proxy-rolebinding\" deleted service \"gb-helm-operator-controller-manager-metrics-service\" deleted deployment.apps \"gb-helm-operator-controller-manager\" deleted This concludes the Helm Operator lab. Go here for additional information on building operators using Helm.","title":"Lab 3. Operator from Helm"},{"location":"generatedContent/kubernetes-operators/lab3/#create-an-operator-using-an-existing-helm-chart","text":"The Operator Framework is an open source project that provides developer and runtime Kubernetes tools, enabling you to accelerate the development of an Operator. The Operator SDK provides the tools to build, test and package Operators. The following workflow is to build an operator using an existing Helm chart : Create a new operator project and initialize it using the SDK Command Line Interface(CLI) Create the API to generate the CRD files for the chart. Build the Operator container image and push it to a registry. Apply the CRD in the cluster and deploy the operator image. Deploy the operand by applying the custom resource (CR) into the cluster. Cleanup the deployment. In this lab, we will use the IBM Guestbook helm chart available here as the base to scaffold a new operator. Information on creating a new operator can be found here Operator SDK made several technology and architecture changes with the release of v1.0 which as listed here .","title":"Create an Operator using an Existing Helm Chart"},{"location":"generatedContent/kubernetes-operators/lab3/#setup","text":"The following must be done before you can get started on the lab: Create your lab environment by following the steps found here The lab requires a newer version of the operator-sdk installed. In the lab terminal, run the commands shown below to install the prerequisites: source < ( curl -s https://raw.githubusercontent.com/ibm/kubernetes-operators/master/src/scripts/operatorInstall.sh ) export PATH = \" ${ HOME } /bin: ${ PATH } \" $ source <(curl -s https://raw.githubusercontent.com/ibm/kubernetes-operators/master/src/scripts/operatorInstall.sh) Downloading operaror-sdk-v1.3.0-linux_amd64 ... % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 632 100 632 0 0 1876 0 --:--:-- --:--:-- --:--:-- 1875 100 64.8M 100 64.8M 0 0 61.9M 0 0:00:01 0:00:01 --:--:-- 61.9M operaror-sdk-v1.3.0-linux_amd64 downloaded. ...... Run a version check after the operator-sdk installation is complete: operator-sdk version $ operator-sdk version operator-sdk version: \"v1.3.0\", commit: \"1abf57985b43bf6a59dcd18147b3c574fa57d3f6\", kubernetes version: \"1.19.4\", go version: \"go1.15.5\", GOOS: \"linux\", GOARCH: \"amd64\" Log into the OpenShift cluster: Scroll down on the Quick Links and Common commands page until you see a terminal command block with green text and a description above it that says Log in to your OpenShift cluster. Click on the command and it will automatically paste into your terminal and execute. This lab uses docker registry to container image storage. Create a new docker hub id, if you do not have one.","title":"Setup"},{"location":"generatedContent/kubernetes-operators/lab3/#create-the-operator","text":"","title":"Create the operator"},{"location":"generatedContent/kubernetes-operators/lab3/#1-create-a-new-project-initialize-it-using-sdk","text":"Certain parameters will be used repetitively. Export these parameters as environment variables prior to starting the project. Replace <your-docker-username> with your docker hub id. export DOCKER_USERNAME = <your-docker-username> Set names for the operator, project and operator version. The operator container images is built using these values. export OPERATOR_NAME = guestbook-operator export OPERATOR_PROJECT = guestbook-operator-project export OPERATOR_VERSION = v1.0.0 export IMAGE = docker.io/ ${ DOCKER_USERNAME } / ${ OPERATOR_NAME } : ${ OPERATOR_VERSION } Create the project directory for the operator. mkdir -p ${ OPERATOR_PROJECT } cd ${ OPERATOR_PROJECT } Use the operator SDK to initialize the project. Specify the plugin and API group as the parameters for this command. operator-sdk init --plugins = helm --domain guestbook.ibm.com $ operator-sdk init --plugins=helm --domain guestbook.ibm.com Next: define a resource with: $ operator-sdk create api The initialization step create a scaffolding with the operator boiler plate code. At high level, this creates the config directory, watches.yaml and the place holder for the helm chart. Use the command tree . to view the complete directory structure as shown in the block below: . \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 PROJECT \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 default \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u2514\u2500\u2500 manager_auth_proxy_patch.yaml \u2502 \u251c\u2500\u2500 manager \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u2514\u2500\u2500 manager.yaml \u2502 \u251c\u2500\u2500 prometheus \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u2514\u2500\u2500 monitor.yaml \u2502 \u251c\u2500\u2500 rbac \u2502 \u2502 \u251c\u2500\u2500 auth_proxy_client_clusterrole.yaml \u2502 \u2502 \u251c\u2500\u2500 auth_proxy_role.yaml \u2502 \u2502 \u251c\u2500\u2500 auth_proxy_role_binding.yaml \u2502 \u2502 \u251c\u2500\u2500 auth_proxy_service.yaml \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u251c\u2500\u2500 leader_election_role.yaml \u2502 \u2502 \u251c\u2500\u2500 leader_election_role_binding.yaml \u2502 \u2502 \u251c\u2500\u2500 role.yaml \u2502 \u2502 \u2514\u2500\u2500 role_binding.yaml \u2502 \u2514\u2500\u2500 scorecard \u2502 \u251c\u2500\u2500 bases \u2502 \u2502 \u2514\u2500\u2500 config.yaml \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2514\u2500\u2500 patches \u2502 \u251c\u2500\u2500 basic.config.yaml \u2502 \u2514\u2500\u2500 olm.config.yaml \u251c\u2500\u2500 helm-charts \u2514\u2500\u2500 watches.yaml Operator SDK uses the kubernetes Kustomize tool for managing the deployment of yaml files, hence you see the kustomization.yaml in all the directories. config/default and confg/manager contains the specification to inject the controller manager container into the operator pod as a side car. The confg/rbac folder contains a set of default access control rules. Review the Makefile to understand the operator-sdk , kustomize and docker commands executed for various tasks.","title":"1. Create a new project &amp; initialize it using SDK"},{"location":"generatedContent/kubernetes-operators/lab3/#2-create-the-api-to-generate-the-crd-files-for-the-chart","text":"Next step, create the API artifacts. Provide the name and the location of the helm chart as input parameters to this command. This command will create the crd folder with the custom resource definition for the Guestbook operator. The command picks the latest version of the helm chart, if the helm version parameter is ignored. operator-sdk create api --helm-chart=guestbook --helm-chart-repo=https://raw.githubusercontent.com/IBM/helm101/master/ operator-sdk create api --helm-chart=guestbook --helm-chart-repo=https://raw.githubusercontent.com/IBM/helm101/master/ Created helm-charts/guestbook Generating RBAC rules I0202 15:46:05.545032 48799 request.go:645] Throttling request took 1.005544854s, request: GET:https://c107-e.us-south.containers.cloud.ibm.com:30606/apis/extensions/v1beta1?timeout=32s WARN[0003] The RBAC rules generated in config/rbac/role.yaml are based on the chart's default manifest. Some rules may be missing for resources that are only enabled with custom values, and some existing rules may be overly broad. Double check the rules generated in config/rbac/role.yaml to ensure they meet the operator's permission requirements. Check the new additions to the scaffolding using the tree . command: . \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 PROJECT \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 crd \u2502 \u2502 \u251c\u2500\u2500 bases \u2502 \u2502 \u2502 \u2514\u2500\u2500 charts.guestbook.ibm.com_guestbooks.yaml \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 default \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u2514\u2500\u2500 manager_auth_proxy_patch.yaml \u2502 \u251c\u2500\u2500 manager \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u2514\u2500\u2500 manager.yaml \u2502 \u251c\u2500\u2500 prometheus \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u2514\u2500\u2500 monitor.yaml \u2502 \u251c\u2500\u2500 rbac \u2502 \u2502 \u251c\u2500\u2500 auth_proxy_client_clusterrole.yaml \u2502 \u2502 \u251c\u2500\u2500 auth_proxy_role.yaml \u2502 \u2502 \u251c\u2500\u2500 auth_proxy_role_binding.yaml \u2502 \u2502 \u251c\u2500\u2500 auth_proxy_service.yaml \u2502 \u2502 \u251c\u2500\u2500 guestbook_editor_role.yaml \u2502 \u2502 \u251c\u2500\u2500 guestbook_viewer_role.yaml \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u251c\u2500\u2500 leader_election_role.yaml \u2502 \u2502 \u251c\u2500\u2500 leader_election_role_binding.yaml \u2502 \u2502 \u251c\u2500\u2500 role.yaml \u2502 \u2502 \u2514\u2500\u2500 role_binding.yaml \u2502 \u251c\u2500\u2500 samples \u2502 \u2502 \u2514\u2500\u2500 charts_v1alpha1_guestbook.yaml \u2502 \u2514\u2500\u2500 scorecard \u2502 \u251c\u2500\u2500 bases \u2502 \u2502 \u2514\u2500\u2500 config.yaml \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2514\u2500\u2500 patches \u2502 \u251c\u2500\u2500 basic.config.yaml \u2502 \u2514\u2500\u2500 olm.config.yaml \u251c\u2500\u2500 helm-charts \u2502 \u2514\u2500\u2500 guestbook \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u251c\u2500\u2500 LICENSE \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 templates \u2502 \u2502 \u251c\u2500\u2500 NOTES.txt \u2502 \u2502 \u251c\u2500\u2500 _helpers.tpl \u2502 \u2502 \u251c\u2500\u2500 guestbook-deployment.yaml \u2502 \u2502 \u251c\u2500\u2500 guestbook-service.yaml \u2502 \u2502 \u251c\u2500\u2500 redis-master-deployment.yaml \u2502 \u2502 \u251c\u2500\u2500 redis-master-service.yaml \u2502 \u2502 \u251c\u2500\u2500 redis-slave-deployment.yaml \u2502 \u2502 \u2514\u2500\u2500 redis-slave-service.yaml \u2502 \u2514\u2500\u2500 values.yaml \u2514\u2500\u2500 watches.yaml View the contents of the CRD. Note the values for names and schema.openAPIV3Schema.properties . more config/crd/bases/charts.guestbook.ibm.com_guestbooks.yaml apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: guestbooks.charts.guestbook.ibm.com spec: group: charts.guestbook.ibm.com names: kind: Guestbook listKind: GuestbookList plural: guestbooks singular: guestbook scope: Namespaced versions: - name: v1alpha1 schema: openAPIV3Schema: ...","title":"2. Create the API to generate the CRD files for the chart."},{"location":"generatedContent/kubernetes-operators/lab3/#3-build-the-operator-container-image-and-push-it-to-registry","text":"Login into the docker registry using your personal id and password. docker login docker.io -u $DOCKER_USERNAME $ docker login docker.io -u $DOCKER_USERNAME Password: WARNING! Your password will be stored unencrypted in /home/student/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded Build the Guestbook operator container image and push image to the docker hub registry. make docker-build docker-push IMG = ${ IMAGE } make docker-build docker-push IMG=${IMAGE} docker build . -t docker.io/rojanjose/guestbook-operator:v1.0.0 [+] Building 4.2s (9/9) FINISHED => [internal] load .dockerignore 0.0s => => transferring context: 2B 0.0s => [internal] load build definition from Dockerfile 0.0s => => transferring dockerfile: 237B ........... ........... 753e76240780: Pushed 4a3bef90e857: Pushed d0e9a59c2057: Pushed 1d8db7e222a6: Pushed 00af10937683: Pushed 3aa55ff7bca1: Pushed v1.0.0: digest: sha256:c0724c7f31a748094621b7623a81fae107511c23819b729f25878f7e5a7377dd size: 1984 You can view the local docker images by running: docker images $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE rojanjose/guestbook-operator v1.0.0 590c0196c2b6 10 seconds ago 160MB quay.io/operator-framework/helm-operator v1.3.0 57683a970d10 6 weeks ago 160MB","title":"3. Build the Operator container image and push it to registry."},{"location":"generatedContent/kubernetes-operators/lab3/#4-apply-the-crd-in-the-cluster-and-deploy-the-operator-image","text":"Install the Guestbook customer resource definition using the make install command: make install make install /home/student/guestbook-operator-project/bin/kustomize build config/crd | kubectl apply -f - customresourcedefinition.apiextensions.k8s.io/guestbooks.charts.guestbook.ibm.com created View the deployed CRD oc describe CustomResourceDefinition guestbooks.charts.guestbook.ibm.com Next step is to deploy the operator. Note that the operator is installed in its own namespace guestbook-operator-project-system . make deploy IMG=${IMAGE} $ make deploy IMG=${IMAGE} cd config/manager && /home/student/guestbook-operator-project/bin/kustomize edit set image controller=docker.io/rojanjose/guestbook-operator:v1.0.0 /home/student/guestbook-operator-project/bin/kustomize build config/default | kubectl apply -f - namespace/guestbook-operator-project-system created customresourcedefinition.apiextensions.k8s.io/guestbooks.charts.guestbook.ibm.com unchanged role.rbac.authorization.k8s.io/guestbook-operator-project-leader-election-role created clusterrole.rbac.authorization.k8s.io/guestbook-operator-project-manager-role created clusterrole.rbac.authorization.k8s.io/guestbook-operator-project-metrics-reader created clusterrole.rbac.authorization.k8s.io/guestbook-operator-project-proxy-role created rolebinding.rbac.authorization.k8s.io/guestbook-operator-project-leader-election-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/guestbook-operator-project-manager-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/guestbook-operator-project-proxy-rolebinding created service/guestbook-operator-project-controller-manager-metrics-service created deployment.apps/guestbook-operator-project-controller-manager created View of what got deployed: oc get all -n ${ OPERATOR_PROJECT } -system $ oc get all -n ${OPERATOR_PROJECT}-system NAME READY STATUS RESTARTS AGE pod/guestbook-operator-project-controller-manager-7bc6f986dd-2r898 2/2 Running 0 2m24s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook-operator-project-controller-manager-metrics-service ClusterIP 172.21.110.64 <none> 8443/TCP 2m24s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-operator-project-controller-manager 1/1 1 1 2m24s NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-operator-project-controller-manager-7bc6f986dd 1 1 1 2m24s","title":"4. Apply the CRD in the cluster and deploy the operator image."},{"location":"generatedContent/kubernetes-operators/lab3/#5-deploy-the-operand-by-applying-the-custom-resource-cr-into-the-cluster","text":"Create a new project called guestbook where Guestbook application will be deployed. oc new-project guestbook $ oc new-project guestbook Now using project \"guestbook\" on server \"https://c107-e.us-south.containers.cloud.ibm.com:30606\". You can add applications to this project with the 'new-app' command. For example, try: oc new-app ruby~https://github.com/sclorg/ruby-ex.git to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application: kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node An example custom resource yaml file was automatically generated under the config/samples directory as part of the create API step earlier. This is based on the default values.yaml from the Guestbook helm chart under helm-charts/guestbook/values.yaml . Let's use this file to create the operand. oc apply -f config/samples/charts_v1alpha1_guestbook.yaml $ oc apply -f config/samples/charts_v1alpha1_guestbook.yaml guestbook.charts.guestbook.ibm.com/guestbook-sample created Outcome of the operand deploy can be viewed by running the command: oc get all -n guestbook $ oc get all -n guestbook NAME READY STATUS RESTARTS AGE pod/guestbook-sample-8594c8dc46-bl7sm 1/1 Running 0 75s pod/guestbook-sample-8594c8dc46-qwgkv 1/1 Running 0 75s pod/redis-master-68857cd57c-bjxt5 1/1 Running 0 75s pod/redis-slave-bbd8d8545-57944 1/1 Running 0 75s pod/redis-slave-bbd8d8545-rxqc5 1/1 Running 0 75s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook-sample LoadBalancer 172.21.41.67 169.45.217.90 3000:30940/TCP 75s service/redis-master ClusterIP 172.21.206.242 <none> 6379/TCP 75s service/redis-slave ClusterIP 172.21.133.74 <none> 6379/TCP 75s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-sample 2/2 2 2 75s deployment.apps/redis-master 1/1 1 1 75s deployment.apps/redis-slave 2/2 2 2 75s NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-sample-8594c8dc46 2 2 2 75s replicaset.apps/redis-master-68857cd57c 1 1 1 75s replicaset.apps/redis-slave-bbd8d8545 2 2 2 75s Validate the Guestbook application is running by accessing it with the following commands: HOSTNAME = ` oc get nodes -ojsonpath = '{.items[0].metadata.labels.ibm-cloud\\.kubernetes\\.io\\/external-ip}' ` SERVICEPORT = ` oc get svc guestbook-sample -o = jsonpath = '{.spec.ports[0].nodePort}' ` echo \"http:// $HOSTNAME : $SERVICEPORT \" http://169.45.242.104:30940 Copy and paste the above URL in a new browser tab to get to the Guestbook landing page as shown below: Open the OpenShift console to view the artificats created as part of this exerice. Guestbook operator pod: Guestbook application pods:","title":"5. Deploy the operand by applying the custom resource (CR) into the cluster."},{"location":"generatedContent/kubernetes-operators/lab3/#6-cleanup-the-deployment","text":"Remove the Guestbook application by deleting the customer resource (CR). oc delete -f config/samples/charts_v1alpha1_guestbook.yaml $ oc delete -f config/samples/charts_v1alpha1_guestbook.yaml guestbook.charts.guestbook.ibm.com \"guestbook-sample\" deleted $ oc get all -n guestbook No resources found in guestbook namespace. $ oc delete project guestbook project.project.openshift.io \"guestbook\" deleted Finally, to delete the CRDs, run the make undeploy command: make undeploy $ make undeploy /home/student/guestbook-operator-project/bin/kustomize build config/default | kubectl delete -f - namespace \"gb-helm-operator-system\" deleted customresourcedefinition.apiextensions.k8s.io \"guestbooks.charts.guestbook.ibm.com\" deleted role.rbac.authorization.k8s.io \"gb-helm-operator-leader-election-role\" deleted clusterrole.rbac.authorization.k8s.io \"gb-helm-operator-manager-role\" deleted clusterrole.rbac.authorization.k8s.io \"gb-helm-operator-metrics-reader\" deleted clusterrole.rbac.authorization.k8s.io \"gb-helm-operator-proxy-role\" deleted rolebinding.rbac.authorization.k8s.io \"gb-helm-operator-leader-election-rolebinding\" deleted clusterrolebinding.rbac.authorization.k8s.io \"gb-helm-operator-manager-rolebinding\" deleted clusterrolebinding.rbac.authorization.k8s.io \"gb-helm-operator-proxy-rolebinding\" deleted service \"gb-helm-operator-controller-manager-metrics-service\" deleted deployment.apps \"gb-helm-operator-controller-manager\" deleted This concludes the Helm Operator lab. Go here for additional information on building operators using Helm.","title":"6. Cleanup the deployment."},{"location":"generatedContent/kubernetes-operators/lab6/","text":"Operator Tools \u00b6 To write applications that use the Kubernetes REST API, you can use one of the following supported client libraries: Go , Python , Java , CSharp dotnet , JavaScript , Haskell . In addition, there are many community-maintained client libraries . At the OperatorHub.io , you find ready to use operators written by the community. To write your own operator you can use existing tools: KUDO (Kubernetes Universal Declarative Operator), kubebuilder , Metacontroller using custom WebHooks, the Operator Framework .","title":"Operator Tools"},{"location":"generatedContent/kubernetes-operators/lab6/#operator-tools","text":"To write applications that use the Kubernetes REST API, you can use one of the following supported client libraries: Go , Python , Java , CSharp dotnet , JavaScript , Haskell . In addition, there are many community-maintained client libraries . At the OperatorHub.io , you find ready to use operators written by the community. To write your own operator you can use existing tools: KUDO (Kubernetes Universal Declarative Operator), kubebuilder , Metacontroller using custom WebHooks, the Operator Framework .","title":"Operator Tools"},{"location":"generatedContent/kubernetes-operators/setup/","text":"Client Setup \u00b6 Access the web-terminal \u00b6 When running the lab for Kubernetes Extensions, you can make use of a web-terminal. The Dockerfile to use is located in https://github.com/IBMAppModernization/web-terminal , and named Dockerfile-s2i-oc-tekton-operator . To run on localhost as a Docker container, git clone https://github.com/IBMAppModernization/web-terminal.git cd web-terminal docker build --no-cache -t web-terminal:latest -f Dockerfile-s2i-oc-tekton-operator . docker run -d --restart always --name terminal -p 7681 :7681 -v $HOME /dev/tmp:/root/dev web-terminal docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 85edc0b0ec27 web-terminal \"ttyd -p 7681 bash\" 17 minutes ago Up 17 minutes 0 .0.0.0:7681->7681/tcp terminal The volume mapping will write all files under the working directory to the host directory $HOME/dev/tmp . So suppose my host's user home directory is /Users/remkohdev@us.ibm.com/ . If I open the terminal in the browser, the working directory for the user is /root . Any file that is created under /root is created on the host's directory $HOME/dev/tmp . Similarly if I create a file in $HOME/dev/tmp it is available in the container's /root directory. Open the web-terminal in a browser and go to http://0.0.0.0:7681 . If Go, Operator SD export CLUSTERNAME = remkohdev-roks-labs-3n-cluster ibmcloud login Go to the OpenShift web console Copy Login command oc login --token = _12AbcD345kIPDIRg2jYpCuZ-g5SM5Im9irY2tol4Q8 --server = https://c100-e.us-south.containers.cloud.ibm.com:30712","title":"Lab 0. Setup"},{"location":"generatedContent/kubernetes-operators/setup/#client-setup","text":"","title":"Client Setup"},{"location":"generatedContent/kubernetes-operators/setup/#access-the-web-terminal","text":"When running the lab for Kubernetes Extensions, you can make use of a web-terminal. The Dockerfile to use is located in https://github.com/IBMAppModernization/web-terminal , and named Dockerfile-s2i-oc-tekton-operator . To run on localhost as a Docker container, git clone https://github.com/IBMAppModernization/web-terminal.git cd web-terminal docker build --no-cache -t web-terminal:latest -f Dockerfile-s2i-oc-tekton-operator . docker run -d --restart always --name terminal -p 7681 :7681 -v $HOME /dev/tmp:/root/dev web-terminal docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 85edc0b0ec27 web-terminal \"ttyd -p 7681 bash\" 17 minutes ago Up 17 minutes 0 .0.0.0:7681->7681/tcp terminal The volume mapping will write all files under the working directory to the host directory $HOME/dev/tmp . So suppose my host's user home directory is /Users/remkohdev@us.ibm.com/ . If I open the terminal in the browser, the working directory for the user is /root . Any file that is created under /root is created on the host's directory $HOME/dev/tmp . Similarly if I create a file in $HOME/dev/tmp it is available in the container's /root directory. Open the web-terminal in a browser and go to http://0.0.0.0:7681 . If Go, Operator SD export CLUSTERNAME = remkohdev-roks-labs-3n-cluster ibmcloud login Go to the OpenShift web console Copy Login command oc login --token = _12AbcD345kIPDIRg2jYpCuZ-g5SM5Im9irY2tol4Q8 --server = https://c100-e.us-south.containers.cloud.ibm.com:30712","title":"Access the web-terminal"},{"location":"generatedContent/kubernetes-storage/","text":"Introduction to Kubernetes Storage \u00b6 Objectives \u00b6 Overview \u00b6 The introductory page of the workshop is broken down into the following sections: Lecture: Update K8s storage lecture. Lab 1: Container storage and Kubernetes Lab 2 File storage with kubernetes Lab 3: Block storage with kubernetes","title":"Kubernetes Storage"},{"location":"generatedContent/kubernetes-storage/#introduction-to-kubernetes-storage","text":"","title":"Introduction to Kubernetes Storage"},{"location":"generatedContent/kubernetes-storage/#objectives","text":"","title":"Objectives"},{"location":"generatedContent/kubernetes-storage/#overview","text":"The introductory page of the workshop is broken down into the following sections: Lecture: Update K8s storage lecture. Lab 1: Container storage and Kubernetes Lab 2 File storage with kubernetes Lab 3: Block storage with kubernetes","title":"Overview"},{"location":"generatedContent/kubernetes-storage/Lab0/","text":"Lab 0: Pre-work \u00b6 1. Setup Kubernetes environment \u00b6 Run through the instructions listed here 2. Cloud shell login \u00b6 3. Docker hub account \u00b6 Create a dockerhub user and set the environment variable. DOCKERUSER = <dockerhub useid> 4. Set the cluster name \u00b6 ibmcloud ks clusters OK Name ID State Created Workers Location Version Resource Group Name Provider user001-workshop bseqlkkd0o1gdqg4jc10 normal 3 months ago 5 Dallas 4 .3.38_1544_openshift default classic CLUSTERNAME = use001-workshop or CLUSTERNAME = ` ibmcloud ks clusters | grep Name -A 1 | awk '{print $1}' | grep -v Name ` echo $CLUSTERNAME user001-workshop","title":"Lab 0. Prework"},{"location":"generatedContent/kubernetes-storage/Lab0/#lab-0-pre-work","text":"","title":"Lab 0: Pre-work"},{"location":"generatedContent/kubernetes-storage/Lab0/#1-setup-kubernetes-environment","text":"Run through the instructions listed here","title":"1. Setup Kubernetes environment"},{"location":"generatedContent/kubernetes-storage/Lab0/#2-cloud-shell-login","text":"","title":"2. Cloud shell login"},{"location":"generatedContent/kubernetes-storage/Lab0/#3-docker-hub-account","text":"Create a dockerhub user and set the environment variable. DOCKERUSER = <dockerhub useid>","title":"3. Docker hub account"},{"location":"generatedContent/kubernetes-storage/Lab0/#4-set-the-cluster-name","text":"ibmcloud ks clusters OK Name ID State Created Workers Location Version Resource Group Name Provider user001-workshop bseqlkkd0o1gdqg4jc10 normal 3 months ago 5 Dallas 4 .3.38_1544_openshift default classic CLUSTERNAME = use001-workshop or CLUSTERNAME = ` ibmcloud ks clusters | grep Name -A 1 | awk '{print $1}' | grep -v Name ` echo $CLUSTERNAME user001-workshop","title":"4. Set the cluster name"},{"location":"generatedContent/kubernetes-storage/Lab1/","text":"Lab 1: Non-persistent storage with Kubernetes \u00b6 Storing data in containers or worker nodes are considered as the non-persistent forms of data storage. In this lab, we will explore storage options on the IBM Kubernetes worker nodes. Follow this lab is you are interested in learning more about container-based storage. The lab covers the following topics: Create and claim IBM Kubernetes non-persistent storage based on the primary and secondary storage available on the worker nodes. Make the volumes available in the Guestbook application. Use the volumes to store application cache and debug information. Access the data from the guestbook container using the Kubernetes CLI. Assess the impact of losing a pod on data retention. Claim back the storage resources and clean up. The primary storage maps to the volume type hostPath and the secondary storage maps to emptyDir . Learn more about Kubernetes volume types here . Reserve Persistent Volumes \u00b6 From the cloud shell prompt, run the following commands to get the guestbook application and the kubernetes configuration needed for the storage labs. cd $HOME git clone --branch fs https://github.com/IBM/guestbook-nodejs.git git clone --branch storage https://github.com/rojanjose/guestbook-config.git cd $HOME /guestbook-config/storage/lab1 Let's start with reserving the Persistent volume from the primary storage. Review the yaml file pv-hostpath.yaml . Note the values set for type , storageClassName and hostPath . apiVersion : v1 kind : PersistentVolume metadata : name : guestbook-primary-pv labels : type : local spec : storageClassName : manual capacity : storage : 10Gi accessModes : - ReadWriteOnce hostPath : path : \"/mnt/data\" Create the persistent volume as shown in the command below: kubectl create -f pv-hostpath.yaml persistentvolume/guestbook-primary-pv created kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE guestbook-primary-pv 10Gi RWO Retain Available manual 13s Next PVC yaml: apiVersion : v1 kind : PersistentVolumeClaim metadata : name : guestbook-local-pvc spec : storageClassName : manual accessModes : - ReadWriteMany resources : requests : storage : 3Gi Create PVC: kubectl create -f pvc-hostpath.yaml persistentvolumeclaim/guestbook-local-pvc created \u276f kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE guestbook-local-pvc Bound guestbook-local-pv 10Gi RWX manual 6s Guestbook application using storage \u00b6 The application is the Guestbook App , which is a simple multi-tier web application built using the loopback framework. Change to the guestbook application source directory: cd $HOME /guestbook-nodejs/src Review the source common/models/entry.js . The application uses storage allocated using hostPath to store data cache in the file data/cache.txt . The file logs/debug.txt records debug messages provisioned using the emptyDir storage type. module . exports = function ( Entry ) { Entry . greet = function ( msg , cb ) { // console.log(\"testing \" + msg); fs . appendFile ( 'logs/debug.txt' , \"Received message: \" + msg + \"\\n\" , function ( err ) { if ( err ) throw err ; console . log ( 'Debug stagement printed' ); }); fs . appendFile ( 'data/cache.txt' , msg + \"\\n\" , function ( err ) { if ( err ) throw err ; console . log ( 'Saved in cache!' ); }); ... Run the commands listed below to build the guestbook image and copy into the docker hub registry: docker build -t $DOCKERUSER /guestbook-nodejs:storage . docker login -u $DOCKERUSER docker push $DOCKERUSER /guestbook-nodejs:storage Review the deployment yaml file guestbook-deplopyment.yaml prior to deploying the application into the cluster. cd $HOME /guestbook-config/storage/lab1 cat guestbook-deployment.yaml Replace the first part of image name with your docker hub user id. The section spec.volumes lists hostPath and emptyDir volumes. The section spec.containers.volumeMounts lists the mount paths that the application uses to write in the volumes. apiVersion : apps/v1 kind : Deployment metadata : name : guestbook-v1 labels : app : guestbook ... spec : containers : - name : guestbook image : rojanjose/guestbook-nodejs:storage imagePullPolicy : Always ports : - name : http-server containerPort : 3000 volumeMounts : - name : guestbook-primary-volume mountPath : /home/node/app/data - name : guestbook-secondary-volume mountPath : /home/node/app/logs volumes : - name : guestbook-primary-volume persistentVolumeClaim : claimName : guestbook-primary-pvc - name : guestbook-secondary-volume emptyDir : {} ... Deploy the Guestbook application: kubectl create -f guestbook-deployment.yaml deployment.apps/guestbook-v1 created kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-6f55cb54c5-jb89d 1 /1 Running 0 14s kubectl create -f guestbook-service.yaml service/guestbook created Find the URL for the guestbook application by joining the worker node external IP and service node port. HOSTNAME = ` kubectl get nodes -o wide | tail -n 1 | awk '{print $7}' ` SERVICEPORT = ` kubectl get svc guestbook -o = jsonpath = '{.spec.ports[0].nodePort}' ` echo \"http:// $HOSTNAME : $SERVICEPORT \" Open the URL in a browser and create guest book entries. Next, inspect the data. To do this, run a bash process inside the application container using kubectl exec . Reference the pod name from the previous kubectl get pods command. Once inside the container, use the subsequent comands to inspect the data. kubectl exec -it [ POD NAME ] -- bash root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# ls -al total 256 drwxr-xr-x 1 root root 4096 Nov 11 23 :40 . drwxr-xr-x 1 node node 4096 Nov 11 23 :20 .. -rw-r--r-- 1 root root 12 Oct 29 21 :00 .dockerignore -rw-r--r-- 1 root root 288 Oct 29 21 :00 .editorconfig -rw-r--r-- 1 root root 8 Oct 29 21 :00 .eslintignore -rw-r--r-- 1 root root 27 Oct 29 21 :00 .eslintrc -rw-r--r-- 1 root root 151 Oct 29 21 :00 .gitignore -rw-r--r-- 1 root root 30 Oct 29 21 :00 .yo-rc.json -rw-r--r-- 1 root root 105 Oct 29 21 :00 Dockerfile drwxr-xr-x 2 root root 4096 Nov 11 03 :40 client drwxr-xr-x 3 root root 4096 Nov 10 23 :04 common drwxr-xr-x 2 root root 4096 Nov 11 23 :16 data drwxrwxrwx 2 root root 4096 Nov 11 23 :44 logs drwxr-xr-x 439 root root 16384 Nov 11 23 :20 node_modules -rw-r--r-- 1 root root 176643 Nov 11 23 :20 package-lock.json -rw-r--r-- 1 root root 830 Nov 11 23 :20 package.json drwxr-xr-x 3 root root 4096 Nov 10 23 :04 server root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# cat data/cache.txt Hello Kubernetes! Hola Kubernetes! Zdravstvuyte Kubernetes! N\u01d0n h\u01ceo Kubernetes! Goedendag Kubernetes! root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# cat logs/debug.txt Received message: Hello Kubernetes! Received message: Hola Kubernetes! Received message: Zdravstvuyte Kubernetes! Received message: N\u01d0n h\u01ceo Kubernetes! Received message: Goedendag Kubernetes! root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# df -h Filesystem Size Used Avail Use% Mounted on overlay 98G 3 .5G 90G 4 % / tmpfs 64M 0 64M 0 % /dev tmpfs 7 .9G 0 7 .9G 0 % /sys/fs/cgroup /dev/mapper/docker_data 98G 3 .5G 90G 4 % /etc/hosts shm 64M 0 64M 0 % /dev/shm /dev/xvda2 25G 3 .6G 20G 16 % /home/node/app/data tmpfs 7 .9G 16K 7 .9G 1 % /run/secrets/kubernetes.io/serviceaccount tmpfs 7 .9G 0 7 .9G 0 % /proc/acpi tmpfs 7 .9G 0 7 .9G 0 % /proc/scsi tmpfs 7 .9G 0 7 .9G 0 % /sys/firmware While still inside the container, create a file on the container file system. This file will not persist when we kill the container. Then run /sbin/killall5 to terminate the container. root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# touch dontdeletemeplease root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# ls dontdeletemeplease dontdeletemeplease root@guestbook-v1-66798779d6-fqh2j:/home/node/app# /sbin/killall5 root@guestbook-v1-66798779d6-fqh2j:/home/node/app# command terminated with exit code 137 The killall5 command will kick you out of the container (which is no longer running), but the pod is still running. Verify this with kubectl get pods . Not the 0/1 status indicating the application container is no longer running. kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-66798779d6-fqh2j 0 /1 CrashLoopBackOff 2 16m After a few seconds, the Pod will restart the container: kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-66798779d6-fqh2j 1 /1 Running 3 16m Run a bash process inside the container to inspect your data again: kubectl exec -it [ POD NAME ] -- bash root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# cat data/cache.txt Hello Kubernetes! Hola Kubernetes! Zdravstvuyte Kubernetes! N\u01d0n h\u01ceo Kubernetes! Goedendag Kubernetes! root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# cat logs/debug.txt Received message: Hello Kubernetes! Received message: Hola Kubernetes! Received message: Zdravstvuyte Kubernetes! Received message: N\u01d0n h\u01ceo Kubernetes! Received message: Goedendag Kubernetes! root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# ls dontdeletemeplease ls: dontdeletemeplease: No such file or directory Notice how the storage from the primary ( hostPath ) and secondary ( emptyDir ) storage types persisted beyond the lifecycle of the container, but the dontdeletemeplease file, did not. Next, we'll kill the pod to see the impact of deleting the pod on data. kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-6f55cb54c5-jb89d 1 /1 Running 0 12m kubectl delete pod guestbook-v1-6f55cb54c5-jb89d pod \"guestbook-v1-6f55cb54c5-jb89d\" deleted kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-5cbc445dc9-sx58j 1 /1 Running 0 86s Enter new data: Log into the pod to view the state of the data. kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-5cbc445dc9-sx58j 1 /1 Running 0 86s kubectl exec -it guestbook-v1-5cbc445dc9-sx58j bash kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [ POD ] -- [ COMMAND ] instead. root@guestbook-v1-5cbc445dc9-sx58j:/home/node/app# cat data/cache.txt Hello Kubernetes! Hola Kubernetes! Zdravstvuyte Kubernetes! N\u01d0n h\u01ceo Kubernetes! Goedendag Kubernetes! Bye Kubernetes! Aloha Kubernetes! Ciao Kubernetes! Sayonara Kubernetes! root@guestbook-v1-5cbc445dc9-sx58j:/home/node/app# cat logs/debug.txt Received message: Bye Kubernetes! Received message: Aloha Kubernetes! Received message: Ciao Kubernetes! Received message: Sayonara Kubernetes! root@guestbook-v1-5cbc445dc9-sx58j:/home/node/app# This shows that the storage type emptyDir loose data on a pod restart whereas hostPath data lives until the worker node or cluster is deleted. Storage Type Persisted at which level Example Uses Container local storage Container ephermal state Secondary Storage ( EmptyDir ) Pod Checkpoint a long computation process Primary Storage ( HostPath ) Node Running cAdvisor in a container Normally Kubernetes clusters have multiple worker nodes in a cluster with replicas for a single application running across different worker nodes. In this case, only applications running on the same worker node will share data persisted with IKS Primary Storage (HostPath). More suitable solutions are available for cross worker nodes, cross availability-zone and cross-region storage. Clean up \u00b6 cd $HOME /guestbook-config/storage/lab1 kubectl delete -f .","title":"Lab 1. Container storage and Kubernetes"},{"location":"generatedContent/kubernetes-storage/Lab1/#lab-1-non-persistent-storage-with-kubernetes","text":"Storing data in containers or worker nodes are considered as the non-persistent forms of data storage. In this lab, we will explore storage options on the IBM Kubernetes worker nodes. Follow this lab is you are interested in learning more about container-based storage. The lab covers the following topics: Create and claim IBM Kubernetes non-persistent storage based on the primary and secondary storage available on the worker nodes. Make the volumes available in the Guestbook application. Use the volumes to store application cache and debug information. Access the data from the guestbook container using the Kubernetes CLI. Assess the impact of losing a pod on data retention. Claim back the storage resources and clean up. The primary storage maps to the volume type hostPath and the secondary storage maps to emptyDir . Learn more about Kubernetes volume types here .","title":"Lab 1: Non-persistent storage with Kubernetes"},{"location":"generatedContent/kubernetes-storage/Lab1/#reserve-persistent-volumes","text":"From the cloud shell prompt, run the following commands to get the guestbook application and the kubernetes configuration needed for the storage labs. cd $HOME git clone --branch fs https://github.com/IBM/guestbook-nodejs.git git clone --branch storage https://github.com/rojanjose/guestbook-config.git cd $HOME /guestbook-config/storage/lab1 Let's start with reserving the Persistent volume from the primary storage. Review the yaml file pv-hostpath.yaml . Note the values set for type , storageClassName and hostPath . apiVersion : v1 kind : PersistentVolume metadata : name : guestbook-primary-pv labels : type : local spec : storageClassName : manual capacity : storage : 10Gi accessModes : - ReadWriteOnce hostPath : path : \"/mnt/data\" Create the persistent volume as shown in the command below: kubectl create -f pv-hostpath.yaml persistentvolume/guestbook-primary-pv created kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE guestbook-primary-pv 10Gi RWO Retain Available manual 13s Next PVC yaml: apiVersion : v1 kind : PersistentVolumeClaim metadata : name : guestbook-local-pvc spec : storageClassName : manual accessModes : - ReadWriteMany resources : requests : storage : 3Gi Create PVC: kubectl create -f pvc-hostpath.yaml persistentvolumeclaim/guestbook-local-pvc created \u276f kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE guestbook-local-pvc Bound guestbook-local-pv 10Gi RWX manual 6s","title":"Reserve Persistent Volumes"},{"location":"generatedContent/kubernetes-storage/Lab1/#guestbook-application-using-storage","text":"The application is the Guestbook App , which is a simple multi-tier web application built using the loopback framework. Change to the guestbook application source directory: cd $HOME /guestbook-nodejs/src Review the source common/models/entry.js . The application uses storage allocated using hostPath to store data cache in the file data/cache.txt . The file logs/debug.txt records debug messages provisioned using the emptyDir storage type. module . exports = function ( Entry ) { Entry . greet = function ( msg , cb ) { // console.log(\"testing \" + msg); fs . appendFile ( 'logs/debug.txt' , \"Received message: \" + msg + \"\\n\" , function ( err ) { if ( err ) throw err ; console . log ( 'Debug stagement printed' ); }); fs . appendFile ( 'data/cache.txt' , msg + \"\\n\" , function ( err ) { if ( err ) throw err ; console . log ( 'Saved in cache!' ); }); ... Run the commands listed below to build the guestbook image and copy into the docker hub registry: docker build -t $DOCKERUSER /guestbook-nodejs:storage . docker login -u $DOCKERUSER docker push $DOCKERUSER /guestbook-nodejs:storage Review the deployment yaml file guestbook-deplopyment.yaml prior to deploying the application into the cluster. cd $HOME /guestbook-config/storage/lab1 cat guestbook-deployment.yaml Replace the first part of image name with your docker hub user id. The section spec.volumes lists hostPath and emptyDir volumes. The section spec.containers.volumeMounts lists the mount paths that the application uses to write in the volumes. apiVersion : apps/v1 kind : Deployment metadata : name : guestbook-v1 labels : app : guestbook ... spec : containers : - name : guestbook image : rojanjose/guestbook-nodejs:storage imagePullPolicy : Always ports : - name : http-server containerPort : 3000 volumeMounts : - name : guestbook-primary-volume mountPath : /home/node/app/data - name : guestbook-secondary-volume mountPath : /home/node/app/logs volumes : - name : guestbook-primary-volume persistentVolumeClaim : claimName : guestbook-primary-pvc - name : guestbook-secondary-volume emptyDir : {} ... Deploy the Guestbook application: kubectl create -f guestbook-deployment.yaml deployment.apps/guestbook-v1 created kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-6f55cb54c5-jb89d 1 /1 Running 0 14s kubectl create -f guestbook-service.yaml service/guestbook created Find the URL for the guestbook application by joining the worker node external IP and service node port. HOSTNAME = ` kubectl get nodes -o wide | tail -n 1 | awk '{print $7}' ` SERVICEPORT = ` kubectl get svc guestbook -o = jsonpath = '{.spec.ports[0].nodePort}' ` echo \"http:// $HOSTNAME : $SERVICEPORT \" Open the URL in a browser and create guest book entries. Next, inspect the data. To do this, run a bash process inside the application container using kubectl exec . Reference the pod name from the previous kubectl get pods command. Once inside the container, use the subsequent comands to inspect the data. kubectl exec -it [ POD NAME ] -- bash root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# ls -al total 256 drwxr-xr-x 1 root root 4096 Nov 11 23 :40 . drwxr-xr-x 1 node node 4096 Nov 11 23 :20 .. -rw-r--r-- 1 root root 12 Oct 29 21 :00 .dockerignore -rw-r--r-- 1 root root 288 Oct 29 21 :00 .editorconfig -rw-r--r-- 1 root root 8 Oct 29 21 :00 .eslintignore -rw-r--r-- 1 root root 27 Oct 29 21 :00 .eslintrc -rw-r--r-- 1 root root 151 Oct 29 21 :00 .gitignore -rw-r--r-- 1 root root 30 Oct 29 21 :00 .yo-rc.json -rw-r--r-- 1 root root 105 Oct 29 21 :00 Dockerfile drwxr-xr-x 2 root root 4096 Nov 11 03 :40 client drwxr-xr-x 3 root root 4096 Nov 10 23 :04 common drwxr-xr-x 2 root root 4096 Nov 11 23 :16 data drwxrwxrwx 2 root root 4096 Nov 11 23 :44 logs drwxr-xr-x 439 root root 16384 Nov 11 23 :20 node_modules -rw-r--r-- 1 root root 176643 Nov 11 23 :20 package-lock.json -rw-r--r-- 1 root root 830 Nov 11 23 :20 package.json drwxr-xr-x 3 root root 4096 Nov 10 23 :04 server root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# cat data/cache.txt Hello Kubernetes! Hola Kubernetes! Zdravstvuyte Kubernetes! N\u01d0n h\u01ceo Kubernetes! Goedendag Kubernetes! root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# cat logs/debug.txt Received message: Hello Kubernetes! Received message: Hola Kubernetes! Received message: Zdravstvuyte Kubernetes! Received message: N\u01d0n h\u01ceo Kubernetes! Received message: Goedendag Kubernetes! root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# df -h Filesystem Size Used Avail Use% Mounted on overlay 98G 3 .5G 90G 4 % / tmpfs 64M 0 64M 0 % /dev tmpfs 7 .9G 0 7 .9G 0 % /sys/fs/cgroup /dev/mapper/docker_data 98G 3 .5G 90G 4 % /etc/hosts shm 64M 0 64M 0 % /dev/shm /dev/xvda2 25G 3 .6G 20G 16 % /home/node/app/data tmpfs 7 .9G 16K 7 .9G 1 % /run/secrets/kubernetes.io/serviceaccount tmpfs 7 .9G 0 7 .9G 0 % /proc/acpi tmpfs 7 .9G 0 7 .9G 0 % /proc/scsi tmpfs 7 .9G 0 7 .9G 0 % /sys/firmware While still inside the container, create a file on the container file system. This file will not persist when we kill the container. Then run /sbin/killall5 to terminate the container. root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# touch dontdeletemeplease root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# ls dontdeletemeplease dontdeletemeplease root@guestbook-v1-66798779d6-fqh2j:/home/node/app# /sbin/killall5 root@guestbook-v1-66798779d6-fqh2j:/home/node/app# command terminated with exit code 137 The killall5 command will kick you out of the container (which is no longer running), but the pod is still running. Verify this with kubectl get pods . Not the 0/1 status indicating the application container is no longer running. kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-66798779d6-fqh2j 0 /1 CrashLoopBackOff 2 16m After a few seconds, the Pod will restart the container: kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-66798779d6-fqh2j 1 /1 Running 3 16m Run a bash process inside the container to inspect your data again: kubectl exec -it [ POD NAME ] -- bash root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# cat data/cache.txt Hello Kubernetes! Hola Kubernetes! Zdravstvuyte Kubernetes! N\u01d0n h\u01ceo Kubernetes! Goedendag Kubernetes! root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# cat logs/debug.txt Received message: Hello Kubernetes! Received message: Hola Kubernetes! Received message: Zdravstvuyte Kubernetes! Received message: N\u01d0n h\u01ceo Kubernetes! Received message: Goedendag Kubernetes! root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# ls dontdeletemeplease ls: dontdeletemeplease: No such file or directory Notice how the storage from the primary ( hostPath ) and secondary ( emptyDir ) storage types persisted beyond the lifecycle of the container, but the dontdeletemeplease file, did not. Next, we'll kill the pod to see the impact of deleting the pod on data. kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-6f55cb54c5-jb89d 1 /1 Running 0 12m kubectl delete pod guestbook-v1-6f55cb54c5-jb89d pod \"guestbook-v1-6f55cb54c5-jb89d\" deleted kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-5cbc445dc9-sx58j 1 /1 Running 0 86s Enter new data: Log into the pod to view the state of the data. kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-5cbc445dc9-sx58j 1 /1 Running 0 86s kubectl exec -it guestbook-v1-5cbc445dc9-sx58j bash kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [ POD ] -- [ COMMAND ] instead. root@guestbook-v1-5cbc445dc9-sx58j:/home/node/app# cat data/cache.txt Hello Kubernetes! Hola Kubernetes! Zdravstvuyte Kubernetes! N\u01d0n h\u01ceo Kubernetes! Goedendag Kubernetes! Bye Kubernetes! Aloha Kubernetes! Ciao Kubernetes! Sayonara Kubernetes! root@guestbook-v1-5cbc445dc9-sx58j:/home/node/app# cat logs/debug.txt Received message: Bye Kubernetes! Received message: Aloha Kubernetes! Received message: Ciao Kubernetes! Received message: Sayonara Kubernetes! root@guestbook-v1-5cbc445dc9-sx58j:/home/node/app# This shows that the storage type emptyDir loose data on a pod restart whereas hostPath data lives until the worker node or cluster is deleted. Storage Type Persisted at which level Example Uses Container local storage Container ephermal state Secondary Storage ( EmptyDir ) Pod Checkpoint a long computation process Primary Storage ( HostPath ) Node Running cAdvisor in a container Normally Kubernetes clusters have multiple worker nodes in a cluster with replicas for a single application running across different worker nodes. In this case, only applications running on the same worker node will share data persisted with IKS Primary Storage (HostPath). More suitable solutions are available for cross worker nodes, cross availability-zone and cross-region storage.","title":"Guestbook application using storage"},{"location":"generatedContent/kubernetes-storage/Lab1/#clean-up","text":"cd $HOME /guestbook-config/storage/lab1 kubectl delete -f .","title":"Clean up"},{"location":"generatedContent/kubernetes-storage/Lab2/","text":"Lab 2: File storage with Kubernetes \u00b6 This lab demonstrates the use of cloud based file storage with Kubernetes. It uses the IBM Cloud File Storage which is persistent, fast, and flexible network-attached, NFS-based File Storage capacity ranging from 25 GB to 12,000 GB capacity with up to 48,000 IOPS. The IBM Cloud File Storage provides data across all worker nodes within a single availability zone. Following topics are covered in this exercise: Claim a classic file storage volume. Make the volumes available in the Guestbook application. Copy media files such as images into the volume using the Kubernetes CLI. Use the Guestbook application to view the images. Claim back the storage resources and clean up. Prereqs \u00b6 Follow the prereqs if you haven't already. Claim file storage volume \u00b6 Review the storage classes for file storage. In addition to the standard set of storage classes, custom storage classes can be defined to meet the storage requirements. kubectl get storageclasses Expected output: $ kubectl get storageclasses default ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-bronze ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-bronze-gid ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-custom ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-gold ( default ) ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-gold-gid ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-retain-bronze ibm.io/ibmc-file Retain Immediate false 27m ibmc-file-retain-custom ibm.io/ibmc-file Retain Immediate false 27m ibmc-file-retain-gold ibm.io/ibmc-file Retain Immediate false 27m ibmc-file-retain-silver ibm.io/ibmc-file Retain Immediate false 27m ibmc-file-silver ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-silver-gid ibm.io/ibmc-file Delete Immediate false 27m IKS comes with storage class definitions for file storage. This lab uses the storage class ibm-file-silver . Note that the default class is ibmc-file-gold is allocated if storgage class is not expliciity definded. kubectl describe storageclass ibmc-file-silver Expected output: $ kubectl describe storageclass ibmc-file-silver Name: ibmc-file-silver IsDefaultClass: No Annotations: kubectl.kubernetes.io/last-applied-configuration ={ \"apiVersion\" : \"storage.k8s.io/v1\" , \"kind\" : \"StorageClass\" , \"metadata\" : { \"annotations\" : {} , \"labels\" : { \"kubernetes.io/cluster-service\" : \"true\" } , \"name\" : \"ibmc-file-silver\" } , \"parameters\" : { \"billingType\" : \"hourly\" , \"classVersion\" : \"2\" , \"iopsPerGB\" : \"4\" , \"sizeRange\" : \"[20-12000]Gi\" , \"type\" : \"Endurance\" } , \"provisioner\" : \"ibm.io/ibmc-file\" , \"reclaimPolicy\" : \"Delete\" } Provisioner: ibm.io/ibmc-file Parameters: billingType = hourly,classVersion = 2 ,iopsPerGB = 4 ,sizeRange =[ 20 -12000 ] Gi,type = Endurance AllowVolumeExpansion: <unset> MountOptions: <none> ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: <none> File sliver has an IOPS of 4GB and a max capacity of 12TB. Claim a file storage volume \u00b6 IBM Cloud File Storage provides fast access to your data for a cluster running in a single available zone. For higher availability, use a storage option that is designed for geographically distributed data . Review the yaml for the file storage PersistentVolumeClaim . When we create this PersistentVolumeClaim , it automatically creates it within an availability zone where the worker nodes are located. cd guestbook-config/storage/lab2 cat pvc-file-silver.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: guestbook-pvc labels: billingType: hourly spec: accessModes: - ReadWriteMany resources: requests: storage: 20Gi storageClassName: ibmc-file-silver Create the PVC kubectl apply -f pvc-file-silver.yaml Expected output: $ kubectl create -f pvc-file-silver.yaml persistentvolumeclaim/guestbook-filesilver-pvc created Verify the PVC claim is created with the status Bound . This may take a minute or two. kubectl get pvc guestbook-filesilver-pvc Expected output: $ kubectl get pvc guestbook-filesilver-pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE guestbook-filesilver-pvc Bound pvc-a7cb12ed-b52b-4342-966a-eceaf24e42a9 20Gi RWX ibmc-file-silver 2m Details associated with the pv . Use the pv name from the previous command output. kubectl get pv [ pv name ] Expected output: $ kubectl get pv pvc-a7cb12ed-b52b-4342-966a-eceaf24e42a9 NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-a7cb12ed-b52b-4342-966a-eceaf24e42a9 20Gi RWX Delete Bound default/guestbook-filesilver-pvc ibmc-file-silver 90s Use the volume in the Guestbook application \u00b6 Change to the guestbook application source directory and review the html files images.html and index.html . images.html has the code to display the images stored in the file storage. cd $HOME /guestbook-nodejs/src cat client/images.html cat client/index.html Run the commands listed below to build the guestbook image and copy into the docker hub registry: (Skip this step if you have already completed lab 1.) cd $HOME /guestbook-nodejs/src docker build -t $DOCKERUSER /guestbook-nodejs:storage . docker login -u $DOCKERUSER docker push $DOCKERUSER /guestbook-nodejs:storage Review the deployment yaml file guestbook-deplopyment.yaml prior to deploying the application into the cluster. cd $HOME /guestbook-config/storage/lab2 cat guestbook-deployment.yaml Replace the first part of the image name with your docker hub user id. The section spec.volumes references the file volume PVC. The section spec.containers.volumeMounts has the mount path to store images in the volumes. apiVersion : apps/v1 kind : Deployment metadata : name : guestbook-v1 ... spec : containers : - name : guestbook image : rojanjose/guestbook-nodejs:storage imagePullPolicy : Always ports : - name : http-server containerPort : 3000 volumeMounts : - name : guestbook-file-volume mountPath : /app/public/images volumes : - name : guestbook-file-volume persistentVolumeClaim : claimName : guestbook-filesilver-pvc Deploy the Guestbook application. kubectl create -f guestbook-deployment.yaml kubectl create -f guestbook-service.yaml Verify the Guestbook application is runing. kubectl get all Expected output: $ kubectl get all NAME READY STATUS RESTARTS AGE pod/guestbook-v1-5bd76b568f-cdhr5 1 /1 Running 0 13s pod/guestbook-v1-5bd76b568f-w6h6h 1 /1 Running 0 13s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/guestbook LoadBalancer 172 .21.238.40 150 .238.30.150 3000 :31986/TCP 6s service/kubernetes ClusterIP 172 .21.0.1 <none> 443 /TCP 9d NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-v1 2 /2 2 2 13s NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-v1-5bd76b568f 2 2 2 13s Check the mount path inside the pod container. Get the pod listing. $ kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-5bd76b568f-cdhr5 1 /1 Running 0 78s guestbook-v1-5bd76b568f-w6h6h 1 /1 Running 0 78s Set these variables for each of your pod names: export POD1 =[ FIRST POD NAME ] export POD2 =[ SECOND POD NAME ] Log into any one of the pod. Use one of the pod names from the previous command output. kubectl exec -it $POD1 -- bash Run the commands ls -al; ls -al images; df -ah to view the volume and files. Review the mount for the new volume. Note that the images folder is empty. $ kubectl exec -it $POD1 -- bash root@guestbook-v1-5bd76b568f-cdhr5:/home/node/app# ls -alt total 252 drwxr-xr-x 1 root root 4096 Nov 13 03 :17 client drwxr-xr-x 1 root root 4096 Nov 13 03 :15 . drwxr-xr-x 439 root root 16384 Nov 13 03 :15 node_modules -rw-r--r-- 1 root root 176643 Nov 13 03 :15 package-lock.json drwxr-xr-x 1 node node 4096 Nov 13 03 :15 .. -rw-r--r-- 1 root root 830 Nov 11 23 :20 package.json drwxr-xr-x 3 root root 4096 Nov 10 23 :04 common drwxr-xr-x 3 root root 4096 Nov 10 23 :04 server -rw-r--r-- 1 root root 12 Oct 29 21 :00 .dockerignore -rw-r--r-- 1 root root 288 Oct 29 21 :00 .editorconfig -rw-r--r-- 1 root root 8 Oct 29 21 :00 .eslintignore -rw-r--r-- 1 root root 27 Oct 29 21 :00 .eslintrc -rw-r--r-- 1 root root 151 Oct 29 21 :00 .gitignore -rw-r--r-- 1 root root 30 Oct 29 21 :00 .yo-rc.json -rw-r--r-- 1 root root 105 Oct 29 21 :00 Dockerfile root@guestbook-v1-5bd76b568f-cdhr5:/home/node/app# ls -alt client/images total 8 drwxr-xr-x 1 root root 4096 Nov 13 03 :17 .. drwxr-xr-x 2 nobody 4294967294 4096 Nov 13 02 :02 . root@guestbook-v1-5bd76b568f-cdhr5:/home/node/app# df -h Filesystem Size Used Avail Use% Mounted on overlay 98G 4 .9G 89G 6 % / tmpfs 64M 0 64M 0 % /dev tmpfs 7 .9G 0 7 .9G 0 % /sys/fs/cgroup shm 64M 0 64M 0 % /dev/shm /dev/mapper/docker_data 98G 4 .9G 89G 6 % /etc/hosts tmpfs 7 .9G 16K 7 .9G 1 % /run/secrets/kubernetes.io/serviceaccount fsf-dal1003d-fz.adn.networklayer.com:/IBM02SEV2058850_2177/data01 20G 0 20G 0 % /home/node/app/client/images tmpfs 7 .9G 0 7 .9G 0 % /proc/acpi tmpfs 7 .9G 0 7 .9G 0 % /proc/scsi tmpfs 7 .9G 0 7 .9G 0 % /sys/firmware root@guestbook-v1-5bd76b568f-cdhr5:/home/node/app# exit Note the filesystem fsf-dal1003d-fz.adn.networklayer.com:/IBM02SEV2058850_2177/data01 is mounted on path /home/node/app/client/images . Find the URL for the guestbook application by joining the worker node external IP and service node port. HOSTNAME = ` kubectl get nodes -ojsonpath = '{.items[0].metadata.labels.ibm-cloud\\.kubernetes\\.io\\/external-ip}' ` SERVICEPORT = ` kubectl get svc guestbook -o = jsonpath = '{.spec.ports[0].nodePort}' ` echo \"http:// $HOSTNAME : $SERVICEPORT \" Verify that the images are missing by viewing the data from the Guestbook application. Click on the hyperlink labled images at the bottom of the guestbook home page. The images.html page shows images with broken links. Load the file storage with images \u00b6 Run the kubectl cp command to move the images into the mounted volume. cd $HOME /guestbook-config/storage/lab2 kubectl cp images $POD1 :/home/node/app/client/ Refresh the page images.html page in the guestbook application to view the uploaded images. Shared storage across pods \u00b6 Login into the other pod $POD2 to verify the volume mount. kubectl exec -it $POD2 -- bash root@guestbook-v1-5bd76b568f-w6h6h:/home/node/app# ls -alt /home/node/app/client/images total 160 -rw-r--r-- 1 501 staff 56191 Nov 13 03 :44 gb3.jpg drwxr-xr-x 2 nobody 4294967294 4096 Nov 13 03 :44 . -rw-r--r-- 1 501 staff 21505 Nov 13 03 :44 gb2.jpg -rw-r--r-- 1 501 staff 58286 Nov 13 03 :44 gb1.jpg drwxr-xr-x 1 root root 4096 Nov 13 03 :17 .. root@guestbook-v1-5bd76b568f-w6h6h:/home/node/app# df -h Filesystem Size Used Avail Use% Mounted on overlay 98G 4 .2G 89G 5 % / tmpfs 64M 0 64M 0 % /dev tmpfs 7 .9G 0 7 .9G 0 % /sys/fs/cgroup shm 64M 0 64M 0 % /dev/shm /dev/mapper/docker_data 98G 4 .2G 89G 5 % /etc/hosts tmpfs 7 .9G 16K 7 .9G 1 % /run/secrets/kubernetes.io/serviceaccount fsf-dal1003d-fz.adn.networklayer.com:/IBM02SEV2058850_2177/data01 20G 128K 20G 1 % /home/node/app/client/images tmpfs 7 .9G 0 7 .9G 0 % /proc/acpi tmpfs 7 .9G 0 7 .9G 0 % /proc/scsi tmpfs 7 .9G 0 7 .9G 0 % /sys/firmware root@guestbook-v1-5bd76b568f-w6h6h:/home/node/app# exit Note that the volume and the data are available on all the pods running the Guestbook application. IBM Cloud File Storage is a NFS-based file storage that is available across all worker nodes within a single availability zone. If you are running a cluster with multiple nodes (within a single AZ) you can run the following commands to prove that your data is available across different nodes: kubectl get pods -o wide kubectl get nodes Expected output: $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES guestbook-v1-6fb8b86876-n9jtz 1 /1 Running 0 39h 172 .30.224.70 10 .38.216.205 <none> <none> guestbook-v1-6fb8b86876-njwcz 1 /1 Running 0 39h 172 .30.169.144 10 .38.216.238 <none> <none> $ kubectl get nodes NAME STATUS ROLES AGE VERSION 10 .38.216.205 Ready <none> 4d5h v1.18.10+IKS 10 .38.216.238 Ready <none> 4d5h v1.18.10+IKS To extend our table from Lab 1 we now have: Storage Type Persisted at which level Example Uses Container local storage Container ephermal state Secondary Storage ( EmptyDir ) Pod Checkpoint a long computation process Primary Storage ( HostPath ) Node Running cAdvisor in a container IBM Cloud File Storage (NFS) Availabilty Zone Applications running in a single availabilty zone Data is available to all nodes within the availability zone where the file storage exists, but the accessMode parameter on the PersistentVolumeClaim determines if multiple pods are able to mount a volume specificed by a PVC. The possible values for this parameter are: ReadWriteMany : The PVC can be mounted by multiple pods. All pods can read from and write to the volume. ReadOnlyMany : The PVC can be mounted by multiple pods. All pods have read-only access. ReadWriteOnce : The PVC can be mounted by one pod only. This pod can read from and write to the volume. [Optional exercises] \u00b6 Another way to see that the data is persisted at the availability zone level, you can: Back up data. Delete pods to confirm that it does not impact the data used by the application. Delete the Kubernetes cluster. Create a new cluster and reuse the volume. Clean up \u00b6 List all the PVCs and PVs kubectl get pvc kubectl get pv Delete all the pods using the PVC. Delete the PVC kubectl delete pvc guestbook-pvc persistentvolumeclaim \"guestbook-pvc\" deleted List PV to ensure that it is removed as well. Cancel the physical storage volume from the cloud account. (Note: requires admin permissions?) ibmcloud sl file volume-list --columns id --columns notes | grep pvc-6362f614-258e-48ee-a596-62bb4629cd75 183223942 { \"plugin\" : \"ibm-file-plugin-7b9db9c79f-86x8w\" , \"region\" : \"us-south\" , \"cluster\" : \"bugql3nd088jsp8iiagg\" , \"type\" : \"Endurance\" , \"ns\" : \"default\" , \"pvc\" : \"guestbook-pvc\" , \"pv\" : \"pvc-6362f614-258e-48ee-a596-62bb4629cd75\" , \"storageclass\" : \"ibmc-file-silver\" , \"reclaim\" : \"Delete\" } ibmcloud sl file volume-cancel 183223942 This will cancel the file volume: 183223942 and cannot be undone. Continue?> yes Failed to cancel file volume: 183223942 . No billing item is found to cancel.","title":"Lab 2. File storage with kubernetes"},{"location":"generatedContent/kubernetes-storage/Lab2/#lab-2-file-storage-with-kubernetes","text":"This lab demonstrates the use of cloud based file storage with Kubernetes. It uses the IBM Cloud File Storage which is persistent, fast, and flexible network-attached, NFS-based File Storage capacity ranging from 25 GB to 12,000 GB capacity with up to 48,000 IOPS. The IBM Cloud File Storage provides data across all worker nodes within a single availability zone. Following topics are covered in this exercise: Claim a classic file storage volume. Make the volumes available in the Guestbook application. Copy media files such as images into the volume using the Kubernetes CLI. Use the Guestbook application to view the images. Claim back the storage resources and clean up.","title":"Lab 2: File storage with Kubernetes"},{"location":"generatedContent/kubernetes-storage/Lab2/#prereqs","text":"Follow the prereqs if you haven't already.","title":"Prereqs"},{"location":"generatedContent/kubernetes-storage/Lab2/#claim-file-storage-volume","text":"Review the storage classes for file storage. In addition to the standard set of storage classes, custom storage classes can be defined to meet the storage requirements. kubectl get storageclasses Expected output: $ kubectl get storageclasses default ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-bronze ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-bronze-gid ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-custom ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-gold ( default ) ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-gold-gid ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-retain-bronze ibm.io/ibmc-file Retain Immediate false 27m ibmc-file-retain-custom ibm.io/ibmc-file Retain Immediate false 27m ibmc-file-retain-gold ibm.io/ibmc-file Retain Immediate false 27m ibmc-file-retain-silver ibm.io/ibmc-file Retain Immediate false 27m ibmc-file-silver ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-silver-gid ibm.io/ibmc-file Delete Immediate false 27m IKS comes with storage class definitions for file storage. This lab uses the storage class ibm-file-silver . Note that the default class is ibmc-file-gold is allocated if storgage class is not expliciity definded. kubectl describe storageclass ibmc-file-silver Expected output: $ kubectl describe storageclass ibmc-file-silver Name: ibmc-file-silver IsDefaultClass: No Annotations: kubectl.kubernetes.io/last-applied-configuration ={ \"apiVersion\" : \"storage.k8s.io/v1\" , \"kind\" : \"StorageClass\" , \"metadata\" : { \"annotations\" : {} , \"labels\" : { \"kubernetes.io/cluster-service\" : \"true\" } , \"name\" : \"ibmc-file-silver\" } , \"parameters\" : { \"billingType\" : \"hourly\" , \"classVersion\" : \"2\" , \"iopsPerGB\" : \"4\" , \"sizeRange\" : \"[20-12000]Gi\" , \"type\" : \"Endurance\" } , \"provisioner\" : \"ibm.io/ibmc-file\" , \"reclaimPolicy\" : \"Delete\" } Provisioner: ibm.io/ibmc-file Parameters: billingType = hourly,classVersion = 2 ,iopsPerGB = 4 ,sizeRange =[ 20 -12000 ] Gi,type = Endurance AllowVolumeExpansion: <unset> MountOptions: <none> ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: <none> File sliver has an IOPS of 4GB and a max capacity of 12TB.","title":"Claim file storage volume"},{"location":"generatedContent/kubernetes-storage/Lab2/#claim-a-file-storage-volume","text":"IBM Cloud File Storage provides fast access to your data for a cluster running in a single available zone. For higher availability, use a storage option that is designed for geographically distributed data . Review the yaml for the file storage PersistentVolumeClaim . When we create this PersistentVolumeClaim , it automatically creates it within an availability zone where the worker nodes are located. cd guestbook-config/storage/lab2 cat pvc-file-silver.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: guestbook-pvc labels: billingType: hourly spec: accessModes: - ReadWriteMany resources: requests: storage: 20Gi storageClassName: ibmc-file-silver Create the PVC kubectl apply -f pvc-file-silver.yaml Expected output: $ kubectl create -f pvc-file-silver.yaml persistentvolumeclaim/guestbook-filesilver-pvc created Verify the PVC claim is created with the status Bound . This may take a minute or two. kubectl get pvc guestbook-filesilver-pvc Expected output: $ kubectl get pvc guestbook-filesilver-pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE guestbook-filesilver-pvc Bound pvc-a7cb12ed-b52b-4342-966a-eceaf24e42a9 20Gi RWX ibmc-file-silver 2m Details associated with the pv . Use the pv name from the previous command output. kubectl get pv [ pv name ] Expected output: $ kubectl get pv pvc-a7cb12ed-b52b-4342-966a-eceaf24e42a9 NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-a7cb12ed-b52b-4342-966a-eceaf24e42a9 20Gi RWX Delete Bound default/guestbook-filesilver-pvc ibmc-file-silver 90s","title":"Claim a file storage volume"},{"location":"generatedContent/kubernetes-storage/Lab2/#use-the-volume-in-the-guestbook-application","text":"Change to the guestbook application source directory and review the html files images.html and index.html . images.html has the code to display the images stored in the file storage. cd $HOME /guestbook-nodejs/src cat client/images.html cat client/index.html Run the commands listed below to build the guestbook image and copy into the docker hub registry: (Skip this step if you have already completed lab 1.) cd $HOME /guestbook-nodejs/src docker build -t $DOCKERUSER /guestbook-nodejs:storage . docker login -u $DOCKERUSER docker push $DOCKERUSER /guestbook-nodejs:storage Review the deployment yaml file guestbook-deplopyment.yaml prior to deploying the application into the cluster. cd $HOME /guestbook-config/storage/lab2 cat guestbook-deployment.yaml Replace the first part of the image name with your docker hub user id. The section spec.volumes references the file volume PVC. The section spec.containers.volumeMounts has the mount path to store images in the volumes. apiVersion : apps/v1 kind : Deployment metadata : name : guestbook-v1 ... spec : containers : - name : guestbook image : rojanjose/guestbook-nodejs:storage imagePullPolicy : Always ports : - name : http-server containerPort : 3000 volumeMounts : - name : guestbook-file-volume mountPath : /app/public/images volumes : - name : guestbook-file-volume persistentVolumeClaim : claimName : guestbook-filesilver-pvc Deploy the Guestbook application. kubectl create -f guestbook-deployment.yaml kubectl create -f guestbook-service.yaml Verify the Guestbook application is runing. kubectl get all Expected output: $ kubectl get all NAME READY STATUS RESTARTS AGE pod/guestbook-v1-5bd76b568f-cdhr5 1 /1 Running 0 13s pod/guestbook-v1-5bd76b568f-w6h6h 1 /1 Running 0 13s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/guestbook LoadBalancer 172 .21.238.40 150 .238.30.150 3000 :31986/TCP 6s service/kubernetes ClusterIP 172 .21.0.1 <none> 443 /TCP 9d NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-v1 2 /2 2 2 13s NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-v1-5bd76b568f 2 2 2 13s Check the mount path inside the pod container. Get the pod listing. $ kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-5bd76b568f-cdhr5 1 /1 Running 0 78s guestbook-v1-5bd76b568f-w6h6h 1 /1 Running 0 78s Set these variables for each of your pod names: export POD1 =[ FIRST POD NAME ] export POD2 =[ SECOND POD NAME ] Log into any one of the pod. Use one of the pod names from the previous command output. kubectl exec -it $POD1 -- bash Run the commands ls -al; ls -al images; df -ah to view the volume and files. Review the mount for the new volume. Note that the images folder is empty. $ kubectl exec -it $POD1 -- bash root@guestbook-v1-5bd76b568f-cdhr5:/home/node/app# ls -alt total 252 drwxr-xr-x 1 root root 4096 Nov 13 03 :17 client drwxr-xr-x 1 root root 4096 Nov 13 03 :15 . drwxr-xr-x 439 root root 16384 Nov 13 03 :15 node_modules -rw-r--r-- 1 root root 176643 Nov 13 03 :15 package-lock.json drwxr-xr-x 1 node node 4096 Nov 13 03 :15 .. -rw-r--r-- 1 root root 830 Nov 11 23 :20 package.json drwxr-xr-x 3 root root 4096 Nov 10 23 :04 common drwxr-xr-x 3 root root 4096 Nov 10 23 :04 server -rw-r--r-- 1 root root 12 Oct 29 21 :00 .dockerignore -rw-r--r-- 1 root root 288 Oct 29 21 :00 .editorconfig -rw-r--r-- 1 root root 8 Oct 29 21 :00 .eslintignore -rw-r--r-- 1 root root 27 Oct 29 21 :00 .eslintrc -rw-r--r-- 1 root root 151 Oct 29 21 :00 .gitignore -rw-r--r-- 1 root root 30 Oct 29 21 :00 .yo-rc.json -rw-r--r-- 1 root root 105 Oct 29 21 :00 Dockerfile root@guestbook-v1-5bd76b568f-cdhr5:/home/node/app# ls -alt client/images total 8 drwxr-xr-x 1 root root 4096 Nov 13 03 :17 .. drwxr-xr-x 2 nobody 4294967294 4096 Nov 13 02 :02 . root@guestbook-v1-5bd76b568f-cdhr5:/home/node/app# df -h Filesystem Size Used Avail Use% Mounted on overlay 98G 4 .9G 89G 6 % / tmpfs 64M 0 64M 0 % /dev tmpfs 7 .9G 0 7 .9G 0 % /sys/fs/cgroup shm 64M 0 64M 0 % /dev/shm /dev/mapper/docker_data 98G 4 .9G 89G 6 % /etc/hosts tmpfs 7 .9G 16K 7 .9G 1 % /run/secrets/kubernetes.io/serviceaccount fsf-dal1003d-fz.adn.networklayer.com:/IBM02SEV2058850_2177/data01 20G 0 20G 0 % /home/node/app/client/images tmpfs 7 .9G 0 7 .9G 0 % /proc/acpi tmpfs 7 .9G 0 7 .9G 0 % /proc/scsi tmpfs 7 .9G 0 7 .9G 0 % /sys/firmware root@guestbook-v1-5bd76b568f-cdhr5:/home/node/app# exit Note the filesystem fsf-dal1003d-fz.adn.networklayer.com:/IBM02SEV2058850_2177/data01 is mounted on path /home/node/app/client/images . Find the URL for the guestbook application by joining the worker node external IP and service node port. HOSTNAME = ` kubectl get nodes -ojsonpath = '{.items[0].metadata.labels.ibm-cloud\\.kubernetes\\.io\\/external-ip}' ` SERVICEPORT = ` kubectl get svc guestbook -o = jsonpath = '{.spec.ports[0].nodePort}' ` echo \"http:// $HOSTNAME : $SERVICEPORT \" Verify that the images are missing by viewing the data from the Guestbook application. Click on the hyperlink labled images at the bottom of the guestbook home page. The images.html page shows images with broken links.","title":"Use the volume in the Guestbook application"},{"location":"generatedContent/kubernetes-storage/Lab2/#load-the-file-storage-with-images","text":"Run the kubectl cp command to move the images into the mounted volume. cd $HOME /guestbook-config/storage/lab2 kubectl cp images $POD1 :/home/node/app/client/ Refresh the page images.html page in the guestbook application to view the uploaded images.","title":"Load the file storage with images"},{"location":"generatedContent/kubernetes-storage/Lab2/#shared-storage-across-pods","text":"Login into the other pod $POD2 to verify the volume mount. kubectl exec -it $POD2 -- bash root@guestbook-v1-5bd76b568f-w6h6h:/home/node/app# ls -alt /home/node/app/client/images total 160 -rw-r--r-- 1 501 staff 56191 Nov 13 03 :44 gb3.jpg drwxr-xr-x 2 nobody 4294967294 4096 Nov 13 03 :44 . -rw-r--r-- 1 501 staff 21505 Nov 13 03 :44 gb2.jpg -rw-r--r-- 1 501 staff 58286 Nov 13 03 :44 gb1.jpg drwxr-xr-x 1 root root 4096 Nov 13 03 :17 .. root@guestbook-v1-5bd76b568f-w6h6h:/home/node/app# df -h Filesystem Size Used Avail Use% Mounted on overlay 98G 4 .2G 89G 5 % / tmpfs 64M 0 64M 0 % /dev tmpfs 7 .9G 0 7 .9G 0 % /sys/fs/cgroup shm 64M 0 64M 0 % /dev/shm /dev/mapper/docker_data 98G 4 .2G 89G 5 % /etc/hosts tmpfs 7 .9G 16K 7 .9G 1 % /run/secrets/kubernetes.io/serviceaccount fsf-dal1003d-fz.adn.networklayer.com:/IBM02SEV2058850_2177/data01 20G 128K 20G 1 % /home/node/app/client/images tmpfs 7 .9G 0 7 .9G 0 % /proc/acpi tmpfs 7 .9G 0 7 .9G 0 % /proc/scsi tmpfs 7 .9G 0 7 .9G 0 % /sys/firmware root@guestbook-v1-5bd76b568f-w6h6h:/home/node/app# exit Note that the volume and the data are available on all the pods running the Guestbook application. IBM Cloud File Storage is a NFS-based file storage that is available across all worker nodes within a single availability zone. If you are running a cluster with multiple nodes (within a single AZ) you can run the following commands to prove that your data is available across different nodes: kubectl get pods -o wide kubectl get nodes Expected output: $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES guestbook-v1-6fb8b86876-n9jtz 1 /1 Running 0 39h 172 .30.224.70 10 .38.216.205 <none> <none> guestbook-v1-6fb8b86876-njwcz 1 /1 Running 0 39h 172 .30.169.144 10 .38.216.238 <none> <none> $ kubectl get nodes NAME STATUS ROLES AGE VERSION 10 .38.216.205 Ready <none> 4d5h v1.18.10+IKS 10 .38.216.238 Ready <none> 4d5h v1.18.10+IKS To extend our table from Lab 1 we now have: Storage Type Persisted at which level Example Uses Container local storage Container ephermal state Secondary Storage ( EmptyDir ) Pod Checkpoint a long computation process Primary Storage ( HostPath ) Node Running cAdvisor in a container IBM Cloud File Storage (NFS) Availabilty Zone Applications running in a single availabilty zone Data is available to all nodes within the availability zone where the file storage exists, but the accessMode parameter on the PersistentVolumeClaim determines if multiple pods are able to mount a volume specificed by a PVC. The possible values for this parameter are: ReadWriteMany : The PVC can be mounted by multiple pods. All pods can read from and write to the volume. ReadOnlyMany : The PVC can be mounted by multiple pods. All pods have read-only access. ReadWriteOnce : The PVC can be mounted by one pod only. This pod can read from and write to the volume.","title":"Shared storage across pods"},{"location":"generatedContent/kubernetes-storage/Lab2/#optional-exercises","text":"Another way to see that the data is persisted at the availability zone level, you can: Back up data. Delete pods to confirm that it does not impact the data used by the application. Delete the Kubernetes cluster. Create a new cluster and reuse the volume.","title":"[Optional exercises]"},{"location":"generatedContent/kubernetes-storage/Lab2/#clean-up","text":"List all the PVCs and PVs kubectl get pvc kubectl get pv Delete all the pods using the PVC. Delete the PVC kubectl delete pvc guestbook-pvc persistentvolumeclaim \"guestbook-pvc\" deleted List PV to ensure that it is removed as well. Cancel the physical storage volume from the cloud account. (Note: requires admin permissions?) ibmcloud sl file volume-list --columns id --columns notes | grep pvc-6362f614-258e-48ee-a596-62bb4629cd75 183223942 { \"plugin\" : \"ibm-file-plugin-7b9db9c79f-86x8w\" , \"region\" : \"us-south\" , \"cluster\" : \"bugql3nd088jsp8iiagg\" , \"type\" : \"Endurance\" , \"ns\" : \"default\" , \"pvc\" : \"guestbook-pvc\" , \"pv\" : \"pvc-6362f614-258e-48ee-a596-62bb4629cd75\" , \"storageclass\" : \"ibmc-file-silver\" , \"reclaim\" : \"Delete\" } ibmcloud sl file volume-cancel 183223942 This will cancel the file volume: 183223942 and cannot be undone. Continue?> yes Failed to cancel file volume: 183223942 . No billing item is found to cancel.","title":"Clean up"},{"location":"generatedContent/kubernetes-storage/Lab3/","text":"Lab 3. Using IBM Cloud Block Storage with Kubernetes \u00b6 Introduction \u00b6 When looking at what kind of storage class you would like to use in Kubernetes, there are are a few choices such as file storage, block storage, object storage, etc. If your use case requires fast and reliable data access then consider block storage. Block storage is a storage option that breaks data into \"blocks\" and stores those blocks across a Storage Area Network (SAN). These smaller blocks are faster to store and retrieve than large data objects. For this reason, block storage is primarily used as a backing storage for databases. In this lab we will deploy a Mongo database on top of block storage on Kubernetes. The basic architecture is as follows When we install MongoDB with the helm chart, a Persistent Volume Claim (PVC) is created on the cluster. This PVC is a request for storage to be used by the application. In IBM Cloud, the request goes to the IBM Cloud storage provider which then provisions a physical storage device within IBM Cloud. A Persistent Volume (PV) is then created which acts as a reference to the physical storage device created earlier. This PV is then mounted as a directory in a container's file system. The guestbook application receives requests to store guestbook entries from the user which the guestbook pod then sends to the MongoDB pod to store. The MongoDB pod receives the request to store information and persists the data to the mounted directory from the Persistent Volume. Setup \u00b6 Before we get into the lab we first need to do some setup to ensure that the lab will flow smoothly. <!-- 1. Replace <docker username> with your DockerHub username and run the following command (be sure to replace the < > too!). DOCKERUSER = <docker username> ``` --> 1 . In your terminal, navigate to where you would like to store the files used in this lab and run the following. ``` bash WORK_DIR = ` pwd ` Ensure that you have run through the prerequistes in Lab0 Using IBM Cloud Block Storage with Kubernetes \u00b6 Log into the Kubernetes cluster and create a project where we want to deploy our application. kubectl create namespace mongo Install Block Storage Plugin \u00b6 By default IBM Kubernetes Service Clusters don't have the option to deploy block storage persistent volumes. However, there is an easy process to add the block storage storageClass to your cluster through the use of an automated helm chart install. Follow the steps outlined here to install the block storage storageClass . First you need to add the iks-charts helm repo to your local helm repos. This will allow you to utilize a variety of charts to install software on the IBM Kubernetes Service. helm repo add iks-charts https://icr.io/helm/iks-charts Then, we need to update the repo to ensure that we have the latest charts: helm repo update Install the block storage plugin from the iks-charts repo: helm install block-storage-plugin iks-charts/ibmcloud-block-storage-plugin Lastly, verify that the plugin installation was successful by retrieving the list of storage classes in the cluster: kubectl get storageclasses You should notice a few options that start with ibmc-block as seen below. NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE ibmc-block-bronze ibm.io/ibmc-block Delete Immediate true 62s ibmc-block-custom ibm.io/ibmc-block Delete Immediate true 62s ibmc-block-gold ibm.io/ibmc-block Delete Immediate true 62s Helm Repo setup \u00b6 The lab uses Bitnami's Mongodb Helm chart to show case the use of block storage. Set the Bitnami helm repo prior to installing mongodb. helm repo add bitnami https://charts.bitnami.com/bitnami Expected output: $ helm repo add bitnami https://charts.bitnami.com/bitnami \"bitnami\" has been added to your repositories Validate the repo is available in the list. helm repo list You should see a list of repos available to you as seen below: NAME URL bitnami https://charts.bitnami.com/bitnami iks-charts https://icr.io/helm/iks-charts Mongodb with block storage \u00b6 Installation Dry Run \u00b6 Before we install MongoDB, let's do a test of the installation to see what the chart will create. Since we are using Helm to install MongoDB, we can make use of the --dry-run flag in our helm install command to show us the manifest files that Helm will apply on the cluster. Dryrun: helm install mongo bitnami/mongodb --set global.storageClass = ibmc-block-gold,auth.password = testing,auth.username = guestbookAdmin,auth.database = guestbook -n mongo --dry-run > mongdb-install-dryrun.yaml There is a detailed breakdown of this command in the next section titled Installing MongoDB if you would like to understand what this helm command is doing. This command will test out our helm install command and save the output manifests in a file called mongodb-install-dryrun.yaml . You can then examine this manifest file so that you know exactly what will be installed on your cluster. Check out the file in your code editor and take a look at the PersistentVolumeClaim object. There should be a property named storageClassName in the spec and the value should be ibmc-block-gold to signify that we will be using block storage for our database. Below is what that PersistentVolumeClaim object should look like. kind : PersistentVolumeClaim apiVersion : v1 metadata : name : mongo-mongodb namespace : mongo labels : app.kubernetes.io/name : mongodb helm.sh/chart : mongodb-10.0.4 app.kubernetes.io/instance : mongo app.kubernetes.io/managed-by : Helm app.kubernetes.io/component : mongodb spec : accessModes : - \"ReadWriteOnce\" resources : requests : storage : \"8Gi\" storageClassName : ibmc-block-gold Install Mongodb \u00b6 Before we install MongoDB we need to generate a password for our database credentials. These credentials will be used in the application to authenticate with the database. For this lab, will be using the openssl tool to generate the password as this is a common open source cryptographic library. The rest of the command will strip out any characters that could cause issues with the password. USER_PASS = ` openssl rand -base64 12 | tr -d \"=+/\" ` Now we can install MongoDB and supply the password that we just generated. helm install mongo bitnami/mongodb --set global.storageClass = ibmc-block-gold,auth.password = $USER_PASS ,auth.username = guestbook-admin,auth.database = guestbook -n mongo Here's an explanation of the above command: helm install mongo bitnami/mongo : Install the MongoDB bitnami helm chart and name the release \"mongo\". --set global.storageClass=ibmc-block-gold : Set the storage class to block storage rather than the default file storage. ... auth.password=$USER_PASS : Create a custom user with this password (Which we generated earlier). ... auth.username=guestbook-admin : Create a custom user with this username. ... auth.database=guestbook : Create a database named guestbook that the custom user can authenticate to. -n mongo : Install this release in the mongo namespace. Expected output: NAME: mongo LAST DEPLOYED: Tue Nov 24 10 :41:15 2020 NAMESPACE: mongo STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ... View the objects being created by the helm chart. kubectl get all -n mongo NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/mongo-mongodb ClusterIP 172 .21.242.70 <none> 27017 /TCP 17s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/mongo-mongodb 0 /1 0 0 17s NAME DESIRED CURRENT READY AGE replicaset.apps/mongo-mongodb-6f8f7cd789 1 0 0 17s View the list of persistence volume claims. Note that the mongo-mongodb is pending volume allocation. kubectl get pvc -n mongo NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mongo-mongodb Pending ibmc-block-gold 21s After waiting for some time. The pod supporting Mongodb should have a Running status. $ kubectl get all -n mongo NAME READY STATUS RESTARTS AGE pod/mongo-mongodb-66d7bcd7cf-vqvbj 1 /1 Running 0 8m37s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/mongo-mongodb ClusterIP 172 .21.242.70 <none> 27017 /TCP 12m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/mongo-mongodb 1 /1 1 1 12m NAME DESIRED CURRENT READY AGE replicaset.apps/mongo-mongodb-66d7bcd7cf 1 1 1 8m37s replicaset.apps/mongo-mongodb-6f8f7cd789 0 0 0 12m And the PVC mongo-mongodb is now bound to volume pvc-2f423668-4f87-4ae4-8edf-8c892188b645 $ kubectl get pvc -n mongo NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mongo-mongodb Bound pvc-2f423668-4f87-4ae4-8edf-8c892188b645 20Gi RWO ibmc-block-gold 2m26s With MongoDB deployed now we need to deploy an application that will utilize it as a datastore. Building Guestbook \u00b6 For this lab we will be using the guestbook application which is a common sample kubernetes application. However, the version that we are using has been refactored as a loopback application. Clone the application repo and the configuration repo. In your terminal, run the following: cd $WORK_DIR git clone https://github.com/IBM/guestbook-nodejs.git git clone https://github.com/IBM/guestbook-nodejs-config/ --branch mongo Then, navigate into the guestbook-nodejs directory. cd $WORK_DIR /guestbook-nodejs/src Replace the code in the server/datasources.json file with the following: { \"in-memory\" : { \"name\" : \"in-memory\" , \"localStorage\" : \"\" , \"file\" : \"\" , \"connector\" : \"memory\" }, \"mongo\" : { \"host\" : \"${MONGO_HOST}\" , \"port\" : \"${MONGO_PORT}\" , \"url\" : \"\" , \"database\" : \"${MONGO_DB}\" , \"password\" : \"${MONGO_PASS}\" , \"name\" : \"mongo\" , \"user\" : \"${MONGO_USER}\" , \"useNewUrlParser\" : true , \"connector\" : \"mongodb\" } } This file will contain the connection information to our MongoDB instance. These variables will be passed into the environment from ConfigMaps and Secrets that we will create. Open the server/model-config.json file and change the entry.datasource value to mongo as seen below: ... \"entry\" : { \"dataSource\" : \"mongo\" , \"public\" : true } } In this file we are telling the application which datasource we should use; in-memory or MongoDB. By default the application comes with an in-memory datastore for storing information but this data does not persist after the application crashes or if the pod goes down for any reason. We are changing in-memory to mongo so that the data will persist in our MongoDB instance external to the application so that the data will remain even after the application crashes. Now we need to build our application image and push it to DockerHub. cd $WORK_DIR /guestbook-nodejs/src IMAGE_NAME = $DOCKERUSER /guestbook-nodejs:mongo docker build -t $IMAGE_NAME . docker login -u $DOCKERUSER docker push $IMAGE_NAME Deploying Guestbook \u00b6 Now that we have built our application, let's check out the manifest files needed to deploy it to Kubernetes. Navigate to the configuration repo that we cloned earlier. cd $WORK_DIR /guestbook-nodejs-config This repo contains 3 manifests that we will be deploying to our cluster today: A deployment manifest A service manifest A configMap manifest These manifests will create their respective kubernetes objects on our cluster. The deployment will deploy our application image that we built earlier while the service will expose that application to external traffic. The configMap will contain connection information for our database such as database hostname and port. Open the guestbook-deployment.yaml file and edit line 25 to point to the image that you built and pushed earlier. Do this by replacing <DockerUsername> with your docker username. (Don't forget to replace the < > too!) For example, my Docker username is odrodrig so line 25 in my guestbook-deployment.yaml file would look like this: ... image : odrodrig/guestbook-nodejs:mongo ... As part of the deployment, kubernetes will copy the database connection information from the configMap into the environment of the application. You can see where this is specified in the env section of the deployment manifest as seen below: ... env : - name : MONGO_HOST valueFrom : configMapKeyRef : name : mongo-config key : mongo_host - name : MONGO_PORT valueFrom : configMapKeyRef : name : mongo-config key : mongo_port - name : MONGO_USER valueFrom : secretKeyRef : name : mongodb key : username - name : MONGO_PASS valueFrom : secretKeyRef : name : mongodb key : password - name : MONGO_DB valueFrom : configMapKeyRef : name : mongo-config key : mongo_db_name You might also notice that we are getting our database username ( MONGO_USER ) and password ( MONGO_PASS ) from a kubernetes secret. We haven't defined that secret yet so let's do it now. kubectl create secret generic mongodb --from-literal = username = guestbook-admin --from-literal = password = $USER_PASS -n mongo Now we are ready to deploy the application. Run the following commands: cd $WORK_DIR /guestbook-nodejs-config/ kubectl apply -f . -n mongo Ensure that the application pod is running: kubectl get pods -n mongo You should see both the mongo pod and the guestbook pod running now: NAME READY STATUS RESTARTS AGE guestbook-v1-9465dcbb4-zdhqv 1 /1 Running 0 19s mongo-mongodb-757d9777d7-j4759 1 /1 Running 0 27m Test out the application \u00b6 Now that we have deployed the application, let's test it out. Find the URL for the guestbook application by joining the worker node external IP and service node port. Run the following to get the IP and service node port of the application: HOSTNAME = ` kubectl get nodes -ojsonpath = '{.items[0].metadata.labels.ibm-cloud\\.kubernetes\\.io\\/external-ip}' ` SERVICEPORT = ` kubectl get svc guestbook -n mongo -o = jsonpath = '{.spec.ports[0].nodePort}' ` echo \"http:// $HOSTNAME : $SERVICEPORT \" In your browser, open up the address that was output as part of the previous command. Type in a few test entries in the text box and press enter to submit them. These entries are now saved in the Mongo database. Let's take down the application and see if the data will truly persist. Find the name of the pod that is running our application: kubectl get pods -n mongo Copy the name of the pod that starts with guestbook . For me, the pod is named guestbook-v1-9465dcbb4-f6s9h . NAME READY STATUS RESTARTS AGE guestbook-v1-9465dcbb4-f6s9h 1 /1 Running 0 4m7s mongo-mongodb-757d9777d7-q64lg 1 /1 Running 0 5m47s Then, run the following command, replacing <pod name> with pod name that you just copied. kubectl delete pod -n mongo <pod name> You should then see a message saying that your pod has been deleted. $ kubectl delete pod -n mongo guestbook-v1-9465dcbb4-f6s9h pod \"guestbook-v1-9465dcbb4-f6s9h\" deleted Now, view your pods again: kubectl get pods -n mongo You should see the guestbook pod is back now with and the age has been reset. This means that it is a brand new pod that kubernetes has deployed automatically after our previous pod was deleted. NAME READY STATUS RESTARTS AGE guestbook-v1-9465dcbb4-8z8bt 1 /1 Running 0 87s mongo-mongodb-757d9777d7-q64lg 1 /1 Running 0 9m13s Refresh your browser tab that had the guestbook application and you will see that your data has indeed persisted after our pod went down. Summary \u00b6 In this lab we used block storage to run our own database on Kubernetes. Block storage allows for fast I/O operations making it ideal for our application database. We utilized configMaps and secrets to store the database configuration making it easy to use this application with different database configurations without making code changes. Cleanup (Optional) \u00b6 This part of the lab desrcibes the steps to delete what was built in the lab. Deleting the application \u00b6 cd $WORK_DIR /guestbook-nodejs-config kubectl delete -f . -n mongo Uninstalling Mongo \u00b6 helm uninstall mongo -n mongo Remove namespace \u00b6 kubectl delete namespace mongo","title":"Lab 3. Block storage with kubernetes"},{"location":"generatedContent/kubernetes-storage/Lab3/#lab-3-using-ibm-cloud-block-storage-with-kubernetes","text":"","title":"Lab 3. Using IBM Cloud Block Storage with Kubernetes"},{"location":"generatedContent/kubernetes-storage/Lab3/#introduction","text":"When looking at what kind of storage class you would like to use in Kubernetes, there are are a few choices such as file storage, block storage, object storage, etc. If your use case requires fast and reliable data access then consider block storage. Block storage is a storage option that breaks data into \"blocks\" and stores those blocks across a Storage Area Network (SAN). These smaller blocks are faster to store and retrieve than large data objects. For this reason, block storage is primarily used as a backing storage for databases. In this lab we will deploy a Mongo database on top of block storage on Kubernetes. The basic architecture is as follows When we install MongoDB with the helm chart, a Persistent Volume Claim (PVC) is created on the cluster. This PVC is a request for storage to be used by the application. In IBM Cloud, the request goes to the IBM Cloud storage provider which then provisions a physical storage device within IBM Cloud. A Persistent Volume (PV) is then created which acts as a reference to the physical storage device created earlier. This PV is then mounted as a directory in a container's file system. The guestbook application receives requests to store guestbook entries from the user which the guestbook pod then sends to the MongoDB pod to store. The MongoDB pod receives the request to store information and persists the data to the mounted directory from the Persistent Volume.","title":"Introduction"},{"location":"generatedContent/kubernetes-storage/Lab3/#setup","text":"Before we get into the lab we first need to do some setup to ensure that the lab will flow smoothly. <!-- 1. Replace <docker username> with your DockerHub username and run the following command (be sure to replace the < > too!). DOCKERUSER = <docker username> ``` --> 1 . In your terminal, navigate to where you would like to store the files used in this lab and run the following. ``` bash WORK_DIR = ` pwd ` Ensure that you have run through the prerequistes in Lab0","title":"Setup"},{"location":"generatedContent/kubernetes-storage/Lab3/#using-ibm-cloud-block-storage-with-kubernetes","text":"Log into the Kubernetes cluster and create a project where we want to deploy our application. kubectl create namespace mongo","title":"Using IBM Cloud Block Storage with Kubernetes"},{"location":"generatedContent/kubernetes-storage/Lab3/#install-block-storage-plugin","text":"By default IBM Kubernetes Service Clusters don't have the option to deploy block storage persistent volumes. However, there is an easy process to add the block storage storageClass to your cluster through the use of an automated helm chart install. Follow the steps outlined here to install the block storage storageClass . First you need to add the iks-charts helm repo to your local helm repos. This will allow you to utilize a variety of charts to install software on the IBM Kubernetes Service. helm repo add iks-charts https://icr.io/helm/iks-charts Then, we need to update the repo to ensure that we have the latest charts: helm repo update Install the block storage plugin from the iks-charts repo: helm install block-storage-plugin iks-charts/ibmcloud-block-storage-plugin Lastly, verify that the plugin installation was successful by retrieving the list of storage classes in the cluster: kubectl get storageclasses You should notice a few options that start with ibmc-block as seen below. NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE ibmc-block-bronze ibm.io/ibmc-block Delete Immediate true 62s ibmc-block-custom ibm.io/ibmc-block Delete Immediate true 62s ibmc-block-gold ibm.io/ibmc-block Delete Immediate true 62s","title":"Install Block Storage Plugin"},{"location":"generatedContent/kubernetes-storage/Lab3/#helm-repo-setup","text":"The lab uses Bitnami's Mongodb Helm chart to show case the use of block storage. Set the Bitnami helm repo prior to installing mongodb. helm repo add bitnami https://charts.bitnami.com/bitnami Expected output: $ helm repo add bitnami https://charts.bitnami.com/bitnami \"bitnami\" has been added to your repositories Validate the repo is available in the list. helm repo list You should see a list of repos available to you as seen below: NAME URL bitnami https://charts.bitnami.com/bitnami iks-charts https://icr.io/helm/iks-charts","title":"Helm Repo setup"},{"location":"generatedContent/kubernetes-storage/Lab3/#mongodb-with-block-storage","text":"","title":"Mongodb with block storage"},{"location":"generatedContent/kubernetes-storage/Lab3/#installation-dry-run","text":"Before we install MongoDB, let's do a test of the installation to see what the chart will create. Since we are using Helm to install MongoDB, we can make use of the --dry-run flag in our helm install command to show us the manifest files that Helm will apply on the cluster. Dryrun: helm install mongo bitnami/mongodb --set global.storageClass = ibmc-block-gold,auth.password = testing,auth.username = guestbookAdmin,auth.database = guestbook -n mongo --dry-run > mongdb-install-dryrun.yaml There is a detailed breakdown of this command in the next section titled Installing MongoDB if you would like to understand what this helm command is doing. This command will test out our helm install command and save the output manifests in a file called mongodb-install-dryrun.yaml . You can then examine this manifest file so that you know exactly what will be installed on your cluster. Check out the file in your code editor and take a look at the PersistentVolumeClaim object. There should be a property named storageClassName in the spec and the value should be ibmc-block-gold to signify that we will be using block storage for our database. Below is what that PersistentVolumeClaim object should look like. kind : PersistentVolumeClaim apiVersion : v1 metadata : name : mongo-mongodb namespace : mongo labels : app.kubernetes.io/name : mongodb helm.sh/chart : mongodb-10.0.4 app.kubernetes.io/instance : mongo app.kubernetes.io/managed-by : Helm app.kubernetes.io/component : mongodb spec : accessModes : - \"ReadWriteOnce\" resources : requests : storage : \"8Gi\" storageClassName : ibmc-block-gold","title":"Installation Dry Run"},{"location":"generatedContent/kubernetes-storage/Lab3/#install-mongodb","text":"Before we install MongoDB we need to generate a password for our database credentials. These credentials will be used in the application to authenticate with the database. For this lab, will be using the openssl tool to generate the password as this is a common open source cryptographic library. The rest of the command will strip out any characters that could cause issues with the password. USER_PASS = ` openssl rand -base64 12 | tr -d \"=+/\" ` Now we can install MongoDB and supply the password that we just generated. helm install mongo bitnami/mongodb --set global.storageClass = ibmc-block-gold,auth.password = $USER_PASS ,auth.username = guestbook-admin,auth.database = guestbook -n mongo Here's an explanation of the above command: helm install mongo bitnami/mongo : Install the MongoDB bitnami helm chart and name the release \"mongo\". --set global.storageClass=ibmc-block-gold : Set the storage class to block storage rather than the default file storage. ... auth.password=$USER_PASS : Create a custom user with this password (Which we generated earlier). ... auth.username=guestbook-admin : Create a custom user with this username. ... auth.database=guestbook : Create a database named guestbook that the custom user can authenticate to. -n mongo : Install this release in the mongo namespace. Expected output: NAME: mongo LAST DEPLOYED: Tue Nov 24 10 :41:15 2020 NAMESPACE: mongo STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ... View the objects being created by the helm chart. kubectl get all -n mongo NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/mongo-mongodb ClusterIP 172 .21.242.70 <none> 27017 /TCP 17s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/mongo-mongodb 0 /1 0 0 17s NAME DESIRED CURRENT READY AGE replicaset.apps/mongo-mongodb-6f8f7cd789 1 0 0 17s View the list of persistence volume claims. Note that the mongo-mongodb is pending volume allocation. kubectl get pvc -n mongo NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mongo-mongodb Pending ibmc-block-gold 21s After waiting for some time. The pod supporting Mongodb should have a Running status. $ kubectl get all -n mongo NAME READY STATUS RESTARTS AGE pod/mongo-mongodb-66d7bcd7cf-vqvbj 1 /1 Running 0 8m37s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/mongo-mongodb ClusterIP 172 .21.242.70 <none> 27017 /TCP 12m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/mongo-mongodb 1 /1 1 1 12m NAME DESIRED CURRENT READY AGE replicaset.apps/mongo-mongodb-66d7bcd7cf 1 1 1 8m37s replicaset.apps/mongo-mongodb-6f8f7cd789 0 0 0 12m And the PVC mongo-mongodb is now bound to volume pvc-2f423668-4f87-4ae4-8edf-8c892188b645 $ kubectl get pvc -n mongo NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mongo-mongodb Bound pvc-2f423668-4f87-4ae4-8edf-8c892188b645 20Gi RWO ibmc-block-gold 2m26s With MongoDB deployed now we need to deploy an application that will utilize it as a datastore.","title":"Install Mongodb"},{"location":"generatedContent/kubernetes-storage/Lab3/#building-guestbook","text":"For this lab we will be using the guestbook application which is a common sample kubernetes application. However, the version that we are using has been refactored as a loopback application. Clone the application repo and the configuration repo. In your terminal, run the following: cd $WORK_DIR git clone https://github.com/IBM/guestbook-nodejs.git git clone https://github.com/IBM/guestbook-nodejs-config/ --branch mongo Then, navigate into the guestbook-nodejs directory. cd $WORK_DIR /guestbook-nodejs/src Replace the code in the server/datasources.json file with the following: { \"in-memory\" : { \"name\" : \"in-memory\" , \"localStorage\" : \"\" , \"file\" : \"\" , \"connector\" : \"memory\" }, \"mongo\" : { \"host\" : \"${MONGO_HOST}\" , \"port\" : \"${MONGO_PORT}\" , \"url\" : \"\" , \"database\" : \"${MONGO_DB}\" , \"password\" : \"${MONGO_PASS}\" , \"name\" : \"mongo\" , \"user\" : \"${MONGO_USER}\" , \"useNewUrlParser\" : true , \"connector\" : \"mongodb\" } } This file will contain the connection information to our MongoDB instance. These variables will be passed into the environment from ConfigMaps and Secrets that we will create. Open the server/model-config.json file and change the entry.datasource value to mongo as seen below: ... \"entry\" : { \"dataSource\" : \"mongo\" , \"public\" : true } } In this file we are telling the application which datasource we should use; in-memory or MongoDB. By default the application comes with an in-memory datastore for storing information but this data does not persist after the application crashes or if the pod goes down for any reason. We are changing in-memory to mongo so that the data will persist in our MongoDB instance external to the application so that the data will remain even after the application crashes. Now we need to build our application image and push it to DockerHub. cd $WORK_DIR /guestbook-nodejs/src IMAGE_NAME = $DOCKERUSER /guestbook-nodejs:mongo docker build -t $IMAGE_NAME . docker login -u $DOCKERUSER docker push $IMAGE_NAME","title":"Building Guestbook"},{"location":"generatedContent/kubernetes-storage/Lab3/#deploying-guestbook","text":"Now that we have built our application, let's check out the manifest files needed to deploy it to Kubernetes. Navigate to the configuration repo that we cloned earlier. cd $WORK_DIR /guestbook-nodejs-config This repo contains 3 manifests that we will be deploying to our cluster today: A deployment manifest A service manifest A configMap manifest These manifests will create their respective kubernetes objects on our cluster. The deployment will deploy our application image that we built earlier while the service will expose that application to external traffic. The configMap will contain connection information for our database such as database hostname and port. Open the guestbook-deployment.yaml file and edit line 25 to point to the image that you built and pushed earlier. Do this by replacing <DockerUsername> with your docker username. (Don't forget to replace the < > too!) For example, my Docker username is odrodrig so line 25 in my guestbook-deployment.yaml file would look like this: ... image : odrodrig/guestbook-nodejs:mongo ... As part of the deployment, kubernetes will copy the database connection information from the configMap into the environment of the application. You can see where this is specified in the env section of the deployment manifest as seen below: ... env : - name : MONGO_HOST valueFrom : configMapKeyRef : name : mongo-config key : mongo_host - name : MONGO_PORT valueFrom : configMapKeyRef : name : mongo-config key : mongo_port - name : MONGO_USER valueFrom : secretKeyRef : name : mongodb key : username - name : MONGO_PASS valueFrom : secretKeyRef : name : mongodb key : password - name : MONGO_DB valueFrom : configMapKeyRef : name : mongo-config key : mongo_db_name You might also notice that we are getting our database username ( MONGO_USER ) and password ( MONGO_PASS ) from a kubernetes secret. We haven't defined that secret yet so let's do it now. kubectl create secret generic mongodb --from-literal = username = guestbook-admin --from-literal = password = $USER_PASS -n mongo Now we are ready to deploy the application. Run the following commands: cd $WORK_DIR /guestbook-nodejs-config/ kubectl apply -f . -n mongo Ensure that the application pod is running: kubectl get pods -n mongo You should see both the mongo pod and the guestbook pod running now: NAME READY STATUS RESTARTS AGE guestbook-v1-9465dcbb4-zdhqv 1 /1 Running 0 19s mongo-mongodb-757d9777d7-j4759 1 /1 Running 0 27m","title":"Deploying Guestbook"},{"location":"generatedContent/kubernetes-storage/Lab3/#test-out-the-application","text":"Now that we have deployed the application, let's test it out. Find the URL for the guestbook application by joining the worker node external IP and service node port. Run the following to get the IP and service node port of the application: HOSTNAME = ` kubectl get nodes -ojsonpath = '{.items[0].metadata.labels.ibm-cloud\\.kubernetes\\.io\\/external-ip}' ` SERVICEPORT = ` kubectl get svc guestbook -n mongo -o = jsonpath = '{.spec.ports[0].nodePort}' ` echo \"http:// $HOSTNAME : $SERVICEPORT \" In your browser, open up the address that was output as part of the previous command. Type in a few test entries in the text box and press enter to submit them. These entries are now saved in the Mongo database. Let's take down the application and see if the data will truly persist. Find the name of the pod that is running our application: kubectl get pods -n mongo Copy the name of the pod that starts with guestbook . For me, the pod is named guestbook-v1-9465dcbb4-f6s9h . NAME READY STATUS RESTARTS AGE guestbook-v1-9465dcbb4-f6s9h 1 /1 Running 0 4m7s mongo-mongodb-757d9777d7-q64lg 1 /1 Running 0 5m47s Then, run the following command, replacing <pod name> with pod name that you just copied. kubectl delete pod -n mongo <pod name> You should then see a message saying that your pod has been deleted. $ kubectl delete pod -n mongo guestbook-v1-9465dcbb4-f6s9h pod \"guestbook-v1-9465dcbb4-f6s9h\" deleted Now, view your pods again: kubectl get pods -n mongo You should see the guestbook pod is back now with and the age has been reset. This means that it is a brand new pod that kubernetes has deployed automatically after our previous pod was deleted. NAME READY STATUS RESTARTS AGE guestbook-v1-9465dcbb4-8z8bt 1 /1 Running 0 87s mongo-mongodb-757d9777d7-q64lg 1 /1 Running 0 9m13s Refresh your browser tab that had the guestbook application and you will see that your data has indeed persisted after our pod went down.","title":"Test out the application"},{"location":"generatedContent/kubernetes-storage/Lab3/#summary","text":"In this lab we used block storage to run our own database on Kubernetes. Block storage allows for fast I/O operations making it ideal for our application database. We utilized configMaps and secrets to store the database configuration making it easy to use this application with different database configurations without making code changes.","title":"Summary"},{"location":"generatedContent/kubernetes-storage/Lab3/#cleanup-optional","text":"This part of the lab desrcibes the steps to delete what was built in the lab.","title":"Cleanup (Optional)"},{"location":"generatedContent/kubernetes-storage/Lab3/#deleting-the-application","text":"cd $WORK_DIR /guestbook-nodejs-config kubectl delete -f . -n mongo","title":"Deleting the application"},{"location":"generatedContent/kubernetes-storage/Lab3/#uninstalling-mongo","text":"helm uninstall mongo -n mongo","title":"Uninstalling Mongo"},{"location":"generatedContent/kubernetes-storage/Lab3/#remove-namespace","text":"kubectl delete namespace mongo","title":"Remove namespace"},{"location":"generatedContent/workshop-setup/docs/","text":"Workshop Setup \u00b6 New IBM Cloud Account Upgrade IBM Cloud Account to Pay-As-You-Go IBM Cloud Shell Terminal at CognitiveClass.ai Grant Cluster Permissions Helm v3 Jenkins","title":"Workshop Setup"},{"location":"generatedContent/workshop-setup/docs/#workshop-setup","text":"New IBM Cloud Account Upgrade IBM Cloud Account to Pay-As-You-Go IBM Cloud Shell Terminal at CognitiveClass.ai Grant Cluster Permissions Helm v3 Jenkins","title":"Workshop Setup"},{"location":"generatedContent/workshop-setup/docs/CALICO/","text":"Calico \u00b6 IBM Cloud Kubernetes Service (IKS) \u00b6 Calico is installed by default on IKS. To install the calicoctl client follow the instructions below. Connect to your Kubernetes cluster to start using the calicoctl . If you need help to setup your free IBM Cloud Kubernetes Service (IKS) cluster, go here , or to setup your RedHat OpenShift Kubernetes Service (ROKS), go here . calicoctl \u00b6 The following commands will install the calicoctl client in the browser-based terminal environment at CognitiveClass.ai . If you need help setting up the client terminal at CognitiveClass.ai, follow the instructions here . mkdir calico wget https://github.com/projectcalico/calicoctl/releases/download/v3.17.1/calicoctl -P ./calico chmod +x calico/calicoctl echo \"export PATH=$(pwd)/calico/:$PATH\" > $HOME/.bash_profile source $HOME/.bash_profile calicoctl version Client Version: v3.17.1 Git commit: 8871aca3 Unable to retrieve Cluster Version or Type: connection is unauthorized: clusterinformations.crd.projectcalico.org \"default\" is forbidden: User \"system:serviceaccount:sn-labs-remkohdev:remkohdev\" cannot get resource \"clusterinformations\" in API group \"crd.projectcalico.org\" at the cluster scope As kubectl Plugin \u00b6 To install the calicoctl as a plugin to the kubectl client, run the following commands, mkdir calico curl -o ./calico/kubectl-calico -L https://github.com/projectcalico/calicoctl/releases/download/v3.17.1/calicoctl chmod +x ./calico/kubectl-calico echo \"export PATH=$(pwd)/calico/:$PATH\" > $HOME/.bash_profile source $HOME/.bash_profile kubectl calico -h","title":"Calico"},{"location":"generatedContent/workshop-setup/docs/CALICO/#calico","text":"","title":"Calico"},{"location":"generatedContent/workshop-setup/docs/CALICO/#ibm-cloud-kubernetes-service-iks","text":"Calico is installed by default on IKS. To install the calicoctl client follow the instructions below. Connect to your Kubernetes cluster to start using the calicoctl . If you need help to setup your free IBM Cloud Kubernetes Service (IKS) cluster, go here , or to setup your RedHat OpenShift Kubernetes Service (ROKS), go here .","title":"IBM Cloud Kubernetes Service (IKS)"},{"location":"generatedContent/workshop-setup/docs/CALICO/#calicoctl","text":"The following commands will install the calicoctl client in the browser-based terminal environment at CognitiveClass.ai . If you need help setting up the client terminal at CognitiveClass.ai, follow the instructions here . mkdir calico wget https://github.com/projectcalico/calicoctl/releases/download/v3.17.1/calicoctl -P ./calico chmod +x calico/calicoctl echo \"export PATH=$(pwd)/calico/:$PATH\" > $HOME/.bash_profile source $HOME/.bash_profile calicoctl version Client Version: v3.17.1 Git commit: 8871aca3 Unable to retrieve Cluster Version or Type: connection is unauthorized: clusterinformations.crd.projectcalico.org \"default\" is forbidden: User \"system:serviceaccount:sn-labs-remkohdev:remkohdev\" cannot get resource \"clusterinformations\" in API group \"crd.projectcalico.org\" at the cluster scope","title":"calicoctl"},{"location":"generatedContent/workshop-setup/docs/CALICO/#as-kubectl-plugin","text":"To install the calicoctl as a plugin to the kubectl client, run the following commands, mkdir calico curl -o ./calico/kubectl-calico -L https://github.com/projectcalico/calicoctl/releases/download/v3.17.1/calicoctl chmod +x ./calico/kubectl-calico echo \"export PATH=$(pwd)/calico/:$PATH\" > $HOME/.bash_profile source $HOME/.bash_profile kubectl calico -h","title":"As kubectl Plugin"},{"location":"generatedContent/workshop-setup/docs/CLOUDSHELL/","text":"IBM Cloud Shell \u00b6 You can access the IBM Cloud Shell directly at IBM Cloud Shell .","title":"IBM Cloud Shell"},{"location":"generatedContent/workshop-setup/docs/CLOUDSHELL/#ibm-cloud-shell","text":"You can access the IBM Cloud Shell directly at IBM Cloud Shell .","title":"IBM Cloud Shell"},{"location":"generatedContent/workshop-setup/docs/COGNITIVECLASS/","text":"CognitiveClass.ai \u00b6 Access CognitiveClass.ai \u00b6 If you have already registered your account, you can access the lab environment at https://labs.cognitiveclass.ai and login. Navigate to https://labs.cognitiveclass.ai/register , Create a new account with your existing IBM Id. Alternative, you can choose to use a Social login (LinkedIn, Google, Github or Facebook), or for using your email account click the Cognitive Class button, Click Create an Account , Fill in your Email, Full Name, Public Username and password, click on the check boxes next to the Privacy Notice and Terms of Service to accept them. Then click on Create Account . You will then be taken to a page with a list of sandbox environments. Click on the option for Theia - Cloud IDE (With OpenShift) Wait a few minutes while your environment is created. You will be taken to a blank editor page once your environment is ready. What we really need is access to the terminal. Click on the Terminal tab near the top of the page and select New Terminal You can then click and drag the top of the terminal section upwards to make the terminal section bigger.","title":"CognitiveClass.ai"},{"location":"generatedContent/workshop-setup/docs/COGNITIVECLASS/#cognitiveclassai","text":"","title":"CognitiveClass.ai"},{"location":"generatedContent/workshop-setup/docs/COGNITIVECLASS/#access-cognitiveclassai","text":"If you have already registered your account, you can access the lab environment at https://labs.cognitiveclass.ai and login. Navigate to https://labs.cognitiveclass.ai/register , Create a new account with your existing IBM Id. Alternative, you can choose to use a Social login (LinkedIn, Google, Github or Facebook), or for using your email account click the Cognitive Class button, Click Create an Account , Fill in your Email, Full Name, Public Username and password, click on the check boxes next to the Privacy Notice and Terms of Service to accept them. Then click on Create Account . You will then be taken to a page with a list of sandbox environments. Click on the option for Theia - Cloud IDE (With OpenShift) Wait a few minutes while your environment is created. You will be taken to a blank editor page once your environment is ready. What we really need is access to the terminal. Click on the Terminal tab near the top of the page and select New Terminal You can then click and drag the top of the terminal section upwards to make the terminal section bigger.","title":"Access CognitiveClass.ai"},{"location":"generatedContent/workshop-setup/docs/FREEIKSCLUSTER/","text":"Create Free Kubernetes Cluster \u00b6 Prerequirements \u00b6 Free IBM Cloud account, to create a new IBM Cloud account go here . Free IBM Cloud Pay-As-You-Go account, to upgrade to a Pay-As-You-Go account go here IBM Cloud CLI with the Kubernetes Service plugin, see the IBM Cloud CLI Getting Started , or use a pre-installed client environment like the Labs environment at CognitiveClass, CognitiveLabs.ai account, to access a client terminal at CognitiveLabs.ai, go here . Using UI \u00b6 Log in to IBM Cloud , Go to the Services Catalog , Filter the services by Kubernetes , Click the Kubernetes Service tile, Or go to the Create Kubernetes Service page directly, Configure the Kubernetes cluster as follows: For Pricing plan select Free . The page options will reload, leaving the default Kubernetes version as your only option, At the time of writing, the default Kubernetes version was set to 1.18.13 , Under Orchestration service , edit the Cluster name to a globally unique name, I recommend to follow a format like username-iks118-1n-cluster1 , where iks118 represents your Kubernetes version, 1n the number of worker nodes, and cluster1 represents your cluster number, in case you have more than 1 cluster. For Resource Group keep the Default resource group, unless you have created a new resource group and want to use your own resource group, Click Create to initiate the create cluster request, You will be forwarded to the Access details for the new cluster, Using CLI \u00b6 To create a free Kubernetes Service, you need to be logged in to a free IBM Cloud Pay-As-You-Go account. IBMID=<your ibm id email> USERNAME=<your short username> KS_CLUSTER_NAME=$USERNAME-iks118-1n-cluster1 KS_ZONE=dal10 KS_VERSION=1.18 KS_FLAVOR=u3c.2x4 KS_WORKERS=1 KS_PROVIDER=classic ibmcloud ks zone ls --provider $KS_PROVIDER ibmcloud ks flavors --zone $KS_ZONE --provider $KS_PROVIDER ibmcloud ks versions ibmcloud login -u $IBMID ibmcloud ks cluster create $KS_PROVIDER --name $KS_CLUSTER_NAME --zone $KS_ZONE --version $KS_VERSION --flavor $KS_FLAVOR --workers $KS_WORKERS The response should display similar output as, $ ibmcloud ks cluster create $KS_PROVIDER --name $KS_CLUSTER_NAME --zone $KS_ZONE --version $KS_VERSION --flavor $KS_FLAVOR --workers $KS_WORKERS Creating cluster... OK Cluster created with ID bvlntf2d0fe4l9hnres0 Retrieve details of the new cluster, ibmcloud ks cluster get --cluster $KS_CLUSTER_NAME --output json","title":"Create Free Kubernetes Cluster"},{"location":"generatedContent/workshop-setup/docs/FREEIKSCLUSTER/#create-free-kubernetes-cluster","text":"","title":"Create Free Kubernetes Cluster"},{"location":"generatedContent/workshop-setup/docs/FREEIKSCLUSTER/#prerequirements","text":"Free IBM Cloud account, to create a new IBM Cloud account go here . Free IBM Cloud Pay-As-You-Go account, to upgrade to a Pay-As-You-Go account go here IBM Cloud CLI with the Kubernetes Service plugin, see the IBM Cloud CLI Getting Started , or use a pre-installed client environment like the Labs environment at CognitiveClass, CognitiveLabs.ai account, to access a client terminal at CognitiveLabs.ai, go here .","title":"Prerequirements"},{"location":"generatedContent/workshop-setup/docs/FREEIKSCLUSTER/#using-ui","text":"Log in to IBM Cloud , Go to the Services Catalog , Filter the services by Kubernetes , Click the Kubernetes Service tile, Or go to the Create Kubernetes Service page directly, Configure the Kubernetes cluster as follows: For Pricing plan select Free . The page options will reload, leaving the default Kubernetes version as your only option, At the time of writing, the default Kubernetes version was set to 1.18.13 , Under Orchestration service , edit the Cluster name to a globally unique name, I recommend to follow a format like username-iks118-1n-cluster1 , where iks118 represents your Kubernetes version, 1n the number of worker nodes, and cluster1 represents your cluster number, in case you have more than 1 cluster. For Resource Group keep the Default resource group, unless you have created a new resource group and want to use your own resource group, Click Create to initiate the create cluster request, You will be forwarded to the Access details for the new cluster,","title":"Using UI"},{"location":"generatedContent/workshop-setup/docs/FREEIKSCLUSTER/#using-cli","text":"To create a free Kubernetes Service, you need to be logged in to a free IBM Cloud Pay-As-You-Go account. IBMID=<your ibm id email> USERNAME=<your short username> KS_CLUSTER_NAME=$USERNAME-iks118-1n-cluster1 KS_ZONE=dal10 KS_VERSION=1.18 KS_FLAVOR=u3c.2x4 KS_WORKERS=1 KS_PROVIDER=classic ibmcloud ks zone ls --provider $KS_PROVIDER ibmcloud ks flavors --zone $KS_ZONE --provider $KS_PROVIDER ibmcloud ks versions ibmcloud login -u $IBMID ibmcloud ks cluster create $KS_PROVIDER --name $KS_CLUSTER_NAME --zone $KS_ZONE --version $KS_VERSION --flavor $KS_FLAVOR --workers $KS_WORKERS The response should display similar output as, $ ibmcloud ks cluster create $KS_PROVIDER --name $KS_CLUSTER_NAME --zone $KS_ZONE --version $KS_VERSION --flavor $KS_FLAVOR --workers $KS_WORKERS Creating cluster... OK Cluster created with ID bvlntf2d0fe4l9hnres0 Retrieve details of the new cluster, ibmcloud ks cluster get --cluster $KS_CLUSTER_NAME --output json","title":"Using CLI"},{"location":"generatedContent/workshop-setup/docs/GRANTCLUSTER/","text":"Grant Cluster \u00b6 IBM Kubernetes Service (IKS) and RedHat OpenShift Kubernetes Service (ROKS) \u00b6 The grant cluster method to get access to a Kubernetes cluster will assign access permissions to a cluster or namespace in a cluster that was created prior to the request. Creating a cluster and provisioning the VMs and other resources and deploying the tools may take up to 15 minutes and longer if queued. Permissioning access to an existing cluster in contrast happens in 1 or 2 minutes depending on the number of concurrent requests. You need an IBM Cloud account to access your cluster, If you do not have an IBM Cloud account yet, register at https://cloud.ibm.com/registration , Or find instructions to create a new IBM Cloud account here , To grant a cluster, You need to be given a URL to submit your grant cluster request, Open the URL to grant a cluster, e.g. https://<workshop>.mybluemix.net , The grant cluster URL should open the following page, Log in to this IBM Cloud account using the lab key given to you by the instructor and your IBM Id to access your IBM Cloud account, Instructions will ask to Log in to this IBM Cloud account When you click the link to log in to the IBM Cloud account, the IBM Cloud overview page will load with an overview of all resources on the account. In the top right, you will see an active account listed. The active account should be the account on which the cluster is created, which is not your personal account. Click the account dropdown if you need to change the active account. Navigate to Clusters, And select the cluster assigned to you... Details for your cluster will load, Go to the Access menu item in the left navigation column, Follow the instructions to access your cluster from the client, Optionally, you can use the IBM Cloud Shell at https://shell.cloud.ibm.com/ to check access. The cloud shell is attached to your IBM Id. It might take a few moments to create the instance and a new session,","title":"Grant Kubernetes Cluster"},{"location":"generatedContent/workshop-setup/docs/GRANTCLUSTER/#grant-cluster","text":"","title":"Grant Cluster"},{"location":"generatedContent/workshop-setup/docs/GRANTCLUSTER/#ibm-kubernetes-service-iks-and-redhat-openshift-kubernetes-service-roks","text":"The grant cluster method to get access to a Kubernetes cluster will assign access permissions to a cluster or namespace in a cluster that was created prior to the request. Creating a cluster and provisioning the VMs and other resources and deploying the tools may take up to 15 minutes and longer if queued. Permissioning access to an existing cluster in contrast happens in 1 or 2 minutes depending on the number of concurrent requests. You need an IBM Cloud account to access your cluster, If you do not have an IBM Cloud account yet, register at https://cloud.ibm.com/registration , Or find instructions to create a new IBM Cloud account here , To grant a cluster, You need to be given a URL to submit your grant cluster request, Open the URL to grant a cluster, e.g. https://<workshop>.mybluemix.net , The grant cluster URL should open the following page, Log in to this IBM Cloud account using the lab key given to you by the instructor and your IBM Id to access your IBM Cloud account, Instructions will ask to Log in to this IBM Cloud account When you click the link to log in to the IBM Cloud account, the IBM Cloud overview page will load with an overview of all resources on the account. In the top right, you will see an active account listed. The active account should be the account on which the cluster is created, which is not your personal account. Click the account dropdown if you need to change the active account. Navigate to Clusters, And select the cluster assigned to you... Details for your cluster will load, Go to the Access menu item in the left navigation column, Follow the instructions to access your cluster from the client, Optionally, you can use the IBM Cloud Shell at https://shell.cloud.ibm.com/ to check access. The cloud shell is attached to your IBM Id. It might take a few moments to create the instance and a new session,","title":"IBM Kubernetes Service (IKS) and RedHat OpenShift Kubernetes Service (ROKS)"},{"location":"generatedContent/workshop-setup/docs/HELM/","text":"Helm v3 \u00b6 Refer to the Helm install docs for more details. To install Helm v3, run the following commands, In the Cloud Shell , download and unzip Helm v3.2. cd $HOME wget https://get.helm.sh/helm-v3.2.0-linux-amd64.tar.gz tar -zxvf helm-v3.2.0-linux-amd64.tar.gz Make Helm v3 CLI available in your PATH environment variable. echo 'export PATH=$HOME/linux-amd64:$PATH' > $HOME/.bash_profile source $HOME/.bash_profile Verify Helm v3 installation. helm version --short outputs, $ helm version --short v3.2.0+ge11b7ce","title":"Helm v3"},{"location":"generatedContent/workshop-setup/docs/HELM/#helm-v3","text":"Refer to the Helm install docs for more details. To install Helm v3, run the following commands, In the Cloud Shell , download and unzip Helm v3.2. cd $HOME wget https://get.helm.sh/helm-v3.2.0-linux-amd64.tar.gz tar -zxvf helm-v3.2.0-linux-amd64.tar.gz Make Helm v3 CLI available in your PATH environment variable. echo 'export PATH=$HOME/linux-amd64:$PATH' > $HOME/.bash_profile source $HOME/.bash_profile Verify Helm v3 installation. helm version --short outputs, $ helm version --short v3.2.0+ge11b7ce","title":"Helm v3"},{"location":"generatedContent/workshop-setup/docs/JENKINS/","text":"Jenkins \u00b6 Pre-requirements \u00b6 OpenShift 4.x cluster Setup \u00b6 From the IBM Cloud cluster dashboard, click the OpenShift web console button, First we need to create a new project named jenkins to deploy the Jenkins service to, From the terminal, oc new-project jenkins outputs, $ oc new-project jenkins Now using project \"jenkins\" on server \"https://c107-e.us-south.containers.cloud.ibm.com:31608\". Or in the Openshift web console, go to Home > Projects , Click Create Project For Name enter jenkins , for Display Name enter jenkins , and for Description enter jenkins , Click Create , Go to Operators > OperatorHub , For Filter by keyword enter Jenkins , Select the Jenkins Operator provided by Red Hat , labeled community , Click Continue to Show Community Operator , Review the operator information, and click Install , In the Install Operator window, in the Update Channel section, select alpha under Update Channel , choose A specific namespace in the cluster and in the Installed Namespace section, select the project jenkins from the dropdown, select Automatic under Approval Strategy , Click Install , The Installed Operators page will load, wait until the Jenkins Operator has a Status of Succeeded , Click the installed operator linked Name of Jenkins Operator , In the Provided APIs section, click the Create Instance link in the Jenkins panel, In the Create Jenkins window, select Form View or YAML View for the new Jenkins instance, change the metadata.name to my-jenkins , accept all other specifications, Click Create , Go to Networking > Routes , and look for a new Route jenkins-my-jenkins , Click the link for jenkins-my-jenkins route in the Location column, A route to your Jenkins instance opens in a new browser window or tab, If your page loads with a Application is not available warning, your Jenkins instance is still being deployed and you need to wait a little longer, keep trying until the Jenkins page loads, You can see the progress of the Jenkins startup, by browsing to the Pods of the Deployment of the Jenkins instance that is being created, Click Log in with OpenShift , Click Allow selected permissions , Welcome to Jenkins ! Configure Jenkins Go to Jenkins > Manage Jenkins > Global Tool Configuration, Go to the Maven section, Click Maven Installations , If no Maven installer is configured, click Add Maven , Configure the Name to be maven , check the option Install automatically and select version 3.6.3 , Click Save,","title":"Jenkins"},{"location":"generatedContent/workshop-setup/docs/JENKINS/#jenkins","text":"","title":"Jenkins"},{"location":"generatedContent/workshop-setup/docs/JENKINS/#pre-requirements","text":"OpenShift 4.x cluster","title":"Pre-requirements"},{"location":"generatedContent/workshop-setup/docs/JENKINS/#setup","text":"From the IBM Cloud cluster dashboard, click the OpenShift web console button, First we need to create a new project named jenkins to deploy the Jenkins service to, From the terminal, oc new-project jenkins outputs, $ oc new-project jenkins Now using project \"jenkins\" on server \"https://c107-e.us-south.containers.cloud.ibm.com:31608\". Or in the Openshift web console, go to Home > Projects , Click Create Project For Name enter jenkins , for Display Name enter jenkins , and for Description enter jenkins , Click Create , Go to Operators > OperatorHub , For Filter by keyword enter Jenkins , Select the Jenkins Operator provided by Red Hat , labeled community , Click Continue to Show Community Operator , Review the operator information, and click Install , In the Install Operator window, in the Update Channel section, select alpha under Update Channel , choose A specific namespace in the cluster and in the Installed Namespace section, select the project jenkins from the dropdown, select Automatic under Approval Strategy , Click Install , The Installed Operators page will load, wait until the Jenkins Operator has a Status of Succeeded , Click the installed operator linked Name of Jenkins Operator , In the Provided APIs section, click the Create Instance link in the Jenkins panel, In the Create Jenkins window, select Form View or YAML View for the new Jenkins instance, change the metadata.name to my-jenkins , accept all other specifications, Click Create , Go to Networking > Routes , and look for a new Route jenkins-my-jenkins , Click the link for jenkins-my-jenkins route in the Location column, A route to your Jenkins instance opens in a new browser window or tab, If your page loads with a Application is not available warning, your Jenkins instance is still being deployed and you need to wait a little longer, keep trying until the Jenkins page loads, You can see the progress of the Jenkins startup, by browsing to the Pods of the Deployment of the Jenkins instance that is being created, Click Log in with OpenShift , Click Allow selected permissions , Welcome to Jenkins ! Configure Jenkins Go to Jenkins > Manage Jenkins > Global Tool Configuration, Go to the Maven section, Click Maven Installations , If no Maven installer is configured, click Add Maven , Configure the Name to be maven , check the option Install automatically and select version 3.6.3 , Click Save,","title":"Setup"},{"location":"generatedContent/workshop-setup/docs/MKDOCS/","text":"MkDocs \u00b6 Installation \u00b6 Go to the Installation instructions for MkDocs, alias python=python3 python --version alias pip=pip3 pip --version pip install --upgrade pip pip install mkdocs mkdocs --version To install Material for MkDocs go to the Getting Started instructions, pip install mkdocs-material In the repository root directory, add a new ~/README.md file, this is the landing page for the Github repository different from the MkDocs landing page. MkDocs assumes its root to be the docs folder. vi README.md Migration \u00b6 Get repository, ORG=<github_org> REPO=<repository> git clone https://github.com/$ORG/$REPO.git cd $REPO When converting an existing repository from Gitbook, rename workshop to docs , mv workshop docs For a repository without Gitbook support, create a new directory docs , mkdir docs When converting from Gitbook, edit .gitbook.yaml and change workshop to docs references, sed -i \"\" 's/workshop/docs/' .gitbook.yaml Skip this step for repositories with a docs/README.md file present! If no file docs/README.md exists, add one. The docs/README.md is the landing page for the MkDocs documentation, cat > docs/README.md <<EOF # $REPO # # About EOF Create a new file .github/workflows/ci.yml to configure a Github action, mkdir -p .github/workflows wget -O .github/workflows/ci.yml https://raw.githubusercontent.com/IBM/workshop-setup/master/.github/workflows/ci.yml In the root folder of your project, create a new file ~/mkdocs.yml and add the following content to the new file, cat > mkdocs.yml <<EOF # Project information site_name: <SITE_NAME> site_url: https://ibm.github.io/<REPO_NAME> site_author: IBM Developer # Repository repo_name: <REPO_NAME> repo_url: https://github.com/ibm/<REPO_NAME> edit_uri: edit/master/docs # Navigation nav: - Welcome: - About the workshop: README.md - Workshop: - Lab 1: lab1.md - Lab 2: lab2.md - References: - Additional resources: references/RESOURCES.md - Contributors: references/CONTRIBUTORS.md # # DO NOT CHANGE BELOW THIS LINE # Copyright copyright: Copyright &copy; 2020 IBM Developer # Theme theme: name: material font: text: IBM Plex Sans code: IBM Plex Mono icon: logo: material/library features: - navigation.tabs # - navigation.instant palette: scheme: default primary: blue accent: blue # Plugins plugins: - search # Customization extra: social: - icon: fontawesome/brands/github link: https://github.com/ibm - icon: fontawesome/brands/twitter link: https://twitter.com/ibmdeveloper - icon: fontawesome/brands/linkedin link: https://www.linkedin.com/company/ibm/ - icon: fontawesome/brands/youtube link: https://www.youtube.com/user/developerworks - icon: fontawesome/brands/dev link: https://dev.to/ibmdeveloper # Extensions markdown_extensions: - abbr - admonition - attr_list - def_list - footnotes - meta - toc: permalink: true - pymdownx.arithmatex: generic: true - pymdownx.betterem: smart_enable: all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji: emoji_index: !!python/name:materialx.emoji.twemoji emoji_generator: !!python/name:materialx.emoji.to_svg - pymdownx.highlight - pymdownx.inlinehilite - pymdownx.keys - pymdownx.mark - pymdownx.smartsymbols - pymdownx.snippets: check_paths: true - pymdownx.superfences: custom_fences: - name: mermaid class: mermaid format: !!python/name:pymdownx.superfences.fence_code_format - pymdownx.tabbed - pymdownx.tasklist: custom_checkbox: true - pymdownx.tilde EOF Edit the above file mkdocs.yml , change: <SITE_NAME> <REPO_NAME> when converting from Gitbook: copy the navigation links from workshop/SUMMARY.md to the Navigation section and copy-edit the links for the navigation, sed -i \"\" \"s/<SITE_NAME>/$REPO/\" mkdocs.yml sed -i \"\" \"s/<REPO_NAME>/$REPO/\" mkdocs.yml The above navigation example includes two example items, you need to add the files docs/lab1.md and docs/lab2.md to enable the examples, echo '# Lab One' > docs/lab1.md echo '# Lab Two' > docs/lab2.md But for existing content, you need to overwrite the navigation. If your repository has an existing Gitbook, you can copy the navigation in the docs/SUMMARY.md file, to the # Navigation nav: section in mkdocs.yml , Add a file .gitignore with the following exceptions, echo \"\\n# MkDocs\\nsite/\\n\" >> .gitignore Add a new file ~/markdownlint.json with the following rule, cat > markdownlint.json <<EOF { \"line-length\": false } EOF Add a new file .markdownlintignore , when converting from Gitbook, add the following exceptions, cat > .markdownlintignore <<EOF workshop/SUMMARY.md workshop/README.md EOF When migrating from Gitbook , if a .travis.yml file already exists, replace reference to workshop by docs , sed -i \"\" 's/workshop/docs/' .travis.yml Add a new file .travis.yml , cat > .travis.yml <<EOF --- language: node_js node_js: 10 before_script: - npm install markdownlint-cli script: - markdownlint -c .markdownlint.json docs EOF If you don't have a LICENSE file yet, download the LICENSE file, wget -O LICENSE https://raw.githubusercontent.com/IBM/workshop-setup/master/LICENSE Test the docs locally with the command mkdocs serve , Review the WARNING output, fix where necessary, Create a new repository in Github, Initialize the local repository, add all files, commit, add the remote, and push changes, echo \"# test1\" >> README.md git init git add . git commit -m \"first commit\" git branch -M main git remote add origin https://github.com/remkohdev/test1.git git push -u origin main Github pages use a branch called gh-pages , this branch is automatically created if it does not exist by the Github action that is defined in the .github/workflows/ci.yml , Github also needs to resolve the domain name and URI to the repository, and point it to the correct branch of the Github page for the repo. To configure this, in your repo, go to Settings, scroll down to GitHub Pages section and from the Source dropdown, select the branch gh-pages , accept the default /root and click Save . Your MkDocs branch should now be live. Whenever you push changes to your main branch, the Github action to deploy MkDocs will be triggered again, Add an update to your README.md, vi README.md Push changes, git add . git commit -m \"second commit\" git push Check your actions at https://github.com/<ORG>/<REPO>/actions . Export to PDF \u00b6 There are several plugins to export mkdocs to PDF, mkdocs-pdf-export , MkPdf , Install the plugin, Enable the plugin by adding the plugin line to the plugins list in mkdocs.yml , to use the pdf-export plugin, add, plugins: - pdf-export: verbose: true media_type: print enabled_if_env: ENABLE_PDF_EXPORT combined: true Run mkdocs build , which will create a pdf directory with the combined PDF, e.g. to run pdf-export , ENABLE_PDF_EXPORT=1 mkdocs build","title":"MkDocs"},{"location":"generatedContent/workshop-setup/docs/MKDOCS/#mkdocs","text":"","title":"MkDocs"},{"location":"generatedContent/workshop-setup/docs/MKDOCS/#installation","text":"Go to the Installation instructions for MkDocs, alias python=python3 python --version alias pip=pip3 pip --version pip install --upgrade pip pip install mkdocs mkdocs --version To install Material for MkDocs go to the Getting Started instructions, pip install mkdocs-material In the repository root directory, add a new ~/README.md file, this is the landing page for the Github repository different from the MkDocs landing page. MkDocs assumes its root to be the docs folder. vi README.md","title":"Installation"},{"location":"generatedContent/workshop-setup/docs/MKDOCS/#migration","text":"Get repository, ORG=<github_org> REPO=<repository> git clone https://github.com/$ORG/$REPO.git cd $REPO When converting an existing repository from Gitbook, rename workshop to docs , mv workshop docs For a repository without Gitbook support, create a new directory docs , mkdir docs When converting from Gitbook, edit .gitbook.yaml and change workshop to docs references, sed -i \"\" 's/workshop/docs/' .gitbook.yaml Skip this step for repositories with a docs/README.md file present! If no file docs/README.md exists, add one. The docs/README.md is the landing page for the MkDocs documentation, cat > docs/README.md <<EOF # $REPO # # About EOF Create a new file .github/workflows/ci.yml to configure a Github action, mkdir -p .github/workflows wget -O .github/workflows/ci.yml https://raw.githubusercontent.com/IBM/workshop-setup/master/.github/workflows/ci.yml In the root folder of your project, create a new file ~/mkdocs.yml and add the following content to the new file, cat > mkdocs.yml <<EOF # Project information site_name: <SITE_NAME> site_url: https://ibm.github.io/<REPO_NAME> site_author: IBM Developer # Repository repo_name: <REPO_NAME> repo_url: https://github.com/ibm/<REPO_NAME> edit_uri: edit/master/docs # Navigation nav: - Welcome: - About the workshop: README.md - Workshop: - Lab 1: lab1.md - Lab 2: lab2.md - References: - Additional resources: references/RESOURCES.md - Contributors: references/CONTRIBUTORS.md # # DO NOT CHANGE BELOW THIS LINE # Copyright copyright: Copyright &copy; 2020 IBM Developer # Theme theme: name: material font: text: IBM Plex Sans code: IBM Plex Mono icon: logo: material/library features: - navigation.tabs # - navigation.instant palette: scheme: default primary: blue accent: blue # Plugins plugins: - search # Customization extra: social: - icon: fontawesome/brands/github link: https://github.com/ibm - icon: fontawesome/brands/twitter link: https://twitter.com/ibmdeveloper - icon: fontawesome/brands/linkedin link: https://www.linkedin.com/company/ibm/ - icon: fontawesome/brands/youtube link: https://www.youtube.com/user/developerworks - icon: fontawesome/brands/dev link: https://dev.to/ibmdeveloper # Extensions markdown_extensions: - abbr - admonition - attr_list - def_list - footnotes - meta - toc: permalink: true - pymdownx.arithmatex: generic: true - pymdownx.betterem: smart_enable: all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji: emoji_index: !!python/name:materialx.emoji.twemoji emoji_generator: !!python/name:materialx.emoji.to_svg - pymdownx.highlight - pymdownx.inlinehilite - pymdownx.keys - pymdownx.mark - pymdownx.smartsymbols - pymdownx.snippets: check_paths: true - pymdownx.superfences: custom_fences: - name: mermaid class: mermaid format: !!python/name:pymdownx.superfences.fence_code_format - pymdownx.tabbed - pymdownx.tasklist: custom_checkbox: true - pymdownx.tilde EOF Edit the above file mkdocs.yml , change: <SITE_NAME> <REPO_NAME> when converting from Gitbook: copy the navigation links from workshop/SUMMARY.md to the Navigation section and copy-edit the links for the navigation, sed -i \"\" \"s/<SITE_NAME>/$REPO/\" mkdocs.yml sed -i \"\" \"s/<REPO_NAME>/$REPO/\" mkdocs.yml The above navigation example includes two example items, you need to add the files docs/lab1.md and docs/lab2.md to enable the examples, echo '# Lab One' > docs/lab1.md echo '# Lab Two' > docs/lab2.md But for existing content, you need to overwrite the navigation. If your repository has an existing Gitbook, you can copy the navigation in the docs/SUMMARY.md file, to the # Navigation nav: section in mkdocs.yml , Add a file .gitignore with the following exceptions, echo \"\\n# MkDocs\\nsite/\\n\" >> .gitignore Add a new file ~/markdownlint.json with the following rule, cat > markdownlint.json <<EOF { \"line-length\": false } EOF Add a new file .markdownlintignore , when converting from Gitbook, add the following exceptions, cat > .markdownlintignore <<EOF workshop/SUMMARY.md workshop/README.md EOF When migrating from Gitbook , if a .travis.yml file already exists, replace reference to workshop by docs , sed -i \"\" 's/workshop/docs/' .travis.yml Add a new file .travis.yml , cat > .travis.yml <<EOF --- language: node_js node_js: 10 before_script: - npm install markdownlint-cli script: - markdownlint -c .markdownlint.json docs EOF If you don't have a LICENSE file yet, download the LICENSE file, wget -O LICENSE https://raw.githubusercontent.com/IBM/workshop-setup/master/LICENSE Test the docs locally with the command mkdocs serve , Review the WARNING output, fix where necessary, Create a new repository in Github, Initialize the local repository, add all files, commit, add the remote, and push changes, echo \"# test1\" >> README.md git init git add . git commit -m \"first commit\" git branch -M main git remote add origin https://github.com/remkohdev/test1.git git push -u origin main Github pages use a branch called gh-pages , this branch is automatically created if it does not exist by the Github action that is defined in the .github/workflows/ci.yml , Github also needs to resolve the domain name and URI to the repository, and point it to the correct branch of the Github page for the repo. To configure this, in your repo, go to Settings, scroll down to GitHub Pages section and from the Source dropdown, select the branch gh-pages , accept the default /root and click Save . Your MkDocs branch should now be live. Whenever you push changes to your main branch, the Github action to deploy MkDocs will be triggered again, Add an update to your README.md, vi README.md Push changes, git add . git commit -m \"second commit\" git push Check your actions at https://github.com/<ORG>/<REPO>/actions .","title":"Migration"},{"location":"generatedContent/workshop-setup/docs/MKDOCS/#export-to-pdf","text":"There are several plugins to export mkdocs to PDF, mkdocs-pdf-export , MkPdf , Install the plugin, Enable the plugin by adding the plugin line to the plugins list in mkdocs.yml , to use the pdf-export plugin, add, plugins: - pdf-export: verbose: true media_type: print enabled_if_env: ENABLE_PDF_EXPORT combined: true Run mkdocs build , which will create a pdf directory with the combined PDF, e.g. to run pdf-export , ENABLE_PDF_EXPORT=1 mkdocs build","title":"Export to PDF"},{"location":"generatedContent/workshop-setup/docs/NEWACCOUNT/","text":"Create IBM Cloud ID / Account \u00b6 To create a new account, follow the steps below, Open a web browser and go to the IBM Cloud Sign up page In the Create an account window, enter your company email id and the password you would like to use. Click the Next button. The Verify email section will inform you that a verification code was sent to your email. Switch to your email provider to retrieve the verification code. Then enter the verification code in the Verify email section, and click the Next button. Enter your first name, last name and country in the Personal information section and click the Next button. Click the Create account button. After your account is created, review the IBM Privacy Statement . Then scroll down and click the Proceed button to acknowledge the privacy statement. You are now ready to login to the IBM Cloud. Open a web browser to the IBM Cloud console . If prompted, enter your IBM Id (the email ID you used to create the account above) followed by your password to login. The IBM Cloud dashboard page should load. You have successfully registered a new IBM Cloud account.","title":"Create IBM Cloud ID / Account"},{"location":"generatedContent/workshop-setup/docs/NEWACCOUNT/#create-ibm-cloud-id-account","text":"To create a new account, follow the steps below, Open a web browser and go to the IBM Cloud Sign up page In the Create an account window, enter your company email id and the password you would like to use. Click the Next button. The Verify email section will inform you that a verification code was sent to your email. Switch to your email provider to retrieve the verification code. Then enter the verification code in the Verify email section, and click the Next button. Enter your first name, last name and country in the Personal information section and click the Next button. Click the Create account button. After your account is created, review the IBM Privacy Statement . Then scroll down and click the Proceed button to acknowledge the privacy statement. You are now ready to login to the IBM Cloud. Open a web browser to the IBM Cloud console . If prompted, enter your IBM Id (the email ID you used to create the account above) followed by your password to login. The IBM Cloud dashboard page should load. You have successfully registered a new IBM Cloud account.","title":"Create IBM Cloud ID / Account"},{"location":"generatedContent/workshop-setup/docs/OPENLABS/","text":"Access OpenShift Cluster at OpenLabs \u00b6 These instructions will walk you through how you can get access to a 2 node OpenShift cluster on IBM Cloud through IBM OpenLabs. This cluster will only be available for 2 hours and then will be deleted. Go to IBM OpenLabs: https://developer.ibm.com/openlabs/openshift Find the lab in the Hands on Labs section called Lab 6: Bring Your Own App You will be asked to sign into IBM Cloud if you aren't already or you can register for a new account. Once you sign in the lab will automatically continue provisioning. You may see messages like Allocating VM and so on. When that is done you should be taken to a new page that has documentation on the left half of the screen along with a terminal environment on the right. If not, click on the Lab 6: Bring Your Own App button again. To access your new OpenShift Cluster, click on the tab on the left side of the page labeled Quick Links and Common Commands From this page you can access the OpenShift Web Console and find instructions on how to log in to the cluster in the terminal with the oc cli tool.","title":"Access OpenShift Cluster at OpenLabs"},{"location":"generatedContent/workshop-setup/docs/OPENLABS/#access-openshift-cluster-at-openlabs","text":"These instructions will walk you through how you can get access to a 2 node OpenShift cluster on IBM Cloud through IBM OpenLabs. This cluster will only be available for 2 hours and then will be deleted. Go to IBM OpenLabs: https://developer.ibm.com/openlabs/openshift Find the lab in the Hands on Labs section called Lab 6: Bring Your Own App You will be asked to sign into IBM Cloud if you aren't already or you can register for a new account. Once you sign in the lab will automatically continue provisioning. You may see messages like Allocating VM and so on. When that is done you should be taken to a new page that has documentation on the left half of the screen along with a terminal environment on the right. If not, click on the Lab 6: Bring Your Own App button again. To access your new OpenShift Cluster, click on the tab on the left side of the page labeled Quick Links and Common Commands From this page you can access the OpenShift Web Console and find instructions on how to log in to the cluster in the terminal with the oc cli tool.","title":"Access OpenShift Cluster at OpenLabs"},{"location":"generatedContent/workshop-setup/docs/PAYASYOUGO/","text":"Upgrade to Pay-As-You-Go Account \u00b6 Login to your IBM Cloud account , Go to account settings , Click the Add credit card button, to create the Pay-As-You-Go account, which will unlock the full catalog, Select the Account type , choose between Company and Personal . For the setup, I will choose Personal . Click Next , Enter your Billing Information , click Next , Enter your Payment information , click Next , Acknowledge that your card will not be charged, but a $1.00 hold will be placed while authorizing the card, Click Upgrade Account , You will receive $200 credit, in case you want to use paid services, Click Done , Under Account settings you should now see Account Type as Pay-As-You-Go . Go to the IBM Cloud Catalog and filter Free and Lite services . Free and Lite Services \u00b6 At the time of writing this setup 115 free and lite services were listed, among other: Analytics Engine, API Connect, API Gateway, App Connect, App ID, Auto Scale for VPC, Certificate Manager, Cloudant, Code Engine, Container Registry, Continuous Delivery, DB2, Direct Link Connect, Discovery, Event Streams with Kafka, Hyper Protect, LogDNA, Sysdig, IBM Cognos, Internet of Things Platform, Internet Services with CloudFlare, Key Protect, Knowledge Studio, Kubernetes Service, Language Translator, Load Balancer for VPC, Machine Learning, MQ, Natural Language Classifier, Natural Language Understanding, Object Storage, PagerDuty, Schematics with Terraform, Secrets Manager, Secure Gateway, Speech to Text, SQL Query, Object Storage with ANSI SQL, Streaming Analytics, Text to Speech, Tone Analyzer, Toolchain, Twilio, Visual Recognition, Voice Agent with Watson, Watson Assistant, Watson Knowledge Catalog, Watson OpenScale, Watson Studio,","title":"Upgrade to Pay-As-You-Go Account"},{"location":"generatedContent/workshop-setup/docs/PAYASYOUGO/#upgrade-to-pay-as-you-go-account","text":"Login to your IBM Cloud account , Go to account settings , Click the Add credit card button, to create the Pay-As-You-Go account, which will unlock the full catalog, Select the Account type , choose between Company and Personal . For the setup, I will choose Personal . Click Next , Enter your Billing Information , click Next , Enter your Payment information , click Next , Acknowledge that your card will not be charged, but a $1.00 hold will be placed while authorizing the card, Click Upgrade Account , You will receive $200 credit, in case you want to use paid services, Click Done , Under Account settings you should now see Account Type as Pay-As-You-Go . Go to the IBM Cloud Catalog and filter Free and Lite services .","title":"Upgrade to Pay-As-You-Go Account"},{"location":"generatedContent/workshop-setup/docs/PAYASYOUGO/#free-and-lite-services","text":"At the time of writing this setup 115 free and lite services were listed, among other: Analytics Engine, API Connect, API Gateway, App Connect, App ID, Auto Scale for VPC, Certificate Manager, Cloudant, Code Engine, Container Registry, Continuous Delivery, DB2, Direct Link Connect, Discovery, Event Streams with Kafka, Hyper Protect, LogDNA, Sysdig, IBM Cognos, Internet of Things Platform, Internet Services with CloudFlare, Key Protect, Knowledge Studio, Kubernetes Service, Language Translator, Load Balancer for VPC, Machine Learning, MQ, Natural Language Classifier, Natural Language Understanding, Object Storage, PagerDuty, Schematics with Terraform, Secrets Manager, Secure Gateway, Speech to Text, SQL Query, Object Storage with ANSI SQL, Streaming Analytics, Text to Speech, Tone Analyzer, Toolchain, Twilio, Visual Recognition, Voice Agent with Watson, Watson Assistant, Watson Knowledge Catalog, Watson OpenScale, Watson Studio,","title":"Free and Lite Services"},{"location":"generatedContent/workshop-setup/docs/ROKS/","text":"Connect to RedHat OpenShift Kubernetes Service (ROKS) \u00b6 Login to IBM Cloud \u00b6 To login to IBM Cloud, Go to https://cloud.ibm.com in your browser and login. Make sure that you are in the correct account#. Note: you may not have access to your OpenShift cluster if you are not in the right account#. Shell \u00b6 Most of the labs are run using CLI commands. The IBM Cloud Shell available at https://shell.cloud.ibm.com is preconfigured with the full IBM Cloud CLI and tons of plug-ins and tools that you can use to manage apps, resources, and infrastructure. Another great online shell is available via the Theia - Cloud IDE (With OpenShift) at https://labs.cognitiveclass.ai . The Cognitive Class shell comes with a Docker Engine and Helm v3 at the time of writing. Login to OpenShift \u00b6 In a new browser tab, go to https://cloud.ibm.com/kubernetes/clusters?platformType=openshift . Make sure the account holding the cluster is selected, Select your cluster instance and open it. Click OpenShift web console button on the top. Click on your username in the upper right and select Copy Login Command option. Click the Display Token link. Copy the contents of the field Log in with this token to the clipboard. It provides a login command with a valid token for your username. Go to the your shell terminal. Paste the oc login command in the IBM Cloud Shell terminal and run it. Verify you connect to the right cluster. oc get all oc get nodes -o wide ![oc get nodes](images/roks/cognitiveclass-get-nodes.png) Optionally, for convenience, set an environment variable for your cluster name. export CLUSTER_NAME=<your_cluster_name>","title":"Connect to RedHat OpenShift Kubernetes Service (ROKS)"},{"location":"generatedContent/workshop-setup/docs/ROKS/#connect-to-redhat-openshift-kubernetes-service-roks","text":"","title":"Connect to RedHat OpenShift Kubernetes Service (ROKS)"},{"location":"generatedContent/workshop-setup/docs/ROKS/#login-to-ibm-cloud","text":"To login to IBM Cloud, Go to https://cloud.ibm.com in your browser and login. Make sure that you are in the correct account#. Note: you may not have access to your OpenShift cluster if you are not in the right account#.","title":"Login to IBM Cloud"},{"location":"generatedContent/workshop-setup/docs/ROKS/#shell","text":"Most of the labs are run using CLI commands. The IBM Cloud Shell available at https://shell.cloud.ibm.com is preconfigured with the full IBM Cloud CLI and tons of plug-ins and tools that you can use to manage apps, resources, and infrastructure. Another great online shell is available via the Theia - Cloud IDE (With OpenShift) at https://labs.cognitiveclass.ai . The Cognitive Class shell comes with a Docker Engine and Helm v3 at the time of writing.","title":"Shell"},{"location":"generatedContent/workshop-setup/docs/ROKS/#login-to-openshift","text":"In a new browser tab, go to https://cloud.ibm.com/kubernetes/clusters?platformType=openshift . Make sure the account holding the cluster is selected, Select your cluster instance and open it. Click OpenShift web console button on the top. Click on your username in the upper right and select Copy Login Command option. Click the Display Token link. Copy the contents of the field Log in with this token to the clipboard. It provides a login command with a valid token for your username. Go to the your shell terminal. Paste the oc login command in the IBM Cloud Shell terminal and run it. Verify you connect to the right cluster. oc get all oc get nodes -o wide ![oc get nodes](images/roks/cognitiveclass-get-nodes.png) Optionally, for convenience, set an environment variable for your cluster name. export CLUSTER_NAME=<your_cluster_name>","title":"Login to OpenShift"},{"location":"generatedContent/workshop-setup/docs/S2I/","text":"Source-to-Image (S2I) \u00b6 To install s2i CLI tool, Download tar file. curl -s https://api.github.com/repos/openshift/source-to-image/releases/latest \\ | grep browser_download_url \\ | grep linux-amd64 \\ | cut -d '\"' -f 4 \\ | wget -qi - Unzip tar file sudo tar xvf source-to-image*.gz Make s2i CLI accessiblee. sudo mv s2i /usr/local/bin verify s2i version With that done, you can start the lab.","title":"Source-to-Image (S2I)"},{"location":"generatedContent/workshop-setup/docs/S2I/#source-to-image-s2i","text":"To install s2i CLI tool, Download tar file. curl -s https://api.github.com/repos/openshift/source-to-image/releases/latest \\ | grep browser_download_url \\ | grep linux-amd64 \\ | cut -d '\"' -f 4 \\ | wget -qi - Unzip tar file sudo tar xvf source-to-image*.gz Make s2i CLI accessiblee. sudo mv s2i /usr/local/bin verify s2i version With that done, you can start the lab.","title":"Source-to-Image (S2I)"},{"location":"generatedContent/workshop-setup/docs/VAULT/","text":"Vault \u00b6 Vault CLI \u00b6 See Vault Project Install Vault , or Hashicorp Install Vault . OSX \u00b6 Homebrew formula for Vault . brew install vault $ vault version Vault v1.6.3 ( 'b540be4b7ec48d0dd7512c8d8df9399d6bf84d76+CHANGES' ) Linux \u00b6 wget https://releases.hashicorp.com/vault/1.6.3/vault_1.6.3_linux_amd64.zip -P ./vault-1.6.3 cd vault-1.6.3 unzip vault_1.6.3_linux_amd64.zip cd .. chmod +x vault-1.6.3/vault echo \"export PATH= $( pwd ) /vault-1.6.3/: $PATH \" > $HOME /.bash_profile source $HOME /.bash_profile vault version","title":"Vault"},{"location":"generatedContent/workshop-setup/docs/VAULT/#vault","text":"","title":"Vault"},{"location":"generatedContent/workshop-setup/docs/VAULT/#vault-cli","text":"See Vault Project Install Vault , or Hashicorp Install Vault .","title":"Vault CLI"},{"location":"generatedContent/workshop-setup/docs/VAULT/#osx","text":"Homebrew formula for Vault . brew install vault $ vault version Vault v1.6.3 ( 'b540be4b7ec48d0dd7512c8d8df9399d6bf84d76+CHANGES' )","title":"OSX"},{"location":"generatedContent/workshop-setup/docs/VAULT/#linux","text":"wget https://releases.hashicorp.com/vault/1.6.3/vault_1.6.3_linux_amd64.zip -P ./vault-1.6.3 cd vault-1.6.3 unzip vault_1.6.3_linux_amd64.zip cd .. chmod +x vault-1.6.3/vault echo \"export PATH= $( pwd ) /vault-1.6.3/: $PATH \" > $HOME /.bash_profile source $HOME /.bash_profile vault version","title":"Linux"},{"location":"generatedContent/workshop-setup/docs/references/CONTRIBUTORS/","text":"Contributors \u00b6 Remko de Knikker Github: remkohdev ] Twitter: @remkohdev LinkedIn: remkohdev Medium: @remkohdev Steve Martinelli John Zaccone","title":"Contributors"},{"location":"generatedContent/workshop-setup/docs/references/CONTRIBUTORS/#contributors","text":"Remko de Knikker Github: remkohdev ] Twitter: @remkohdev LinkedIn: remkohdev Medium: @remkohdev Steve Martinelli John Zaccone","title":"Contributors"},{"location":"generatedContent/workshop-setup/docs/references/RESOURCES/","text":"Additional resources \u00b6 IBM Demos \u00b6 Collection: InfoSphere Information Server Tutorial: Transforming your data with IBM DataStage Redbooks \u00b6 IBM InfoSphere DataStage Data Flow and Job Design InfoSphere DataStage Parallel Framework Standard Practices Videos \u00b6 Video: Postal codes and part numbers (DataStage) Video: Calculate employee compensation (read from CSV) (DataStage and Gov. Catalog) Video: Banks have merged (DataStage and Gov. Catalog) Video: Groceries with Kafka (DataStage) Video: Find relationships between sales, employees, and customers (Information Analyzer) Video: Clean and analyze data (Governance Catalog)","title":"Additional resources"},{"location":"generatedContent/workshop-setup/docs/references/RESOURCES/#additional-resources","text":"","title":"Additional resources"},{"location":"generatedContent/workshop-setup/docs/references/RESOURCES/#ibm-demos","text":"Collection: InfoSphere Information Server Tutorial: Transforming your data with IBM DataStage","title":"IBM Demos"},{"location":"generatedContent/workshop-setup/docs/references/RESOURCES/#redbooks","text":"IBM InfoSphere DataStage Data Flow and Job Design InfoSphere DataStage Parallel Framework Standard Practices","title":"Redbooks"},{"location":"generatedContent/workshop-setup/docs/references/RESOURCES/#videos","text":"Video: Postal codes and part numbers (DataStage) Video: Calculate employee compensation (read from CSV) (DataStage and Gov. Catalog) Video: Banks have merged (DataStage and Gov. Catalog) Video: Groceries with Kafka (DataStage) Video: Find relationships between sales, employees, and customers (Information Analyzer) Video: Clean and analyze data (Governance Catalog)","title":"Videos"},{"location":"pre-work/","text":"Cloud Shell Configuration \u00b6 If you are running the labs using the IBM Cloud Shell, we need to complete a couple of setup steps before we proceed. This section is broken up into the following steps: Access the Cloud Shell Configure Kubectl Install Helm Version 3 1. Access the Cloud Shell \u00b6 If you do not already have it open, go ahead and open the Cloud shell. From the IBM Cloud Home Page , select the terminal icon in the upper lefthand menu. Note: Ensure the cloud shell is using the same account where your cluster is provisioned. Ensure that the account name shown in the top right of the screen, next to Current account is the correct one. 2. Configure Kubectl \u00b6 If you have not setup your kubectl to access your cluster, you can do so in the Cloud Shell. Run the ibmcloud ks clusters command to verify the terminal and setup for access to the cluster ibmcloud ks clusters Configure the kubectl cli available within the terminal for access to your cluster. If you previously stored your cluster name to an environment variable, use that (ie. $CLUSTER_NAME ), otherwise copy and paste your cluster name from the previous commands output to the $CLUSTER_NAME portion below. ibmcloud ks cluster config --cluster $CLUSTER_NAME Verify access to the Kubernetes API by getting the namespaces. kubectl get namespace You should see output similar to the following, if so, then your're ready to continue. NAME STATUS AGE default Active 125m ibm-cert-store Active 121m ibm-system Active 124m kube-node-lease Active 125m kube-public Active 125m kube-system Active 125m Note: You can also validate that your kubectl is configured by looking at its context. kubectl config current-context 3. Alias Helm Version 3 \u00b6 The Cloud Shell comes with both Helm 2 and Helm 3 versions. To make sure we use the Helm Version 3 variant, we will create an alias. Run the following commands to install Helm Version 3: alias helm = helm3 Check the helm version by running the following command: helm version --short The result is that you should have Helm Version 3 installed. $ helm version --short v3.2.1+gfe51cd1","title":"Cloud Shell"},{"location":"pre-work/#cloud-shell-configuration","text":"If you are running the labs using the IBM Cloud Shell, we need to complete a couple of setup steps before we proceed. This section is broken up into the following steps: Access the Cloud Shell Configure Kubectl Install Helm Version 3","title":"Cloud Shell Configuration"},{"location":"pre-work/#1-access-the-cloud-shell","text":"If you do not already have it open, go ahead and open the Cloud shell. From the IBM Cloud Home Page , select the terminal icon in the upper lefthand menu. Note: Ensure the cloud shell is using the same account where your cluster is provisioned. Ensure that the account name shown in the top right of the screen, next to Current account is the correct one.","title":"1. Access the Cloud Shell"},{"location":"pre-work/#2-configure-kubectl","text":"If you have not setup your kubectl to access your cluster, you can do so in the Cloud Shell. Run the ibmcloud ks clusters command to verify the terminal and setup for access to the cluster ibmcloud ks clusters Configure the kubectl cli available within the terminal for access to your cluster. If you previously stored your cluster name to an environment variable, use that (ie. $CLUSTER_NAME ), otherwise copy and paste your cluster name from the previous commands output to the $CLUSTER_NAME portion below. ibmcloud ks cluster config --cluster $CLUSTER_NAME Verify access to the Kubernetes API by getting the namespaces. kubectl get namespace You should see output similar to the following, if so, then your're ready to continue. NAME STATUS AGE default Active 125m ibm-cert-store Active 121m ibm-system Active 124m kube-node-lease Active 125m kube-public Active 125m kube-system Active 125m Note: You can also validate that your kubectl is configured by looking at its context. kubectl config current-context","title":"2. Configure Kubectl"},{"location":"pre-work/#3-alias-helm-version-3","text":"The Cloud Shell comes with both Helm 2 and Helm 3 versions. To make sure we use the Helm Version 3 variant, we will create an alias. Run the following commands to install Helm Version 3: alias helm = helm3 Check the helm version by running the following command: helm version --short The result is that you should have Helm Version 3 installed. $ helm version --short v3.2.1+gfe51cd1","title":"3. Alias Helm Version 3"},{"location":"resources/ADMIN/","text":"Admin Guide \u00b6 This section is comprised of the following steps: Instructor Step 1. Instructor Step \u00b6 Things specific to instructors can go here.","title":"Admin Guide"},{"location":"resources/ADMIN/#admin-guide","text":"This section is comprised of the following steps: Instructor Step","title":"Admin Guide"},{"location":"resources/ADMIN/#1-instructor-step","text":"Things specific to instructors can go here.","title":"1. Instructor Step"},{"location":"resources/CONTRIBUTORS/","text":"Contributors \u00b6 Remko de Knikker \u00b6 Github: remkohdev Twitter: @remkohdev LinkedIn: remkohdev Medium: @remkohdev Steve Martinelli \u00b6 Github: stevemar Twitter: @stevebot LinkedIn: stevemar","title":"Contributors"},{"location":"resources/CONTRIBUTORS/#contributors","text":"","title":"Contributors"},{"location":"resources/CONTRIBUTORS/#remko-de-knikker","text":"Github: remkohdev Twitter: @remkohdev LinkedIn: remkohdev Medium: @remkohdev","title":"Remko de Knikker"},{"location":"resources/CONTRIBUTORS/#steve-martinelli","text":"Github: stevemar Twitter: @stevebot LinkedIn: stevemar","title":"Steve Martinelli"},{"location":"resources/MKDOCS/","text":"mkdocs examples \u00b6 This page includes a few neat tricks that you can do with mkdocs . For a complete list of examples visit the mkdocs documentation . Code \u00b6 print ( \"hello world!\" ) Code with line numbers \u00b6 1 2 3 4 5 def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Code with highlights \u00b6 def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Code with tabs \u00b6 Tab Header #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } Another Tab Header #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } More tabs \u00b6 Windows If on windows download the Win32.zip file and install it. MacOS Run brew install foo . Linux Run apt-get install foo . Checklists \u00b6 Lorem ipsum dolor sit amet, consectetur adipiscing elit Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst Add a button \u00b6 Launch the lab Visit IBM Developer Sign up! Call outs \u00b6 Tip You can use note , abstract , info , tip , success , question warning , failure , danger , bug , quote or example . Note A note. Abstract An abstract. Info Some info. Success A success. Question A question. Warning A warning. Danger A danger. Example A example. Bug A bug. Call outs with code \u00b6 Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Nunc eu odio eleifend, blandit leo a, volutpat sapien. Phasellus posuere in sem ut cursus. Nullam sit amet tincidunt ipsum, sit amet elementum turpis. Etiam ipsum quam, mattis in purus vitae, lacinia fermentum enim. Formatting \u00b6 In addition to the usual italics , and bold there is now support for: highlighted underlined strike-through Tables \u00b6 OS or Application Username Password Windows VM Administrator foo Linux VM root bar Emojis \u00b6 Yes, these work. Images \u00b6 Nunc eu odio eleifend, blandit leo a, volutpat sapien right align image \u00b6 Nunc eu odio eleifend, blandit leo a, volutpat sapien","title":"mkdocs examples"},{"location":"resources/MKDOCS/#mkdocs-examples","text":"This page includes a few neat tricks that you can do with mkdocs . For a complete list of examples visit the mkdocs documentation .","title":"mkdocs examples"},{"location":"resources/MKDOCS/#code","text":"print ( \"hello world!\" )","title":"Code"},{"location":"resources/MKDOCS/#code-with-line-numbers","text":"1 2 3 4 5 def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ]","title":"Code with line numbers"},{"location":"resources/MKDOCS/#code-with-highlights","text":"def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ]","title":"Code with highlights"},{"location":"resources/MKDOCS/#code-with-tabs","text":"Tab Header #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } Another Tab Header #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; }","title":"Code with tabs"},{"location":"resources/MKDOCS/#more-tabs","text":"Windows If on windows download the Win32.zip file and install it. MacOS Run brew install foo . Linux Run apt-get install foo .","title":"More tabs"},{"location":"resources/MKDOCS/#checklists","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst","title":"Checklists"},{"location":"resources/MKDOCS/#add-a-button","text":"Launch the lab Visit IBM Developer Sign up!","title":"Add a button"},{"location":"resources/MKDOCS/#call-outs","text":"Tip You can use note , abstract , info , tip , success , question warning , failure , danger , bug , quote or example . Note A note. Abstract An abstract. Info Some info. Success A success. Question A question. Warning A warning. Danger A danger. Example A example. Bug A bug.","title":"Call outs"},{"location":"resources/MKDOCS/#call-outs-with-code","text":"Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Nunc eu odio eleifend, blandit leo a, volutpat sapien. Phasellus posuere in sem ut cursus. Nullam sit amet tincidunt ipsum, sit amet elementum turpis. Etiam ipsum quam, mattis in purus vitae, lacinia fermentum enim.","title":"Call outs with code"},{"location":"resources/MKDOCS/#formatting","text":"In addition to the usual italics , and bold there is now support for: highlighted underlined strike-through","title":"Formatting"},{"location":"resources/MKDOCS/#tables","text":"OS or Application Username Password Windows VM Administrator foo Linux VM root bar","title":"Tables"},{"location":"resources/MKDOCS/#emojis","text":"Yes, these work.","title":"Emojis"},{"location":"resources/MKDOCS/#images","text":"Nunc eu odio eleifend, blandit leo a, volutpat sapien","title":"Images"},{"location":"resources/MKDOCS/#right-align-image","text":"Nunc eu odio eleifend, blandit leo a, volutpat sapien","title":"right align image"},{"location":"resources/RESOURCES/","text":"Additional resources \u00b6 IBM Demos \u00b6 Collection: InfoSphere Information Server Tutorial: Transforming your data with IBM DataStage Redbooks \u00b6 IBM InfoSphere DataStage Data Flow and Job Design InfoSphere DataStage Parallel Framework Standard Practices Videos \u00b6 Video: Postal codes and part numbers (DataStage) Video: Find relationships between sales, employees, and customers (Information Analyzer) Video: Clean and analyze data (Governance Catalog)","title":"Additional resources"},{"location":"resources/RESOURCES/#additional-resources","text":"","title":"Additional resources"},{"location":"resources/RESOURCES/#ibm-demos","text":"Collection: InfoSphere Information Server Tutorial: Transforming your data with IBM DataStage","title":"IBM Demos"},{"location":"resources/RESOURCES/#redbooks","text":"IBM InfoSphere DataStage Data Flow and Job Design InfoSphere DataStage Parallel Framework Standard Practices","title":"Redbooks"},{"location":"resources/RESOURCES/#videos","text":"Video: Postal codes and part numbers (DataStage) Video: Find relationships between sales, employees, and customers (Information Analyzer) Video: Clean and analyze data (Governance Catalog)","title":"Videos"}]}